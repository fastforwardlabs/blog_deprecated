---
layout: newsletter
slug: 2018-05-23-subscribers
---

Hello!  

---

## A Multi-task Learning Application: Open-Domain Questions

Current large-scale question answering (QA) systems rely on multiple sources
(Wikipedia, knowledge bases, dictionaries etc); these systems use information
redundancy among the sources to answer correctly. Machine comprehension, on the
other hand, assumes that a short piece of relevant text is already identified
and given to the model; the model is then able to answer questions after reading
the relevant text. What if we wanted a system that can answer any question by
using only Wikipedia as a source of information? In that case, neither large scale QA
nor machine comprehension would work.

[A recent paper](https://arxiv.org/abs/1704.00051) discusses a possible solution
based on two components: 1) a document retriever module for finding relevant
Wiki articles and 2) a machine comprehension model that uses multi-task learning
and distant supervision to answer questions. The document retriever module
compares articles to questions using TF-IDF weighted (a statistical measure used
to find important words) bag-of-word vectors and takes local word order into
account. It returns 5 Wikipedia articles given any question. These articles are
then fed into the machine comprehension model. The machine comprehension model
uses two neural networks to separately encode a paragraph and a question. Once
encoded, the paragraph and question vectors are trained to predict the beginning
and end of a span of tokens that will form the answer to a question.

![Proposed system]({{ site.github.url }}/images/2018/05/Screen_Shot_2018_05_21_at_11_08_03_AM-1526915319234.png)

To train the system the authors began with SQuAD, a dataset for machine
comprehension based on Wikipedia. Each example in the dataset is composed of a
paragraph extracted from a Wikipedia article, an associated human-generated
question and an answer which is a span from this paragraph. To ensure the system
can answer any question (open domain), the authors added other datasets that
have been constructed in different ways, including CuratedTREC, WebQuestions, and
WikiMovies. Unfortunately these datasets only contain question-answer pairs and
do not have an associated document or paragraph (as in SQuAD).  The authors use
the following process (distant supervision) to build their training set. For
each question, they run the document retriever to get the top 5 Wikipedia
articles. Paragraphs from those articles without an exact match to the known
answer are directly discarded.  Paragraphs shorter than 25 or longer than 1500
characters are also filtered out. All remaining paragraphs are scored using
unigram and bigram overlap between the question and a 20-token window. The top
ranking ones are added to the training set. The authors compared various models
(a single Document Reader trained on SQuAD data only and a multi-task model
jointly trained an all datasets) and found that the multi-task version
outperforms. 

We think this is a cool application of multi-task learning on a
challenging task!

---

## Avoiding the angry rainbow

Do you know what an “angry rainbow” is? It’s what visualization people have dubbed the rainbow that results from cycling through the hue range while keeping constant saturation and lightness values in the HSL (Hue, Saturation, Lightness) color space.

![The angry rainbow. Generated using HSL space, it has spiky transitions.]({{ site.github.url }}/images/editor_uploads/2018-05-19-153852-Screen_Shot_2018_05_18_at_11_38_05_AM.png)

##### The angry rainbow.

This approach feels rational in code. One of the benefits is of using the HSL space instead of RGB (Red, Green, Blue) space is that it makes it easier to programmatically adjust colors. The resulting rainbow is not *perceptually* pleasing, however. The yellow, light blue, and purple hue range appear spiky. The solution, as shown in [a notebook by Mike Bostock](https://beta.observablehq.com/@mbostock/sinebow), is to stick to the RGB space and use three offset sine waves. This provides a much smoother rainbow, called the *sinebow*.

![]({{ site.github.url }}/images/editor_uploads/2018-05-19-154014-Screen_Shot_2018_05_18_at_11_38_24_AM.png)

##### The sinebow.

Bostock cites two posts as originating the sinebow, [Jim Bumgardner’s](https://krazydad.com/tutorials/makecolors.php) and [Charlie Loyd’s](http://basecase.org/env/on-rainbows). Both are great reads. We were especially interested in Loyd’s further exploration of how to use the sinebow to generate `n` number of colors that are as distinct as possible while having similar perceptual weights. The answer turns out to be stepping around the sinebow using the golden ratio. As Loyd points out, the problem is closely related to [how plants position leaves around a stalk so that they minimize blockage of the leaves below](http://en.wikipedia.org/wiki/Phyllotaxis). Compared to its use in many [questionable logo presentations](https://www.cbsnews.com/news/pepsis-nonsensical-logo-redesign-document-1-million-for-this/), it’s heartening to see a non-bs application of the golden ratio.

The distinct color sequence of unknown length problem was interesting to us because we often use color as to add an extra dimension of information to our prototypes. The dream is to be able to switch out the number of categories (or whatever else you’re color coding) and have your color functions set up so that everything automatically adjusts. I (Grant) did some experiments along those lines using the sinebow. It worked as claimed, but I realized that by focusing on colors that were perceptually similar I was greatly limiting the number of distinct colors I could access.

![An image of bricks of text, each containing a fake username. The background color for each brick was randomly selected.]({{ site.github.url }}/images/editor_uploads/2018-05-19-154335-Screen_Shot_2018_05_18_at_11_43_12_AM.png)

##### Randomly selected background colors that pass contrast accessibility guidelines.

To avoid that limitation, I ended up going a slight different direction, inspired by [Brent Jackson’s Colorable library](http://jxnblk.com/colorable/demos/text/), I made a notebook that examines all the colors that pass the [web content accessiblity guidelines for contrast](https://www.w3.org/TR/WCAG20/#visual-audio-contrast) compared to the selected text color. This can be used for randomly assigning background colors while ensuring any text on top of will be readable. It has a wider range of colors available because it does not try to maintain a consistent perceptual weight (so it will use both yellow and a lighter yellow, for example). This ignorance of perceptual weight means you wouldn’t want to use it in strictly random mode for a data visualization.

![An animated GIF of the 3d accessible color explorer. It shows cubes of color arranged by hue, saturation and lightness. As you change the foreground color it hides the blocks which do not meet the contrast requirements.]({{ site.github.url }}/images/editor_uploads/2018-05-19-154444-acccube.gif)

##### A 3d version of the accessible color contrast explorer.

I also made a [3d version of the explorer](https://beta.observablehq.com/@grantcuster/acessible-color-contrast-explorer-3d-edition) that is less practical but cooler. These explorations end up having an interesting connection to creative machine learning tools. To build useful and collaborative ML tools, we’ll need interfaces that visualize multi-dimensional possibility spaces. Color space visualizations are actually interfaces for exploring a 3-dimension possibility space, so we can take lessons from them to apply to future machine learning tools.

A final note: it’s fun to explore the the theory and math behind all this, but if you're looking for stable, well-thought out color schemes for a data visualization we recommend [colorbrewer](http://colorbrewer2.org/). Also be sure to [consider your color-blind users](http://blog.usabilla.com/how-to-design-for-color-blindness/) when deciding on your final scheme.

---

## CFFL Updates

