---
slug: 2020-01-15-client
layout: newsletter
---

### Recommended Reading

Hello! In keeping with our reputation as your data nerd friends, here's a quick peek into what we've been reading lately.

----

#### [How front-end development can improve Artificial Intelligence](https://explosion.ai/blog/how-front-end-can-improve-ai). 

This article from 2016 by our friend Ines at explosion.ai is still very much valid today. The role of front-end in data science is often restricted to visualization and dashboards. This is an enormous lost opportunity. It’s a personal resolution of mine to work more on interfaces for using and understanding machine learning systems in 2020. -   *[Chris](https://twitter.com/_cjwallace)*

---

#### [Anti-Overfitting Techniques](https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323)

This catalog of overfitting problems in ML models is both a pre-flight checklist for models, and a set of recipes to mitigate overfitting. It's directed specifically to deep learning, but it's applicable to other types of models as well.  - *[Ryan](https://twitter.com/MicallefEsq)*

---

#### [ORL: Reinforcement Learning Benchmarks for Online Stochastic Optimization Problems](https://arxiv.org/abs/1911.10641)

DRL shows promise for real world problems! Amazon applied DRL (via OpenAI Gym) to canonical operations research/supply chain problems such as bin packing, newsvendor and vehicle routing. They find that DRL beats or matches baseline. Next step - can this work for real world instances of these problems? (They think not yet).  - *[Shioulin](https://twitter.com/shioulin_sam)*

---

#### [NLP's Clever Hans Moment has Arrived](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)

This article questions the value of benchmark datasets for evaluating the true performance of NLP models. Some models may be exploiting shortcuts to obtain excellent scores while failing at the core of the task - in this case, reasoning and comprehension. - *[Victor](https://twitter.com/vykthur)*

---

#### [Why are so many AI systems named after Muppets?](https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie) 

What may have started out as a bit of a joke actually highlights the importance of collaboration and respect within the open-source community towards the development of ML/AI products - characteristics that, as this article points out, are something Sesame Street fosters. One of my favorite excerpts from this article: "AI isn’t a discipline where lone scientists toil away in the lab at night, pumping electricity through processors, and cackling “It’s aliiiive” over a glowing command line. (Disclaimer: this certainly does happen, but it’s not always the most productive approach.)" - *[Danielle](https://www.linkedin.com/in/daniellethorp/)*

---

#### Work from Everest Pipkin's Data Gardens Class
https://twitter.com/everestpipkin/status/1210636168242614274

Everest Pipkin provides a behind-the-scenes look at a creative coding class they recently taught. The assignments are all interesting and the student work looks great. I especially liked the "folder structure as memory palace" prompt. - *[Grant](https://twitter.com/GrantCuster)*

---

#### [Manifold](https://github.com/uber/manifold)

Uber recently released a visual debugging tool for machine learning - Manifold. It is a model monitoring and debugging tool which compares feature distributions across tabular data subsets. It is model agnostic and helps users determine what data slices a model fails on and the potential causes for certain performance issues. It also integrates with Jupyter Notebook. It will be interesting to watch this space and how the features for the subsequent versions of Manifold unfold! - *[Nisha](https://twitter.com/NishaMuktewar)*

---

#### [code2seq](https://code2seq.org/)

A really interesting project from 2019 called code2seq introduced a method for generating natural language sequences from the structured representation of source code. This research sheds opportunity for automated code documentation and summarization. - *[Andrew](https://twitter.com/andrew_reed_r)*

---

#### [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

Brand new on arXiv this week, "Reformer: The Efficient Transformer" shows how old dogs can still learn new tricks. The authors reimplement the now-standard Transformer architecture (first brought to fame in the BERT NLP model) using Locality Sensitive Hashing, a long-standing tried-and-true technique for efficient look-up of similar items. This reduces the complexity of the algorithm and allows for longer sequences (e.g., sentences) to be used successfully.  I love seeing classic techniques reinvented in modern algorithms! - *[Melanie](https://www.linkedin.com/in/melanierbeck/)*

---

### Upcoming Events

* Shioulin Sam is speaking at [Data Day Texas](https://datadaytexas.com/) on in Austin on January 25th.
* Victor Dibia and Nisha Muktewar will be hosting a webinar on [Deep Learning for Anomaly Detection](https://www.buttera.com/about/events/webinars/deep-learning-for-anomaly-detection.html) on February 13th, in conjunction with the launch of our newest research report. [Register today!](https://www.buttera.com/about/events/webinars/deep-learning-for-anomaly-detection.html)
* Victor and Nisha will also be presenting on Deep Learning for Anomaly Detection at the [Strata Data Conference](https://conferences.oreilly.com/strata-data-ai/stai-ca/public/schedule/detail/80421) in San Jose on March 18th.
