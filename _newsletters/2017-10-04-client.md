---
layout: newsletter
slug: 2017-10-04-client
---

Hello!

"We learned we are truly rhythmic organisms," [says Michael W. Young](https://www.washingtonpost.com/news/to-your-health/wp/2017/10/02/nobel-prize-in-medicine-or-physiology-awarded-to-tktk/?utm_term=.d03136659314), who just [won the Nobel Prize](https://www.newyorker.com/tech/elements/the-real-message-of-the-2017-nobel-prize-in-physiology-medicine) in medicine alongside his colleagues Michael Rosbash and Jeffrey C. Hall, for their discoveries about the mechanisms that control an organism's internal clocks (or circadian rhythms); their work explains why jet lag feels so discordant. 

In this week's newsletter, we cover synthetic gradients (an alternative to backpropagation that enables distributed (read: much faster) training of neural networks) and reflect on new machine intelligence research institutions like The Allen Institute for AI, OpenAI, and yes, Cloudera Fast Forward Labs.  Also, problematic kittens.

---

## Synthetic gradients: gradient based neural net training without "locking"

*"My view is throw it all away and start again,"* says Goeffrey Hinton of [backpropagation](https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html), an algorithm he [first described in 1986](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) for computing gradients. Gradients are used by algorithms such as stochastic gradient descent (SGD) to repeatedly adjust the weights (i.e., connections) of neural networks so as to minimize the difference (i.e., error) between the network's prediction and the actual true label during training. Backpropagation, alongside SGD, has become the bread and butter mechanism for deep learning; it helps algorithms learn (see [Why we should be Deeply Suspicious of BackPropagation](https://medium.com/intuitionmachine/the-deeply-suspicious-nature-of-backpropagation-9bed5e2b085e)).

*“Science advances one funeral at a time,”* says Hinton, paraphrasing Max Planck. Why should we lay backpropagation to rest?

*"I don't think it's how the brain works,"* says Hinton and he is most likely right. The brain is a complex organ we do not (yet) understand. “Science … means unresting endeavor and continually progressing development toward an aim which the poetic intuition may apprehend, but the intellect can never fully grasp,” a Max Planck quote we like. But backpropagation has other issues, such as *locking*.

Neural networks can be represented as a graph of computational modules (or layers, nodes). Training networks amounts to optimizing the weights associated with the modules of this graph to minimise loss (a function of the difference (i.e., error) between the predicted and true label) using, most often, backpropagation and SGD. Backpropagation results in locking because the weights of a layer can only be updated after a full forward propagation of the input data through the network to make a prediction, followed by a comparison of the predicted and true label to compute the loss, and then backwards propagation of the gradient to adjust the weights to make a better prediction next time. Locking increases training time and limits our ability to train neural networks in a distributed fashion.

Can we do without backpropagation?

Yes. Instead, we use *synthetic gradients*. The idea of synthetic gradients is to use a model to predict the approximate true gradient of the loss. Instead of using the true gradient during training, we use the predicted gradient by the model as described in a set of papers by [Jaderberg and colleagues](https://arxiv.org/abs/1608.05343) and [Czarnecki and colleagues](https://arxiv.org/abs/1703.00522).

![](/images/2017/10/Screen_Shot_2017_10_03_at_10-1507041836229.43)

##### (a) A section of a feed-forward neural network. (b) Incorporating one synthetic gradient model for the output of layer *i* results in two subnetworks which can be updated independently. (c) Incorporating multiple synthetic gradient models after every layer results in *N* independently updated layers.

The model to predict the gradient is trained *at the same time* as the neural network based on the difference between the true gradient and the predicted gradient, and it works. [Jaderberg et al.](https://arxiv.org/abs/1608.05343) show that synthetic gradients remove locking and enable us to train computational components independently: *the* requirement for distributed computing (training). Synthetic gradients also increase the capacity of recurrent neural networks to model long-range temporal dependencies (synthetic gradients complement and extend backpropagation through time, BPTT), which is important for natural language modeling. 

[Czarnecki et al.](https://arxiv.org/abs/1703.00522) show that synthetic gradient-based models converge to the same solution as models using backpropagation, but intruigingly, models trained with synthetic gradients can be qualitiatively different from models trained using backpropagation. 

![](/images/2017/10/Screen_Shot_2017_10_03_at_11-1507045880907.50)

##### Assessing representational similarity between different models is difficult; one approach is [representation dissimilarity matrices](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/). Here, the authors sampled a subset of 400 points from MNIST, ordered them by label, and then recorded activations on each hidden layer when the network was presented with these points. Then, they  plotted  the  pairwise  correlation  matrix  for  each layer. The emergence of a block-diagonal correlation matrix means that at a given layer, points from the same class already have very correlated representations. We here see differences between a model trained with backpropagation (top), a single synthetic gradient between layer 11 and 12 (middle), and only synthetic gradients (bottom).

The brain may not use synthetic gradients, but they are exciting nevertheless. Synthetic gradients enable us to train neural networks faster, extend the range of temporal dependencies (recurrent) neural networks can learn, and allow us to build complex systems of neural networks.   

---

## In between research and development

There's a philosophy of history that sees history as shaped by the dramatic decisions powerful individuals make in times of turmoil, upheaval, and change. These 'Great Man' histories (and they are almost always men, in such accounts) center generals, kings, and presidents (as well as a few bit players) as the primary actors in history, leaving the balance of humanity relatively unaccounted for. Such stories do a reasonable job telling the 'what' of history, but seldom get at the 'why' of things. Without accounting for broader social movements, contradictory economic interests, or relevant institutional developments, 'great man' histories often obscure more about the past than they reveal.

One of the 'great men' of science, Louis Pasteur, is usually given credit for a great many things, particularly the germ theory of disease. But, as Bruno Latour demonstrates in [_The Pasteurization of France_](http://www.hup.harvard.edu/catalog.php?isbn=9780674657618), Pasteur's solitary work in his laboratory was not what transformed France's public health system, farm-to-market economy, or military. Rather, Pasteur worked relentlessly to convince and coordinate a complex assemblage of laboratory tools, individuals, institutions, and other objects (including microbes) to conspire together for his scientific findings to have the any effect on society at all. History mattered to this effort, too. Had France not still been reeling from the loss of Napoleon's disease-weakened army to Russian forces, it is difficult to imagine the French citizenry ever being mobilized to take up the cause of public hygiene as a matter of national survival in the ways that they ultimately did.

Science and technology, broadly speaking, still preserves some of the aura of 'great man' history. The image of the lone genius toiling away in a laboratory until some spark ignites a new industry built on that breakthrough remains firmly lodged in our public consciousness. But we know that scientific and technological progress are not neatly siloed in this way. Basic research is not the sole domain of academics in university labs, nor are new and profitable technologies the sole domain of startups and large firms. A range of other actors fill the gaps between these spaces and articulate connections between research and business in important ways. We have a model for this kind of work from early in the 20th century, when [Bell Labs](https://www.bell-labs.com/) was first founded to build upon and complement basic research, develop ideas into profitable products, and test and reshape knowledge about these technologies into forms useful for policy makers, investors, and the consuming public. We can also look at [JPL](https://www.jpl.nasa.gov/), [Xerox PARC](https://www.parc.com/), and the [National Laboratories](https://energy.gov/national-laboratories) as institutions that have functioned in similar ways.

Today we are seeing this pattern replicated by new in-between institutions for machine intelligence and artificial intelligence. Some of these entities, like [The Allen Institute for AI](http://allenai.org/) or [The Vector Institute](http://vectorinstitute.ai/) are more focused on more traditional R&D, while [Open.ai](https://openai.com/about/) gives R&D a twist by prioritizing research towards 'safe' AI. Other entities like [Partnership on AI](https://www.partnershiponai.org/) focus more explicitly on governance for AI, working towards policy frameworks and best practices for AI. Consultancies like [Element.ai](http://element.ai/), [Traction Technology Partners](http://www.tractiontechpartners.com/) and [Quantum Black](https://www.quantumblack.com/) also occupy a position in between research and deployment. Many of these entities are built around individuals who have been fundamental to the information revolution that has driven today's tech industry (Elon Musk at Open.ai, Paul Allen at the Allen Institute), while others are built around foundational researchers whose academic work is central to AI (Geoff Hinton at The Vector Institute, Yoshua Bengio at Element.ai), just as the original Bell Labs was centered on the inventor of the telephone, Alexander Graham Bell.

Looking at the boards of directors of these institutions and the corporate partners they trumpet on their websites reveals how they work to connect VCs, academic researchers (and their labs), large companies, policy think tanks, and other non-profits. These boards also reveal how much work needs to be done to connect these various actors. One wouldn't think this would be such a challenge, or require so much work, given that the discipline of computer science originated as an applied discipline, but the lesson we take from looking at all of these institutions is that machine learning and artificial intelligence do not move neatly from the computer lab to the real world. Like Pasteur's microbes, they must be mobilized.  They must be adapted to business plans, and business plans must be adapted to the unique needs of these technologies. They require data, and data must be reshaped to play nice with ML and AI algorithms. They require people to make use of them, but they also need to be made governable by people so they will be trusted and trustworthy. 

By paying attention to how these entities work to make AI and ML mobile and to reshape both the technology and social institutions affected by it, we at Cloudera Fast Forward Labs stay on top of the inventory of challenges our clients face in helping research find application in industry. This is a practice anyone can emulate by looking at who is active in these in-between institutions, asking themselves what problems they are trying to solve, and what opportunities they are trying to create or maintain for their partners and board members. (It doesn't hurt that by staying on top of how these institutions are organizing, we also stay on top of the very useful and exciting research that they are publishing!)

---

## The decisive camera

What happens when you ask people to label images as "the past" or "the future," "the solution" or "the problem"? You get "[The Decisive Camera](https://linkcabinet.eu/)," powered by an algorithm that knows decisively who is who and what is what.

![](/images/2017/10/Screen_Shot_2017_10_03_at_12-1507047550212.18)

##### Looks 57% like the problem and 40% like the future. Are we surprised?

Whimsical, yes. But [the project](https://linkcabinet.eu/About/index.html) also illustrates the trouble we face when machine intelligence touches upon abstract concepts like, the past and the present, fairness and freedom. Machine learning wants unambiguous truth; the world is far more complex.

---

## Cloudera Fast Forward Labs Updates

We will be out and about at some upcoming conferences:
* 10/4 - 10/6 - Hilary - [Grace Hopper](https://ghc.anitaborg.org/) (attending)
* 10/19 - Friederike - [AEC Technology Symposium](https://www.eventbrite.com/e/2017-aec-technology-symposium-and-hackathon-tickets-34830026547) (speaking)
* 10/25 - Hilary - [Machine Learning and the Market for Intelligence](http://www.marketforintelligence.com/) (speaking)
* 11/27 -11/30 - Friederike - [PyData 2017](https://pydata.org/nyc2017/) (speaking)

Be sure to say hello if you're attending!

And as always, thank you for reading.  We'd love to hear your thoughts! You can reach us anytime at clients@fastforwardlabs.com. 

-- The Fast Forward Labs Team
