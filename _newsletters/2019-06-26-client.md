---
slug: 2019-06-26-client
layout: newsletter
---

## [Sneak Peek] Our New Prototype - ConvNet Playground

_by [Victor](https://twitter.com/vykthur)_

An upcoming report here at Fast Forward Labs focuses on recent advances in Deep Learning for Image Analysis. As part of this report, we have created a prototype - Convnet Playground - a web interface for the interactive exploration of convolutional neural networks. The prototype demonstrates how layers from a convolutional neural network can be used to implement semantic image search (search for images by content) and also visualizations of the features learned by trained models.


### Using Layers in a Pretrained Model as Feature Extractors. 

![]({{ site.github.url }}/images/editor_uploads/2019-06-27-180605-extract.jpg)

Convolutional Neural Networks (CNNs) excel at learning meaningful representations of concepts within images. When these networks are trained on a huge, diverse dataset such as imagenet, they learn representations that are useful for many other tasks. In fact, it is this set of representations that make pretrained models so useful for transfer learning. 

A valuable application of representations learned from pretrained models is the task of semantic search. We define semantic search as follows: given a dataset of existing images, and a new arbitrary image, find a subset of images from the dataset that are most similar to the new image. To achieve this, we can use CNNs to generate representation (feature vectors) of all images and compute similarity as the distance between vectors for any pair of images.

While the above is fairly straightforward, there are several decisions to be made. What model architecture should I use? What layer in the model best captures notions of similarity for images across my dataset? What distance metric is appropriate?


### Building Intuition about CNNs with Convnet Playground

Our prototype is designed to help users explore the questions above and build intuition about _how_ CNNs work. To support these objectives we have created an experience where the user can perform a search query (select an image from a dataset we provide), and view the results (a list of similar results based on features extracted from a CNN model).

![]({{ site.github.url }}/images/editor_uploads/2019-06-27-180530-search.jpg)

More advanced users can explore how these results change when different layers from different models are used for feature extraction. They can also explore the results when different distance metrics are used to compute similarity. One interesting intuition is to observe how later layers in a model perform better in representing the similarity between complex images. Interestingly, in scenarios where the primary concept of similarity is simple (e.g., for clothing, similarity is frequently about shape and color), there are cases where earlier layers in the model perform better. 

![]({{ site.github.url }}/images/editor_uploads/2019-06-27-180627-config.jpg)

Users can change the configuration of their search. They can choose from 4 datasets, use features extracted from layers within 7 model architectures, and explore similarity results using 4 different distance metrics. 

![]({{ site.github.url }}/images/editor_uploads/2019-06-27-180645-embedding.jpg)

Users can also inspect a UMAP visualization of the features generated for images within a given layer in a model. They can begin to visually inspect how different models and layers embed images and why some results are listed as similar (hint: because the model has embedded them close to each other). It also becomes clear how high performing models show clear separation between classes and are less like likely to return incorrect search results.


### Closing Thoughts

Our example and definition of semantic search is deliberately defined to be simple and broad. We use features extracted from model architectures pretrained on imagenet. In practice, it is common (and recommended) to further fine-tune these models using training objects specific to a given dataset, or utilize additional information available (e.g., text descriptions, interaction logs, etc.) in constructing embeddings that better capture the similarity between images. That being said, our experiments show that these pretrained models can get you really far and work very well for many similarity search use cases. More importantly, we recommend the use of visualizations (as presented in Convnet Playground) as a way to inspect the results of models and iterate on decisions for improving performance.

Interested in exploring this prototype and learning more about other recent advances in deep learning for image analysis? We look forward to sharing our findings in our upcoming report at the end of July; stay tuned!
 
---

## Upcoming Events

* Please join us tomorrow for a virtual event: [Introducing Cloudera Data Science Workbench 1.6 and New ML Services](https://www.cloudera.com/about/events/webinars/virtual-event-ml-services-cdsw.html?utm_medium=email&utm_source=email&utm_campaign=ml&src=email&cid=7012H000001l1jG&utm_content=CDSW_Showcase_1.6_organic_AMER_Webinar_2019-06-27)!  (Just click through to register.)
* Hilary Mason is speaking at the [VentureBeat Transform 2019](https://www.vbtransform.com/) conference on July 11th in San Francisco, CA.