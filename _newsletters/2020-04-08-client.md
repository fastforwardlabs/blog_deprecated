---
slug: 2020-04-08-client
layout: newsletter
---

Hello! We've put together another collection of articles we've found interesting; we hope you enjoy them. Happy reading!  

Also, please be sure to join us tomorrow for our webinar on **[Opening the ML Black Box: Deploying Interpretable Models to Business Users](https://www.cloudera.com/about/events/webinars/opening-the-ml-black-box-deploying-interpretable-models-to-business-users.html?utm_medium=clouderan&utm_source=field&keyplay=ml&utm_campaign=FY21-Q1_CW_AMER_ML_Black_Box_2020-04-09&cid=7012H000001Oh5c)**.

---

## Recommended Reading

#### [The impossibility of low-rank representations for triangle-rich complex networks](https://arxiv.org/abs/2003.12635) 

An esoteric title for the layperson, but the results of this paper have heavy implications for the users of graph embeddings. The authors prove that such embeddings fail to capture significant properties of their originating graphs. If embeddings lose key information, this can call into question the downstream results (e.g., machine learning models) that use those embeddings. Such implications are very relevant to my current work in biased knowledge graphs, where embeddings are used in many use cases. - *[Keita](https://twitter.com/keitabr)*

#### [The Basecamp Guide to Internal Communication](https://basecamp.com/guides/how-we-communicate) 

Everyone and their dog is giving out remote working tips at the moment. This is how Basecamp - a remote and asynchronous company - communicate. The guide is short and authoritative, and you should read it. - *[Chris](https://twitter.com/_cjwallace)*

#### [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893) 

This is a beautiful paper intersecting causality and machine learning. I’ve spent quite some time with it while preparing our next research release (spoiler alert). For those with a machine learning background who are mystified by causal inference, this is a great starting point on the path, making the connection between causality, invariance, and generalization. - *[Chris](https://twitter.com/_cjwallace)*

#### [Data science at StitchFix](https://cultivating-algos.stitchfix.com/) 

An amazing post by StitchFix on how they approach data science within their organization through an interactive visualization! StitchFix’s take on organizational structure, roles and processes that are unique to them, yet applicable at other companies. And in many ways echoes Cloudera Fast Forward's thoughts in this space. - *[Nisha](https://twitter.com/NishaMuktewar)*

#### [A Visual Explanation of Transformer Models](http://jalammar.github.io/illustrated-transformer/) 

Transformer models have become the golden standard in sequence transduction tasks. This blog post visually depicts the inner workings of a Transformer architecture in a digestible level of detail and explains how this architecture lends itself to efficient, parallel computing. - *[Andrew](https://www.linkedin.com/in/andrew-r-reed/)*

#### [A Step Towards Protecting Patients from Medication Errors](https://ai.googleblog.com/2020/04/a-step-towards-protecting-patients-from.html)
Medication errors happen more often than we’d like to think and its not always the fault of the prescribing doctor or nurse. Most medical systems only have very simple rules-based error alerts for flagging medication issues. In this article, researchers discuss how they are using more sophisticated ML approaches, which are starting to learn how and when a doctor might prescribe a certain medication. This is an important step towards recognizing abnormal prescriptions and potentially saving lives. - *[Melanie](https://www.linkedin.com/in/melanierbeck/)*

#### [Generalization without Systematicity: On the compositional Skills of Sequence-to-Sequence Recurrent Networks](https://arxiv.org/abs/1711.00350) 

When we learn the meaning of a new verb "bah," we can easily generate and understand new phrases such as "bah twice." This is not true for neural networks, as they don't quite have the ability to exploit algebraic compositionality. Lake et al construct a simple dataset to test this hypothesis on various RNNs in a controlled manner. The dataset maps commands in words (jump left) into action sequences (LTURN JUMP). The learner's goal is to translate commands into a sequence of actions. The authors find that RNNs can make successful zero-shot generalizations when the difference between training and test commands are small, but fail spectacularly when the link between training and testing data is dependent on the ability to extract systematic rules (as in the "bah" example above). - *[Shioulin](https://twitter.com/shioulin_sam)*