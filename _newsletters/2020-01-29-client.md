---
slug: 2020-01-29-client
layout: newsletter
---

## Ten Wrongs Make a Right(ish)

by *[Ryan](https://twitter.com/MicallefEsq)*

We published a report a couple of years ago on [federated learning](https://blog.fastforwardlabs.com/2018/11/14/federated-learning.html). Briefly, federated learning is an approach to machine learning in which several "worker" devices (e.g., smartphones or blood pressure monitors) train independent machine learning models using their local data. The workers transfer those trained local models (but not the data) to a central manager machine that combines the models into a common model. The manager sends the common model back to the workers. The workers continue to train their local models, and the cycle repeats.

Lately I've been working on some follow-up research in this area and I've stumbled on an unexpected property of federation. It turns out that federating a bunch of truly awful local models, in a system with pathologically bad data allocation, can make a decent, if not spectacular, combined model.

I've been using the MNIST toy dataset with ten workers for some of my experiments. The MNIST digit images are allocated without replacement to each of the ten workers. I vary the way digits are allocated to workers and compare the outcomes. The obvious way to allocate the data is uniformly, where each worker sees about as many examples of any given digit. So about as many examples of the digit '7' go to worker A, worker B, and all the other workers.

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190528-equal_allocations.png)

When the data is allocated uniformly across workers, we get an expected result: the ten federated models converge and perform nearly as well as a standard non-federated model that has access to all the data in one place.

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190708-nonfederated_vs_federated_equal_loss.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190722-nonfederated_vs_federated_equal_accuracy.png)

One of the variables I've been looking into is how performance changes when data allocation gets biased. In these cases, I'm considering "biased" to mean that some digits are allocated more to one worker than others. For example, more examples of the digit '0' go to worker A than to the other workers, while more examples of the digit '1' go to worker B than to the other workers, and so on. This bias degrades the performance, as we might expect. 

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190622-federated_bias0_2_allocation.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190802-federated_bias0_2_comparison_loss.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190817-federated_bias0_2_comparison_accuracy.png)

When we make the bias in the allocations more extreme, each digit concentrating more and more at one worker, the performance suffers more and more. 

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190900-heavy_bias_allocation.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190931-heavy_bias_comparison_loss.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-190959-heavy_bias_comparison_accuracy.png)

The part I didn't really expect is out at the far corner: full-bias, where each worker sees exactly one type of sample. Put differently, all of the samples of the digit '4' go to only one worker, and all samples of the digit '5' go to only one worker. 

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-191039-federated_bias1_0_allocation.png)

So, no worker sees more than one class of digit. But this is a ten-class classifier. The individual worker models perform terribly, as one might expect. 

[IMAGE - terrible performance]

But over time, the federated combination (simple weight averaging) of these terrible models produces a common model that converges. So somehow, a federated system with 100% bias still works. 

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-191054-federated_bias1_0_performance_loss.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-191123-federated_bias1_0_performance_accuracy.png)

It's not pretty compared to less-biased models, no doubt, but it's odd that it works at all.

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-191218-full_bias_comparison_loss.png)

![]({{ site.github.url }}/images/editor_uploads/2020-01-28-191233-full_bias_comparison_accuracy.png)

So it seems that ten wrongs *can* make a (kind-of) right.

I have a bunch of questions about *why* this works, and some experimental directions to pursue, but I thought I'd share my findings to date. If you've tried anything like this before, please contact me; I'd love to trade ideas!

---

## Upcoming Events

* Victor Dibia and Nisha Muktewar will be hosting a webinar on [Deep Learning for Anomaly Detection](https://www.cloudera.com/about/events/webinars/deep-learning-for-anomaly-detection.html?utm_source=website&utm_medium=organic&utm_term=ml&utm_campaign=CFFL12_Report_AMER_Webinar_2020-02-13&cid=7012H000001OYfQ&utm_content=FFLnewsletter) on February 13th, in conjunction with the launch of our newest research report. [Register today!](https://www.cloudera.com/about/events/webinars/deep-learning-for-anomaly-detection.html?utm_source=blog&utm_medium=organic&utm_term=ml&utm_campaign=CFFL12_Report_AMER_Webinar_2020-02-13&cid=7012H000001OYfQ&utm_content=FFL)
* Victor and Nisha will also be presenting on Deep Learning for Anomaly Detection at the [Strata Data Conference](https://conferences.oreilly.com/strata-data-ai/stai-ca/public/schedule/detail/80421) in San Jose on March 18th.