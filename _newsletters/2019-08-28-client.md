---
slug: 2019-08-28-client
layout: newsletter
---

Hello!  In this month's edition of the Cloudera Fast Forward Labs newsletter, we share some thoughts on the direction of machine learning in general, NLP and Transfer Learning, and two approaches to data validation in ML production.

--- 

## Is machine learning research moving in the right direction? 

*by [Alice](https://twitter.com/AliceAlbrecht)*

Research in machine learning has seen some of the biggest and brightest minds of our time -  and copious amounts of funding - funneled into the pursuit of better, safer, and more generalizable algorithms. As the field grows, there is vigorous debate around the direction that growth should take (for a less biased take, see [here](https://www.technologyreview.com/s/612768/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/)). This week, I give some background on the major algorithm types being researched, help frame aspects of the ongoing debate, and ultimately conclude that there is no single direction to build toward - but that through collaboration, we’ll see advances on all fronts. 

![]({{ site.github.url }}/images/editor_uploads/2019-08-21-204110-robots_question.jpg)

##### image taken from https://systemdesign.intel.com/inferring-the-future-of-machine-learning/

One place where debates over whether machine learning is heading in the “right” direction hit a major roadblock is when those debating the issue don’t have a clear and agreed upon destination. There tend to be three general goals:

1) For some who dedicate their lives to researching, developing, and testing machine learning algorithms, the ultimate goal is something akin to artificial general intelligence often referred to as AGI and equally as often mis-labeled with the much narrower term “AI”. The goal of this line of research is to create a machine that can operate in the world in a way that’s indistinguishable from humans (and likely pass the turing test). 

2) For others, machine learning is a promising tool to model the human brain and further our understanding of human cognition. 

3) Still others are focused solely on building commercially viable products that can replicate and automate simple processes, and (in some cases) even outperform humans on highly specific tasks. 

Each goal stated above requires a different weighting of the algorithms being used, and hence a differential investment in lines of research. I won’t belabor the landscape of possible machine learning research areas here, but to ensure everyone reading this has at least a basic understanding of the landscape, I’ll touch on a few key areas. 

* Deep learning (a type of neural network with many layers) has seen an explosion in terms of research over the last 7 years, mainly employing supervised learning (check out our [recent report](https://clients.fastforwardlabs.com/ff03-2019/report) on Deep Learning for more). 
* Reinforcement learning (also a neural network) is an algorithm that aims to choose the optimal behavioral action in an environment given a pre-specified goal to achieve the largest cumulative reward (a lot of the research in this area is being led by Google’s [DeepMind](https://deepmind.com/) and [OpenAI](https://openai.com/)). 
* Natural language processing is a larger field in computer science but recent work has focused on using recurrent neural networks to glean meaning and even generate language (several of our research reports utilize NLP, the most recent of which you can find [here](https://clients.fastforwardlabs.com/ff11/report)). 

A problem with most of these neural network-based techniques is that they are very data hungry. A lot of recent research has been dedicated to finding ways to train these algorithms with less data (see our [recent blog posts](https://blog.fastforwardlabs.com/) for a sampling of some exciting directions).   

Alongside the fervor for neural networks, there’s been renewed attention given to Bayesian methods for developing machine learning models that can learn from less data and learn in a way that’s more akin to how human beings learn [see Josh Tennehbaum](https://web.mit.edu/cocosci/josh.html). [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl) has also recently re-popularized the use of Bayesian inference and causal models when trying to build intelligent machines. 

Bayseian methods remain less used for industrial automation purposes but do hold value if our goal is to build machines that think like humans. One difficulty in finding applied use cases utilizing Bayesian machine learning techniques is that they are not yet as easy to implement (in part because less research has been devoted to this area, comparatively).  

![]({{ site.github.url }}/images/editor_uploads/2019-08-21-204233-RL_CausalMapping_Comic.jpg)

##### See full comic here: http://existentialcomics.com/comic/70

Now that we understand some of the landscape of the methods that are being researched most today, we can start to understand the debate on the direction of that work (and how it differs depending on our goals). In the interest of brevity, I’ll focus on reinforcement learning here. 

Reinforcement learning has the potential to allow us to build machines that can accomplish complicated human tasks (it’s deeply implicated in autonomous driving). It’s also useful in informing neuroscience (e.g. [this article](https://www.sciencedirect.com/science/article/pii/S1364661319300610) and others from Jane X Wang). The goal of using this method to build enterprise-ready products is less straightforward as solutions here focus on well-established machine learning methods that can make inferences on short time scales and that can generalize to many real-world unplanned situations. This type of goal is not neatly aligned with methods like reinforcement learning. 

A critique of the funding poured into reinforcement learning was brought forth by Gary Marcus last week when he wrote [an article](https://www.wired.com/story/deepminds-losses-future-artificial-intelligence/) criticizing Google’s investment in DeepMind. Part of the debate here centers on the lack of applicable use cases and financial returns on such a large investment in research. Another part stems from a fear that if we don’t start to see some real progress toward AI, despite the hype and massive funding dedicated to this research, we’ll see another “[AI Winter](https://en.wikipedia.org/wiki/AI_winter)”. 

Just a few weeks prior to Marcus’ article, Google’s DeepMind released [a paper](https://www.nature.com/articles/s41586-019-1390-1) in the journal Nature detailing how they’d developed an algorithm to predict kidney injury. Unfortunately, the study has major flaws in the way the data  was chosen, such that the sample used for training consists of mostly white male patients ([see here for more](https://techcrunch.com/2019/07/31/deepmind-touts-predictive-healthcare-ai-breakthrough-trained-on-heavily-skewed-data/)). This sample choice severely limits the generalizability of this work to clinical populations and could lead to more skepticism around the applicability of machine learning research for entirely the wrong reasons. However, in this case at least, this is due not to algorithmic limitations per se, but rather to not having a team that is cross-disciplinary enough to spot this major flaw. 

This problem is pervasive as machine learning evolves, and lest we cast DeepMind in a shadow, they are one of the biggest contributors to tools and libraries that make reinforcement learning more accessible to those outside big corporations, such as their release of [bsuite](https://deepmind.com/research/open-source/bsuite) earlier this week. 

We’re seeing progress in all three goals outlined above (though we’re nowhere near building an AGI). That progress relies on a rich combination of the different types of algorithms explained above (as well as others). The fact is that much of the work done in pursuit of one goal will create libraries and artifacts that can be borrowed from those focusing on another goal. In that same vein, as we push for more applicable research (in part to create commercially viable applications to offset the cost of this research), it’s imperative that we collaborate with people across multiple disciplines to ensure what’s being built isn’t only algorithmically excellent but also methodologically sound and avoids bias as much as possible.  

A lack of clear definition around the direction of machine learning research can fuel larger paranoia and make room for others not involved in machine learning research to claim a direction. This is illustrated in an opinion piece by [Peter Theil](https://www.nytimes.com/2019/08/01/opinion/peter-thiel-google.html), where he argued that AI is a “military technology” and chastised Google for building AI research labs in China. The answer then, as I see it, is not to continue to throw stones across the virtual wall and debate which algorithms or methods reign supreme - especially since we don’t even have a clear end point in mind (are we building a human-like intelligence, or a reasonable customer service call router?). We should instead seek to include those with complementary subject matter expertise and perpetuate the current culture of open science in machine learning.            

---

## Two approaches for data validation in ML production
by *[Shioulin](https://twitter.com/shioulin_sam)*

Machine learning models start to deteriorate once we deploy them. This is partly
because real life data changes, and models need to be re-trained to maintain
their performance. Typical ML pipelines re-train periodically (daily, for
example) using newly available data. But how do we validate data fed into the
pipelines to make sure tainted data does not accidentally sneak into the
production system? Tainted data could cause system crashes or lead to slow
degradation of model performance. The impact of the former is painful and
immediate; the impact of the latter is perhaps more dreaded. It's hard to
debug and isolate.

### Unit-tests for datasets

[Amazon research (PDF)](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf)
proposed a unit-test approach. The idea is to create a system that (i) allows
users to (easily) define constraints (or checks) on the data, (ii) converts these
constraints to computable metrics, and (iii) reports which constraints succeeded
and failed, and the metric value that triggered a failure.

![]({{ site.github.url }}/images/2019/07/amzn_constraint-1562965509938.png)
##### Example constraints ([image source](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf))

How does it work? Imagine some data as a set of log files generated by an
on-demand video platform. The log contains information about platform usage,
type of device, length of session, title, and customer id. This is ingested and
used as training data for recommendation systems. To validate this data using
the proposed system, the user defines a set of checks including (i) _completeness_
and _consistency_ (for example: *customerID* and *title* columns should have no
missing values), (ii) _uniqueness_ (for example: each row of combined *customerID*
and *title* value should be unique), and (iii) _counting_ (for example: number of
distinct values in the *title* column should be less than the total number of
movies in the system). Once the constraints are specified, the system converts
them into actual computable metrics. For example, _completeness_ would convert
into a "fraction of non-missing values in a column" metric. The last step is to
generate a report to show how all the constraints fared. The report also lists
the ones that failed, along with the value that triggered a failure.

![]({{ site.github.url }}/images/2019/07/amzn_system-1562965579363.png)
##### System architecture ([image source](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf))

Because new data comes in continously, the system uses a recursive computation
approach that only looks at _new data since the last time step_ to update the
metrics incrementally. In addition, to lower the barrier of adoption, the system
automatically suggests constraints for datasets. This is accomplished through
clever use of heuristics and a machine learning model. 

The heuristics approach employs single-column profiling - all the user needs to provide is a single
table dataset with column names to start. The system then executes single column
profiling in three passes; the goal is to figure out data size, data type,
summary statistics (min, max, mean, etc.) and frequency distribution of the
data. Based on the profiling results, the system recommends constraints based on
a set of heuristics. For example, if a column is complete, the system suggests
an _isComplete_ constraint. If the number of distinct values in a column is
below a threshold, the column is assumed to be categorical and the system
suggests an _isInRange_ constraint that checks whether future values are
contained in the set of already observed values. The heuristic approach is
complemented by a machine learning model. The model is trained to predict
constraints based on table name, column name, and type. Finally, the system
performs anomaly detection on historic time series of data quality metrics (for
example: the ratio of missing values for different versions of a dataset). This
can be done using either built-in or user-provided algorithms.

### Data Schema applied to ML

In [a separate paper (PDF)](https://www.sysml.cc/doc/2019/167.pdf), Google
introduced a similar system, but one that relies on the concept of data schema
to codify expectations for correct data. The system integrates into existing ML
pipelines right after training data ingestion - training data is validated
before being piped into a training algorithm. This system consists of three main
components: a _Data Analyzer_ that computes a predefined set of statistics that
is considered sufficient to describe the data, a _Data Validator_ that checks
the data against a Schema, and a _Model Unit Tester_ that checks for errors in
the training code using synthetic data generated through the schema. The system
can (i) detect anomalies in a single batch of data, (ii) detect significant
changes between successive batches of training data, and (iii) find assumptions
in the training code that are not reflected in the data (for example: taking the
log of a feature that turns out to be a negative number).

![]({{ site.github.url }}/images/2019/07/goog_system-1562965719123.png)
##### System architecture ([image source](https://www.sysml.cc/doc/2019/167.pdf))

In simple terms, a schema is just a _logical_ model of the data. Because machine learning data typically
consists of features, the schema would have a collection of features, and the
constraints associated with each. These constraints include basic properties
(for example: type, domain) and ML relevant ones. For example, we want a
_presence_ constraint to make sure features are not accidentally dropped. We
also want a _domain_ constraint to make sure country codes are always
represented by upper-case strings. To overcome the adoption hurdle (constructing
a schema manually is tedious!), a basic version of the schema is sythesized
based on all available data in the pipeline by relying on a set of reasonable
heuristics.

![]({{ site.github.url }}/images/2019/07/goog_schema-1562965821479.png)
##### Schema example ([image source](https://www.sysml.cc/doc/2019/167.pdf))

Once we have the schema in place, the _Data Validator_ validates each batch of
data by comparing it against the schema. Any disagreement is flagged and sent
over to a human for further investigation. In addition, the _Data Validator_
also recommend updates to the schema as new data is ingested and analyzed. Users
can easily apply or edit the suggested changes using a click-button interface.

Some anomalies only manifest when we look across multiple batches rather than
focusing on a single batch in isolation. Distribution skew, for example, occurs when
the distribution of feature values of a batch of training data is different from
that seen at serving time. To detect this type of anomaly, the system relies on
metrics that can quantify the distance between the training and serving
distributions. Metrics such as KL divergence or cosine similarity can be used,
but they rely on setting a threshold; anything that crosses the threshold is
considered an anomaly. Unfortunately, this threshold needs to be tuned, and
product teams have a hard time building intuition for what this threshold
actually means. The proposed system thus uses a different metric that comes with
a natural interpretation: it sets the largest allowed change (in probability)
for a particular value. With this metric, teams can say "allow changes of up to
1% for each value" rather than having to a set an obscure threhold for KL
divergence.

Lastly, the system builds in model unit testing - it catches mismatches between
the expected data and the assumptions made in the training code. Imagine a
scenario where the training code applies a log computation on a numeric
feature. This implies that the feature's value is always positive, but if the
schema does not encode this assumption, how do we catch this type of error? The
proposed system uses a _fuzz testing_ approach. First, synthetic training
examples that adhere to the schema constraints are generated. Second, this
generated data is used to drive a few iterations of the training code. The goal
is to trigger hidden assumptions in the code that do not agree with schema
constraints.

Both approaches are used in actual production systems at Amazon and Google and
seem to be impactful. As enterprises put more machine learning models into
production, monitoring the quality of data that feeds into these models becomes
even more important. [Data needs to be elevated to a first-class citizen in ML
pipelines](https://www.sysml.cc/doc/2019/167.pdf), and not just in the model
development stage!

---

## NLP and Transfer Learning
*by [Ryan](https://twitter.com/jqpubliq)*

Natural language processing just took a leap forward with the release of new high-quality language models combined with transfer learning. As you know, at Cloudera Fast Forward Labs, we have been researching this leap forward and have just finished a [new report on the topic](https://blog.fastforwardlabs.com/2019/07/17/new-research-transfer-learning-for-natural-language-processing.html).

These new NLP models rely on deep neural networks, which are data hungry and compute intensive to train. Various research groups trained these new models with tremendous amounts of data and compute power. They are also aware of the sequential nature of language, i.e., the flow of words and sentences, and the context in which words appear. These models have a much more nuanced understanding of language than prior approches, and they're now available for anyone to use:

* [BERT (Google)](https://github.com/google-research/bert)
* [ELMo (AllenNLP / UW Computer Science and Engineering)](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md)
* [ULMFiT (available through fast.ai libraries)](http://nlp.fast.ai/)
* [GPT-2 (OpenAI)](https://openai.com/blog/better-language-models/)

These models are very powerful, but they are limited, at least facially, to the task for which they were trained. Specifically, they are trained to generate language, i.e., predict the next words following an input prompt. Training a model for this task is straightforward because any text is self-labeling - that is: any text can be used as input data because the next word is always known, so the model can guess the next word and then check the next word to see whether it was correct.

Training models for other tasks requires data that is harder to obtain. For example, sentiment analysis - the task of predicting how an author feels based on their writing - would require writing samples labeled with the author's actual opinion. Labeling this data requires a human to read the text and make their own assessment of the author's feeling about the subject (positive, negative, or neutral). This training data is very expensive to create.

The technique of transfer learning adapts recently-released pre-trained language models for other tasks. Transfer learning uses the pre-trained models' extensive understanding of language and applies it to new tasks with relatively little training data. So a pre-trained model can be adapted to the task of sentiment analysis with very little labeled sentiment data. 

The chart below shows the accuracy of several models predicting sentiment as a function of how many labeled training examples each model was shown. For example, a naive Bayes support vector machine and a word vector model (both older technologies) scored around 50% accuracy -- about the same as guessing -- when shown 100 training examples. Two of the newer pre-trained models, BERT and ULMFiT, adapted for sentiment analysis, are 85% accurate with the same 100 training examples.

![]({{ site.github.url }}/images/editor_uploads/2019-08-08-164145-NLP_Comparison.png)

Not only do the pre-trained models work with fewer labeled examples, they also work better than the older technologies at any number of labeled examples.

These models permit higher performance with less data, and they are easier to use than traditional deep learning models, so they don't require a data scientist with NLP specialization. They can also be adapted to any natural language task. Here are some examples of these tasks:

* Customer service - routing customer phone calls or emails based on what the customer says they want
* Marketing - understanding public opinions about a new product or service
* Document management - understanding what information a document contains
* Search - looking for answers to questions within text data

So with pre-trained models and transfer learning, many NLP tasks are now accessible without a great deal of labeled data, without a NLP PhD on staff, and without special computer hardware.  This opens up dozens of possibilities.

---

## Upcoming Events

* Victor Dibia will be speaking on *"Handtrack.js: Building Gesture Based Interactions in the Browser Using Tensorflow.js"* at the [GDG Rochester DevFest](https://www.meetup.com/GDG-Rochester/events/262710495/) event on September 7th; he'll also be presenting on this topic at the [Strata Data Conference]((https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/77389)  in NYC on September 26th, and at the [TensorflowWorld](https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/77833) Conference on October 31st.
* Alice Albrecht will be speaking at the [Activate Conference](https://www.activate-conf.com/) in Washington, D.C. on September 11th.
* Shioulin Sam will be speaking on [Learning with Limited Labeled Data](https://conferences.oreilly.com/strata/strata-ny/public/schedule/speaker/313248) at the Strata Data Conference in New York City on September 25th.
* Chris Wallace is speaking on Federated Learning at [MCubed](https://www.mcubed.london/sessions/federated-learning/) in London on October 1st.
* Shioulin Sam and Ryan Micallef will both be speaking at [PyGotham](https://2019.pygotham.org/) in New York City on October 4th/5th.
* Ryan Micallef is also speaking on Natural Language Processing with Transfer Learning at [AI Dev World](https://aidevworld.com/) in San Jose on October 9th.

If you're attending any of these events, please let us know - we'd love to say hello!   Reach out anytime to [cffl@cloudera.com](mailto:cffl@cloudera.com).

All the best,

The Cloudera Fast Forward Labs Team