---
layout: newsletter
slug: 2018-03-28-client
---

Hello!

When you look under the hood of neural networks, you see arrays of numbers, or tensors. How exactly do these numbers relate to the network's input - an image of an adorable Labrador puppy, let's say, skeptically eying a cute little kitten? And how exactly do these numbers lead to the network's response: `a dog` and `a cat`. (Remarkable, and yet without some of the nuances a human observer might add, if one might humbly say so.) 

In [*The Building Blocks of Interpretability*](https://distill.pub/2018/building-blocks/), researchers from Google and CMU set out to provide answers to these questions. Applied to images, their methods allow us to look at and make sense of what neurons see — strange, psychedelic distortions of floppy ears and soft fur. Their methods likely generalize beyond images to audio files and text documents. It's just, averaged words are harder to make sense of. In *The Building Blocks of Interpretability*, images are chosen for pedagogical reasons.

We covered *The Building Blocks of Interpretability* in a [recent newsletter](http://blog.fastforwardlabs.com/newsletters/2018-03-14-client.html). We recommend reading the article and playing with the interactive visualizations provided by the authors to get a feel for neural activity patterns and what they might mean. What we'd like to add — we are intrigued by similarities in the authors' approach to approaches in neuroscience. 

Within neuroscience, we've been studying squishy, biological brains for quite some time with a heavy emphasis on the understanding of neuronal activity in response of controlled input and as it relates to observable output. Within the neuroscience community, researchers sometimes wonder if they have the right concepts and vocabulary to adequately describe the brain. Our brain evolved over time; so did our language abstractions. Does this guarantee sufficient alignment of biological structure and conceptual apparatus?

Maybe. In *The Building Blocks of Interpretability*, the authors outline techniques to map human concepts to neural dynamics. Intriguingly, [related work](https://arxiv.org/pdf/1801.03454.pdf) showed that semantic concepts may map to multiple neural network responses and that neural network responses encode multiple concepts, a less than neat overlap of neural dynamics and conceptual apparatus. Perhaps it's just the oddity of our artificial brains and their limited exposure to the world. Perhaps the easier-to-study (and cheaper) nature of silicone enabled brains will allow us to answer questions pondered within neuroscience. From biological to silicone brains, and back.

---

## Tech giants and the changing landscape of personal data usage and privacy

You’ve likely seen a lot of press recently about Facebook allowing Cambridge Analytica to abscond with [50 million users’ personal data](https://www.wired.com/story/wired-facebook-cambridge-analytica-coverage/). The company’s stock has slid 14%, a "delete Facebook" campaign has breezed through the collective social conscious, and a lot of discussion has been raised about regulation - whether in the form of government action or (more likely) new industry [self-guided checks](https://digiday.com/media/googles-gdpr-consent-plan-template-tech-giants/) and declarations on how such companies might move forward. 

The other tech giants, [Apple](https://www.google.com/search?q=apple+stock&ie=utf-8&oe=utf-8&client=firefox-b-1-ab), [Amazon](https://www.google.com/search?client=firefox-b-1-ab&ei=nMa5WofLKtLSjwONzpSACQ&q=amazon+stock&oq=amazon+stock&gs_l=psy-ab.3..0i131i67k1j0i131k1j0l3j0i131k1j0l2.63226.63992.0.64972.5.5.0.0.0.0.154.452.0j3.3.0....0...1..64.psy-ab..2.3.452...0i7i30k1.0.GXs3ItXK50c), and [Google](https://www.google.com/search?client=firefox-b-1-ab&ei=3sa5WrC5OpmWjQP37ImwCw&q=google+stock&oq=google+stock&gs_l=psy-ab.3..0i131i67k1j0i131k1l2j0l2j0i67k1j0l2.65304.65970.0.66292.6.6.0.0.0.0.186.496.0j3.3.0....0...1.1.64.psy-ab..3.3.496...0i7i30k1.0.dESRK_waf1Y) have also seen their stocks dip due to regulatory concern (and begin to rebound), but there isn’t the same level of public uproar over violation of privacy with these other companies... yet. While these companies aren’t tied directly into social behaviors to the same degree as Facebook is, they each have the capacity to extract just as much personal profile knowledge - if not more - from the data resources available to them. Facebook has social relationship data, website pixel tracking, and can make predictions of what users might want to buy, and (as shown by the scandal) potentially influence those purchases. Meanwhile, Amazon tracks the true shopping behavior of consumers, their friends, and neighbors and doesn’t need to guess at that first level of interest. The company’s understanding of real behaviors is only growing - Amazon wants to be your home security, your [grocery store](https://www.amazon.com/b?ie=UTF8&node=17235386011), your [health insurance](https://www.cnbc.com/2018/01/30/amazon-berkshire-hathaway-and-jpmorgan-chase-to-partner-on-us-employee-health-care.html), potentially your [bank](http://money.cnn.com/2018/03/05/investing/amazon-jpmorgan-chase-banks-checking-accounts/index.html), certainly your [movie theater](https://www.amazon.com/Movies/b?ie=UTF8&node=2858905011) (and [producer](https://studios.amazon.com/)) and even your [joke-teller](http://www.businessinsider.com/eleven-jokes-that-show-off-amazon-alexas-sense-of-humor-2017-10): [Maslow](https://www.simplypsychology.org/maslow.html), bottom to top. The company will be positioned to use data across users’ entire spectrum of life to achieve such a fundamental understanding of customers that they’ll be able to target and incentivize loyalty well beyond what current ad tracking may produce. 

Apple and Amazon had previously set themselves apart from Facebook and Google by being more reliant on real-world product delivery for their customers than [focused on ad revenue](http://www.wired.co.uk/article/big-four-data-facebook-google-apple-amazon-privacy). That prospect is beginning to change - for both groups. Amazon’s share of the ad game [is growing](https://mindstreammedia.com/tldr-roundup-facebook-data-controversy-googles-e-commerce-move-amazons-rising-value/) while Google (and Facebook) are also expanding in real world [products](https://www.reuters.com/article/us-google-retail-exclusive/exclusive-where-can-i-buy-google-makes-push-to-turn-product-searches-into-cash-idUSKBN1GV0B0) and [services](https://techcrunch.com/2017/09/28/facebook-partners-with-ziprecruiter-and-more-aggregators-as-it-ramps-up-in-jobs/) - though Amazon is still well ahead in the products space. 

So what does all of this mean? 
* The competitive landscape is consolidating. (For example, Amazon’s likely entry into the health insurance space is being matched by the merger of [Cigna and Express Scripts](https://www.cnbc.com/2018/03/08/amazon-entry-into-health-care-narrows-with-cigna-express-scripts-deal.html), while the integration of CVS and Aetna (and [the value of their data](https://www.washingtonpost.com/news/wonk/wp/2018/03/26/cvs-aetna-wants-be-in-your-neighborhood-because-zip-codes-powerfully-shape-peoples-health/?utm_term=.b1f3e2751a32)) has already been on the table. Meanwhile Google has also been expanding in both [healthcare](https://www.healthcaredive.com/news/alphabets-verily-seeks-collaboration-with-health-insurers/518046/) and [digital media](http://adage.com/article/digital/google-sweeten-publisher-deals-tech-woos-media/312824/), and expanding its [ML](https://cloud.google.com/solutions/media-entertainment/) to benefit the space. 
* The most competitive players in each market are going to be leveraging data from outside their traditional markets. 
* Algorithms for Multi-task learning, Federated Learning, and [Semantic Recommendation systems](https://arxiv.org/pdf/1709.09973.pdf) (our own research on this can be found [here](http://blog.fastforwardlabs.com/2018/01/22/exploring-recommendation-systems.html) and [here](https://www.fastforwardlabs.com/research/FF07)) are going to be increasingly vital to maintaining competitive edges with the expanded data. 
* The privacy landscape is going to continue to twist and develop in the US and other parts of the world. Data product models in the face of the larger data sets will need to be [interpretable](http://blog.fastforwardlabs.com/2017/09/11/interpretability-webinar.html) and will need to be able to adapt to avoid penalties. (The penalty for a [GDPR violation](https://www.gdpreu.org/compliance/fines-and-penalties/) for any company using EU citizen data without permission is €20 million, or 4% of the worldwide annual revenue of the prior financial year, whichever is higher.)
* While “knowing the customer” stopped being qualitatively measureable years ago, in the market ahead, “knowing everything about the customer" will become the norm. However, the question of appropriate personal data usage is far from settled in any industry, and the brands most likely to survive the next wave of consumer outrage are going to be the ones most focused on delivering value for their customers - and the planet - instead of just for their partners. 

---

## Learning to active learn

Training data is the foundation of machine learning - but because of its
expense, it is often the stumbling block too. Active learning is a technique
that aims to reduce the time and cost of acquiring that training data. Until
recently, active learning involved arbitrary and sub-optimal heuristics. But,
by incorporating reinforcement learning in [Active One-shot
Learning](https://arxiv.org/abs/1702.06559), Woodward and Finn are able to get
rid of the heuristics and make the application of active learning much more
natural.

The basic idea behind all forms of active learning is very simple, and best
explained with a short example.

Imagine a patient arrives at a hospital with symptoms of pneumonia. The doctor
can either diagnose them by applying a machine learning model to the patient's
basic symptoms, or they can perform a full examination. The machine learning
diagnosis is quick and cheap, but it might be wrong. The examination is slow
and expensive, but is guaranteed to be correct. Which technique should the
doctor use?

If the machine learning model gives not just its diagnosis but also a measure
of its confidence, then a strategy becomes clear. If the model is almost
certain, go with its quick decision - otherwise, conduct the examination. If
the results of full examinations are used to re-train the model (to help it
become more confident the next time a patient presents with similar symptoms), then
we're doing _active learning_.

The devil is in the details, however. At what point should we trust the model?
How certain is almost certain? Traditionally, active learning sets an arbitrary
threshold. This might be in terms of confidence (e.g. "is the model at least
80% certain?") or in comparison to other samples (e.g. "is this one of the ten
pending classifications about which the model is most uncertain"), or a more
formal statistic (entropy, information gain, etc.). These approaches certainly
work, in the sense that they don't waste time expensively classifying _all_ the
data by hand. But the thresholds feel arbitrary, and it's certainly not
_optimal_ (particularly in our example). Perhaps we're manually classifying too many examples, or too few.

[Woodward and Finn's contribution](https://arxiv.org/abs/1702.06559) (presented
at NIPS 2016 and explained clearly in this [short
video](https://www.youtube.com/watch?v=CzQSQ_0Z-QU)) is to get rid of these
heuristics by _learning_ a policy for whether to manually label the example.
They do this using reinforcement learning. This allows them to set the cost of
manual labeling (or mistakes by an overconfident model), in actual dollar terms
that a non-technical user knows, rather than arbitrary confidence thresholds
that a data scientist must pull out of thin air.

The end result is an end-to-end system that is seeded with a handful of manual
classifications, then rapidly becomes smarter, while asking an external labeler
for as little help as possible. We're very excited about the future of this
approach!

As a purely practical matter, reinforcement learning can be tricky to implement,
though, so there's plenty of life in the "classical" threshold-based active
learning approach yet ([reviewed in this great 2010
paper](http://burrsettles.com/pub/settles.activelearning.pdf) by Burr Settles).
If you're interested in exploring an active workflow, then we highly recommend
trying out [Prodigy](https://prodi.gy/) (especially the [live
demos](https://prodi.gy/demo?view_id=objectdet)). This web application
implements an active strategy and presents the manual learning component in a
thoughtful UX. It's a great tool for data scientists or end users.

---

## CFFL Updates

* * We're hiring for several positions on the Machine Learning Team at Cloudera; you can learn more [here](http://blog.fastforwardlabs.com/2018/03/21/join-the-machine-learning-team-at-cloudera.html).  If you know someone who would be a great fit for one of these positions, please do encourage them to apply!

* Mike will be speaking on interpretability at [Qcon.ai](https://qcon.ai/) on April 11th in San Francisco.

* Hilary will be speaking at the [MIT Digital Economy Conference 2018](http://mitsloan.mit.edu/alumni/events/2018-new-york-ide-conference/) on April 27th in New York.

* Friederike will be speaking at the [Data Science Salon](https://www.eventbrite.com/e/data-science-salon-nyc-tickets-40072527007) on applying AI and Machine Learning To Media and Entertainment on June 19th in New York.

* Friederike will also be speaking at the [Research & Applied AI Summit](https://raais.co/) in London on June 29th.

As always, thank you for reading. We welcome your thoughts, feedback, and suggestions; please drop us a note anytime at clients@fastforwardlabs.com.

Until next week,

The Cloudera Fast Forward Labs Team
