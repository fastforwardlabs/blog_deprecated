---
slug: 2019-03-27-client
layout: newsletter
---

Hello! In this week's newsletter, we highlight our new research on [Learning with Limited Labeled Data](https://blog.fastforwardlabs.com/2019/03/20/learning-with-limited-labeled-data.html), and take a deep dive into ML Experimentation with Tensorflow Hub.

---
## Machine Learning with Limited Labeled Data

Our latest research report and prototype (which will be released next week!) explores learning with limited labeled data. This capability relaxes the stringent labeled data requirement in supervised machine learning and opens up new product possibilities. It is industry invariant, addresses the labeling pain point and enables applications to be built faster and more efficiently. 

Please join us on Wednesday, April 3rd at 10:00am PT / 1:00pm ET for a webinar introducing the report.  The webinar will be hosted by Shioulin Sam and Nisha Muktewar, with special guests Ines Montani (co-founder of Explosion AI) and Sanjay Krishnan (Assistant Professor of Computer Science at the University of Chicago).  You can register [here](https://www.cloudera.com/about/events/webinars/learning_with_limited_labeled_data.html?src=FFL) - and be sure to check out our latest blogpost on [Learning with Limited Labeled Data](https://blog.fastforwardlabs.com/2019/03/20/learning-with-limited-labeled-data.html), too.

---

## Bootstrapping ML Experimentation with Tensorflow Hub
_by [Victor](https://twitter.com/vykthur)_


As a data scientist, being able to establish good experiment baselines in a manner that is fast, reliable, and reproducible is always useful. One approach that I certainly find helpful is to experiment with applying pretrained models or fine-tuning them using data available for a specific task. [Tensorflow Hub](https://www.tensorflow.org/hub) (TF Hub) - a library for reusable machine learning modules - can be a valuable resource for these types of experimentations. Each entry in TF Hub is referred to as a module, which is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks. The modules can be used _as is_ (e.g., for feature extraction) or finetuned through the process of [transfer learning](https://blog.fastforwardlabs.com/2018/09/17/deep-learning-is-easy-an-introduction-to-transfer-learning.html).

### Image Feature Extraction Example

In the following example, I walk through code snippets for loading the `InceptionV3` [TFHub module](https://tfhub.dev/google/imagenet/inception_v3/classification/1) for image classification and using one of its early layers as a feature extractor. Consider the use case of automatically classifying thousands of (unlabelled) images of scanned legal documents into given categories (government documents, internal company documents, external customer documents). Given that this dataset may contain distinctive visual characteristics for each category (e.g., logos, colors, layout, fonts, etc.), there may be opportunity to adopt unsupervised methods to automatically cluster similar documents. To achieve this, a good feature extractor is needed to generate vectors which can be used for clustering and similarity comparison. 

TF Hub simplifies the process of loading and applying modules via methods encapsulated in the `tensorflow-hub` [python library](https://github.com/tensorflow/hub). Each module is accessed via a url and comes with a version number to ensure users can rely on consistent results across changes or updates to the module. In addition, a description of the module, the underlying network, dataset used for training, api with examples, and publisher are also provided. 



```shell
pip install tensorflow-hub 

```


```python
module_url = "https://tfhub.dev/google/imagenet/inception_v3/classification/1"
module = hub.Module(module_url) # load module
```

Once a module is loaded, it is cached on the local machine’s `temp` folder and can be called like a python function with tensor inputs and tensor outputs. Each module also provides a discoverable set of signatures (`module.get_signature_names()`) which defines any set of operations (think methods) that are supported by the module. Details on the expected input  and output can also be discovered using the (`module.get_input_info_dict()`)  and (`module.get_input_info_dict()`) methods respectively.  

```python
# Signatures and I/O specification for the Inception V3 classification module
print(module.get_signature_names())
print(module.get_input_info_dict())
print(module.get_output_info_dict())
print(module.get_output_info_dict(signature="image_feature_vector"))
```

```text
# results ..
['default', 'image_classification', 'image_feature_vector']
{'images': <hub.ParsedTensorInfo shape=(?, 299, 299, 3) dtype=float32 is_sparse=False>}
{'default': <hub.ParsedTensorInfo shape=(?, 1001) dtype=float32 is_sparse=False>}
{'InceptionV3/Mixed_5c': <hub.ParsedTensorInfo shape=(?, 35, 35, 288) dtype=float32 is_sparse=False>, 'InceptionV3/Mixed_5d': <hub.ParsedTensorInfo shape=(?, 35, 35, 288) dtype=float32 is_sparse=False>, … }
```

The snippet above shows that the `InceptionV3` classification module supports 2 signatures - `image_feature_vector` which allows it to be used for extracting feature vectors from an image and the default `image_classification` signature which allows for classifying the input image. The module expects an input tensor of shape `(?, 299, 299, 3)` and when used for image classification, provides an output of shape `(?, 1001)` corresponding to ImageNet labels. When the `image_feature_vector` signature is used, various output sizes are provided depending on the output layer that is selected. Note: `?` represents batchsize.

```python
# images = ...  # A batch of images with shape [batch_size, height, width, 3].
features = module(images)
outputs = module(dict(images=images), signature="image_feature_vector",as_dict=True)
features = outputs["InceptionV3/Mixed_5c"]  # get features from layer "InceptionV3/Mixed_5c"

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # print a vector containing extracted features
    print (sess.run(features)) 

```

In the above snippet, we assume variable `images` contains a tensor of images with shape `(batch_size, height, width, 3)`. We then extract features from all images in this tensor using layer `InceptionV3/Mixed_5c` in the InceptionV3 network. These extracted features can then be used as input to other downstream activities e.g clustering features, interactive visualization and exploration (tsne, umap), semantic similarity etc. In this example, we use the module _as is_ for our feature extraction task. In principle, we can also finetune the weights with our data for improved performance, especially for classification tasks. This is acheived by passing a `trainable=True` parameter when loading the module. Learn more about   [finetuning TF Hub modules here](https://www.tensorflow.org/hub/fine_tuning).


```python
module = hub.Module("https://tfhub.dev/google/imagenet/inception_v3/classification/1" trainable=True ) # load module for finetuning
```

<img src="images/tfhubsnippet.jpg" width="100%">

##### Screenshot of full code snippet running in Cloudera [CDSW](https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_overview.html).

 
### Notes on Tensorflow 2.0 and TF Hub Modules
 
 You will immediately notice that the example above is written using the Tensorflow graph execution paradigm (where `session.run` must be invoked to build the computation graph). For those who are embracing the more intuitive Tensorflow 2.0 paradigm  which enables eager execution by default, many of the current modules will not run out of the box. However, [work is being done](https://www.tensorflow.org/community/roadmap#reference_models) to create modules that support Tensorflow 2.0. [ 8 modules ](https://tfhub.dev/s?q=tf2) have been introduced which support TF 2.0, including mobilenet and inceptionV3 modules for classification. These modules are designed to follow the Keras api, are intuitive to use, and the user is encouraged to review them. They are also experimental at the moment and I expect that there may be slight changes in their TF Hub api in the near future. 

Overall, TF Hub provides a rich collection of modules that address tasks in the text, image and video domains. Currently, it contains 46 modules for text embeddings (including the [universal sentence encoder](https://arxiv.org/abs/1803.11175), [elmo](https://arxiv.org/abs/1802.05365), [word2vec](https://arxiv.org/abs/1301.3781)), 71 modules for image classification and feature extraction (including inception, resnet, mobilenet) and 2 models for video action recognition.  

--- 
## Upcoming Events

* Hilary Mason will be speaking at the [Marketing Analytics and Data Science Conference](https://marketing.knect365.com/marketing-analytics-data-science/) in San Francisco, CA on April 10th.
* Mike Lee Williams will be chairing the "deep learning in practice" track at [Qcon.ai](https://qcon.ai/) in San Francisco on April 16-17th.
* Chris Wallace will be speaking on _[Federated learning: machine learning with privacy on the edge](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74327)_ on May 1st at the [Strata Data Conference](https://conferences.oreilly.com/strata/strata-eu) in London.
* Shioulin Sam will also be speaking at Strata in London on _[Learning with Limited Labeled Data](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74341)_ on May 2nd.

We'll also be at the Dataworks conference in Washington, D.C. on May 23rd! Here are the talks we'll be giving:
* Hilary Mason: _[Building an Enterprise AI Factory](https://dataworkssummit.com/washington-dc-2019/keynote/building-an-enterprise-ai-factory/)_
* Tristan Zajonc: _[Cloud-Native Machine Learning: Emerging Trends and the Road Ahead](https://dataworkssummit.com/washington-dc-2019/session/cloud-native-machine-learning-emerging-trends-and-the-road-ahead/)_
* Nisha Muktewar: _[Learning with Limited Labeled Data](https://dataworkssummit.com/washington-dc-2019/session/learning-with-limited-labeled-data-2/)_
* Alice Albrecht: _[A Framework for Developing a Winning Data Project Portfolio](https://dataworkssummit.com/washington-dc-2019/session/a-framework-for-developing-a-winning-data-project-portfolio-2/)_
* Justin Norman and Sagar Kewalramani: _[Machine Learning Model Deployment: Strategy to Implementation](https://dataworkssummit.com/washington-dc-2019/session/machine-learning-model-deployment-strategy-to-implementation-3/)_

If you're attending any of these conferences, please [let us know](mailto:cffl@cloudera.com) and stop by to say hello! 

All the best,

The Cloudera Fast Forward Labs Team