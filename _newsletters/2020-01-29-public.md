---
slug: 2020-01-29-public
layout: newsletter
---

## Introducing Anomagram — An Interactive Visualization of Autoencoders, Built with Tensorflow.js

_by [Victor](https://victordibia.github.io/anomagram/)_

Across many business use cases that generate data, it is frequently desirable to automatically identify data samples that deviate from “normal.” In many cases, these deviations are indicative of issues that need to be addressed. For example, an abnormally high cash withdrawal from a previously unseen location may be indicative of fraud. An abnormally high CPU temperature may be indicative of impending hardware failure. 

The task of finding these anomalies is broadly referred to as Anomaly Detection, and many excellent approaches have been proposed (clustering-based approaches, 
nearest neighbors, density estimation, etc.). However, as data become high dimensional, with complex patterns, existing approaches (linear models which mostly focus on univariate data) can be unwieldy to apply. For such problems, deep learning can help.

In [a recent post on Medium](https://medium.com/@victor.dibia/anomagram-433d00db7261), I introduced [Anomagram](https://victordibia.github.io/anomagram/#/), an interactive visualization of how autoencoders can be applied to the task of anomaly detection. Anomagram is created as both a learning tool, and a prototype example of what an ML product interface could look like. The interface is built with Tensorflow.js and allows install-free experimentation in the browser. 

The first part of the interface introduces important concepts (autoencoders, data transformations, thresholds, etc.) paired with appropriate interactive visualizations. The second part (pictured below) is geared towards more technical users and allows you to design, train, and evaluate an autoencoder model entirely in the browser. 

![]({{ site.github.url }}/images/editor_uploads/2020-01-03-210806-Anomagram.gif)
##### **Train a Model Module**: Anomagram provides a direct manipulation interface that allows the user to specify a model (add/remove layers and units within layers), modify model parameters (training steps, batchsize, learning rate, regularizer, optimizer, etc), modify training/test data parameters (data size, data composition), train the model, and evaluate model performance (visualization of accuracy, precision, recall, false positive, false negative, ROC, etc. metrics) as each parameter is changed. The task is anomaly detection within ECG signal data.

If you're interested in learning more about other deep learning approaches to anomaly detection, my colleagues and I will cover additional details on this topic in our upcoming report on Deep Learning for Anomaly Detection. We'll keep you posted on the release date and webinar registration in future editions of this newsletter.

In the meantime, you can read my full article [on Medium](https://medium.com/@victor.dibia/anomagram-433d00db7261), view the full demo of Anomagram [here](https://victordibia.github.io/anomagram/), and find the project source code [here](https://github.com/victordibia/anomagram).

---
## A Symbiotic Relationship: Knowledge Graphs & Machine Learning

*by [Andrew](https://twitter.com/andrew_reed_r)*

For the past decade, humans have unknowingly come to depend on Knowledge Graphs on a daily basis. From personalized shopping recommendations to intelligent assistants and user-friendly search results, many of these accepted (and expected) features have come to fruition through the exploitation of knowledge graphs. Despite their longstanding conceptual and practical existence, knowledge graphs were just added to the *Gartner Hype Cycle for Emerging Technologies* in 2018 and have continued to garner attention as an area of active research and development for their distinct ability to represent real-world relationships.

In this article, we'll take a high-level look at what knowledge graphs are and explore a few ways they interact with the field of machine learning.

### What is a knowledge graph?

With all the hype comes confusion. In its simplest form, a knowledge graph is a set of data points linked by relations that describe a real-world domain. A cursory Google search will result in a myriad of explanations, but I believe there are a few core concepts that characterize a knowledge graph implementation. 

![cypher_graph_v1]({{ site.github.url }}/images/2020/01/cypher_graph-1579272372494.png)

[***Image Credit***](https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/cypher_graph_v1.jpg)

- **It's a graph -** Contrary to traditional data stores, knowledge graphs are composed not only of entities, but also connections between each entity. In a graph network, these entities are called ***Nodes*** or ***Vertices*** and are connected together via ***Edges*** or ***Links***. Graph data structures excel at modeling one-to-many relationships.
- **It provides context -** Knowledge graphs glean semantic meaning by design - namely, the *meaning of the data* is implicitly encoded in the data representation itself, making it easy to query and explore. In the example above, we can quickly interpret that ***Jennifer*** is a ***Person*** who ***works for*** a ***Company*** called ***Neo4j*** because of the inherent directional metadata structure.
- **It's intelligent -** Knowledge graphs are built from dynamic, logical constructs ***- ontologies -*** that by default possess a framework supportive of inference. Regardless of the specific entities in the graph, the entity-to-entity connections hold fundamental meaning.

#### A Familiar Example

A concrete and relatable application of knowledge graph technology is demonstrated by *Google's Knowledge Graph* which was launched in 2012 and has become a relied upon feature for all Google search users.

![image-20200115163856787]({{ site.github.url }}/images/2020/01/alex_ferguson-1579272142395.png)

When searching for a specific person, Google provides users a side panel that contains relevant information surrounding the entity/subject in the query. This quick insight is made possible by Google's Knowledge Graph - a pre-populated knowledge base of connected facts relating people, places, and things. Because the graph structure effectively represents this type of data by design, the facts seen above can be easily called upon to provide contextual insight.

### Intersection of Machine Learning and Knowledge Graphs

Now that we have established a baseline intuition of what knowledge graphs are, let's take a look at ways machine learning and knowledge graphs support each other.

#### Getting knowledge into a knowledge graph

Because knowledge graphs preserve relational information (and are therefore more complex than traditional data representations), the data they take in demands a more refined state. Specifically, the edges between nodes must be established and then wrangled into a complementary form *before* populating a graph. 

![image-20200114132132253]({{ site.github.url }}/images/2020/01/manual_graph-1579272546736.png)

Let's imagine a hand-crafted graph describing characteristics of Sir Alex Ferguson as seen above. Defining these entities and relationships is a simple endeavor for anyone knowledgeable of the English Premier League (EPL) and organizing the connections upfront allows the graph to be efficiently queried later on. But what happens if we want to create subgraphs for every manager in the EPL? Or every soccer manager in the world? Or every professional sports manager that ever existed?

Manually identifying all of these relationships by hand is not scalable. This is where machine learning and Natural Language Processing (NLP) offer intelligent solutions to automatically curate raw data into useable facts. The general techniques involved include sentence segmentation, part of speech tagging, dependency parsing, word sense disambiguation, entity extraction, entity resolution, and entity linking applied to corpuses of both structured and unstructured data.

![image-20200114162031150]({{ site.github.url }}/images/2020/01/joe_burrow-1579272452103.png)

The simplified example above is intended to highlight the general NLP process on a single sentence. In practice, organizations use more advanced, [patented systems](https://searchnewscentral.com/blog/2018/10/19/constucting-knowledge-bases-with-context-clouds/) built on these underlying techniques to automatically extract information, resolve conflicting entities, and populate millions of entities into production knowledge graphs.

#### Getting *richer* knowledge out of a knowledge graph

"Increasingly we're learning that you can make better predictions about people by getting all the information from their friends and their friends' friends than you can from the information you have about the person themselves."

​														- James Fowler, *[Connected](https://www.amazon.com/dp/B002OFVO5Y/)*

The quote above poses a justified, but unconventional approach to predictive modeling. Traditional machine learning focuses on modeling tabular data that inherently cannot represent all of the cascading relationships found within networks and knowledge graphs. This often means data scientists are left trying to abstract, simplify, and even leave out predictive relationships baked into a knowledge graph's structure. But what if features of every node in a knowledge graph could be derived from the context of all the nodes and edges around them?

There are few different methods for making use of connected features in machine learning, but a main area of attention is ***Knowledge Graph Embeddings (KGE)***. The goal of KGE's is to learn a fixed vector space representation of any given node in a graph based on its nearby connections. Drawing a quick parallel to the ***Word2Vec*** algorithm (and concept of word embeddings) - where we learn a fixed vector representation for every word in a corpus based on nearby words - helps to frame the concept of KGEs. Specifically, the ***[Node2Vec](https://snap.stanford.edu/node2vec/)*** model expands upon ideas from Word2Vec by first randomly traversing subgraphs for each node in a network to build a large number of sequences [sentences]. Once we have a body of graph sequences [corpus], we can utilize Word2Vec methodology as it applies to text sequences to produce graph node embeddings.

Ultimately by learning embedding representations from the full context of a knowledge graph, we can extract deeply rich features to be used in downstream tasks. A few uses are:

- **Link prediction** - Can we find nodes that are likely connected or are about to be connected? (For example, a graph of *products* and *customers* connected by *orders* could be used to predict (and thus recommend) which new products should likely be connected to which new customers.)
- **Supervised modeling** - Embeddings can be fed as input features to supervised models for classification tasks.

![image-20200114175148278]({{ site.github.url }}/images/2020/01/better_ml-1579272303642.png)

[***Image Credit***](https://go.neo4j.com/rs/710-RRC-335/images/Neo4j-ai-graph-technology-white-paper-EN-US.pdf?_ga=2.7474418.156179876.1578945914-1645584309.1578945914)

### Final Thoughts

Knowledge graphs are an effective tool for modeling interconnected, real-world scenarios while retaining contextual details that are not easily captured with traditional data structures. In this article, we explored two examples that demonstrate the symbiotic relationship between knowledge graphs and machine learning, which only scratches the surface of the intersection between the two technologies. Additional concepts - like [Graph Neural Networks](https://blog.fastforwardlabs.com/newsletters/2019-10-24-client.html) and ML driven Entity Resolution - stand as exciting areas of research and application.

---
### Recommended Reading

In keeping with our reputation as your data nerd friends, here's a quick peek into what we've been reading lately:

#### [How front-end development can improve Artificial Intelligence](https://explosion.ai/blog/how-front-end-can-improve-ai). 

This article from 2016 by our friend Ines at explosion.ai is still very much valid today. The role of front-end in data science is often restricted to visualization and dashboards. This is an enormous lost opportunity. It’s a personal resolution of mine to work more on interfaces for using and understanding machine learning systems in 2020. -   *[Chris](https://twitter.com/_cjwallace)*

#### [Anti-Overfitting Techniques](https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323)

This catalog of overfitting problems in ML models is both a pre-flight checklist for models, and a set of recipes to mitigate overfitting. It's directed specifically to deep learning, but it's applicable to other types of models as well.  - *[Ryan](https://twitter.com/MicallefEsq)*

#### [ORL: Reinforcement Learning Benchmarks for Online Stochastic Optimization Problems](https://arxiv.org/abs/1911.10641)

DRL shows promise for real world problems! Amazon applied DRL (via OpenAI Gym) to canonical operations research/supply chain problems such as bin packing, newsvendor and vehicle routing. They find that DRL beats or matches baseline. Next step - can this work for real world instances of these problems? (They think not yet).  - *[Shioulin](https://twitter.com/shioulin_sam)*

#### [NLP's Clever Hans Moment has Arrived](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)

This article questions the value of benchmark datasets for evaluating the true performance of NLP models. Some models may be exploiting shortcuts to obtain excellent scores while failing at the core of the task - in this case, reasoning and comprehension. - *[Victor](https://twitter.com/vykthur)*

#### [Why are so many AI systems named after Muppets?](https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie) 

What may have started out as a bit of a joke actually highlights the importance of collaboration and respect within the open-source community towards the development of ML/AI products - characteristics that, as this article points out, are something Sesame Street fosters. One of my favorite excerpts from this article: "AI isn’t a discipline where lone scientists toil away in the lab at night, pumping electricity through processors, and cackling “It’s aliiiive” over a glowing command line. (Disclaimer: this certainly does happen, but it’s not always the most productive approach.)" - *[Danielle](https://www.linkedin.com/in/daniellethorp/)*

#### Work from Everest Pipkin's Data Gardens Class
https://twitter.com/everestpipkin/status/1210636168242614274

Everest Pipkin provides a behind-the-scenes look at a creative coding class they recently taught. The assignments are all interesting and the student work looks great. I especially liked the "folder structure as memory palace" prompt. - *[Grant](https://twitter.com/GrantCuster)*

#### [Manifold](https://github.com/uber/manifold)

Uber recently released a visual debugging tool for machine learning - Manifold. It is a model monitoring and debugging tool which compares feature distributions across tabular data subsets. It is model agnostic and helps users determine what data slices a model fails on and the potential causes for certain performance issues. It also integrates with Jupyter Notebook. It will be interesting to watch this space and how the features for the subsequent versions of Manifold unfold! - *[Nisha](https://twitter.com/NishaMuktewar)*

#### [code2seq](https://code2seq.org/)

A really interesting project from 2019 called code2seq introduced a method for generating natural language sequences from the structured representation of source code. This research sheds opportunity for automated code documentation and summarization. - *[Andrew](https://twitter.com/andrew_reed_r)*

#### [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

Brand new on arXiv this week, "Reformer: The Efficient Transformer" shows how old dogs can still learn new tricks. The authors reimplement the now-standard Transformer architecture (first brought to fame in the BERT NLP model) using Locality Sensitive Hashing, a long-standing tried-and-true technique for efficient look-up of similar items. This reduces the complexity of the algorithm and allows for longer sequences (e.g., sentences) to be used successfully.  I love seeing classic techniques reinvented in modern algorithms! - *[Melanie](https://www.linkedin.com/in/melanierbeck/)*

---

## Upcoming Events

* Victor Dibia and Nisha Muktewar will be hosting a webinar on [Deep Learning for Anomaly Detection](https://www.cloudera.com/about/events/webinars/deep-learning-for-anomaly-detection.html?utm_source=blog&utm_medium=organic&utm_term=ml&utm_campaign=CFFL12_Report_AMER_Webinar_2020-02-13&cid=7012H000001OYfQ&utm_content=FFL) on February 13th, in conjunction with the launch of our newest research report. [Register today!](https://www.cloudera.com/about/events/webinars/deep-learning-for-anomaly-detection.html?utm_source=blog&utm_medium=organic&utm_term=ml&utm_campaign=CFFL12_Report_AMER_Webinar_2020-02-13&cid=7012H000001OYfQ&utm_content=FFL)
* Victor and Nisha will also be presenting on Deep Learning for Anomaly Detection at the [Strata Data Conference](https://conferences.oreilly.com/strata-data-ai/stai-ca/public/schedule/detail/80421) in San Jose on March 18th.