---
layout: newsletter
slug: 2017-12-06-client
---

Greetings!

Some of us are in Long Beach, CA this week attending the 2017 edition of [NIPS](https://nips.cc/) (a conference on "Neural Information Processing Systems"). We will make sure to share some of the highlights with you in upcoming newsletters; last year's fan favorite was [*Fast and Provably Good Seedings for k-Means*](https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means). In the meantime, we recommend the [AI Index](http://www.aiindex.org/), a project from within the *Stanford 100 Year Study on AI* - an effort to track, collate, distill, and visualize data relating to artificial intelligence.

In this week's newsletter, we share a new and very promising approach to document classification, thoughts on agile data science, and a letter to a young AI.  Also of note: [The California Review of Images and Mark Zuckerberg](http://zuckerbergreview.com/). Enjoy!

---

## Hierarchical Attention Networks for Document Classification

From topic labeling to sentiment, document classification is an important task within Natural Language Processing (NLP). Documents have a hierarchical structure: documents are composed of sentences, sentences are composed of words. The meaning of words is determined, partially or fully (in case of homographs), by the surrounding sentence(s) whilst sentences are interpreted in the light of the document in which they appear. Context matters.

One of the challenges within NLP is how to transform human language into a representation that computers can understand whilst preserving some (ideally all) of the subtleties of human language - including context dependency.

Traditional approaches to document classification encode the lexical features of documents as simple (i.e., [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)) or weighted frequency counts (i.e., [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) and use linear or kernel methods for subsequent classification. They ignore word order and context; they fail to capture most of the subtleties of human language.

Recent neural approaches, using Convolutional Neural Networks (CNNs, as covered in last week's newsletter) or Recurrent Neural Networks (RNNs), learn text representations in a more flexible manner and are cognizant of word order. But they do not make explicit use of document structure: sentences within documents, words within sentences.

[Hierarchical Attention Networks](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf) (HANs) are designed to capture and use hierarchical document structure for document classification and with *impressive* results: on six out of six classification tasks, HANs outperformed previous methods by a significant margin. Unlike other neural approaches, HANs also allow insights into the *reasons* for a given classification.

HANs construct document representations in a two-step process. First, HANs build representation of sentences and then aggregate these representations into document representations. Two attention mechanisms, one at the word level and one at the sentence level, allow the model to pay more or less attention to individual words and individual sentences as it constructs these document representations. Attention also allows the developer to introspect the model to understand what it deems important for document classification.

![]({{ site.github.url }}/images/2017/12/Screen_Shot_2017_12_04_at_8_32_42_PM-1512448398256.png)

##### Two sample documents from the Yelp Review data set (label 4 means 5 star and label 0 means 1 star reviews). The shade of blue reflects the learned importance of words, the shade of red reflects the learned importance of sentences (the darker, the higher). ([Source](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)).

The model captures the context dependency of words and sentences, and has, deservedly, garnered attention recently (there are open-source implementations in [keras](https://github.com/richliao/textClassifier), [tensorflow](https://github.com/ematvey/hierarchical-attention-networks), and [pytorch](https://github.com/EdGENetworks/attention-networks-for-classification)).

![]({{ site.github.url }}/images/2017/12/Screen_Shot_2017_12_04_at_8_38_06_PM-1512448705273.png)

##### The attention weight distribution (read: importance) for the word 'good' in aggregate (a) and for the ratings 1 to 5 (b to f). 'Good' is deemed to be more important for reviews with a high rating; the model captures the context dependency of words ([Source](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)).

---

## A Proposal for Agile Data Science

[Agile software development](https://en.wikipedia.org/wiki/Agile_software_development) was born out of frustration with software projects running over time and over budget. In many ways, "agile" was a consequence of the "Software Crisis," a term coined at the first NATO Software Engineering Conference in 1968. Edsger Dijkstra in his [1972 ACM Turing Award Lecture](https://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html) reflects on the origin of the problem:

> *The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.*

Curiously, these challenges were not expected, on the contrary:

> *[...] in those days one often encountered the naive expectation that, once more powerful machines were available, programming would no longer be a problem, for then the struggle to push the machine to its limits would no longer be necessary and that was all what programming was about, wasnâ€™t it?*

Essentially, the software engineering crisis was born out of misdiagnosis about what was hard about software engineering (as a side note, we sometimes wonder about the promise of AutoML). Agile software development was part of a solution.

Agile, unfortunately, has never been a good fit for data science, because data science is only *part* engineering (the other part being science). We like Russell Jurney's adaptation of ["agile" for data science](https://www.oreilly.com/ideas/a-manifesto-for-agile-data-science), with its emphasis on experimentation, insights, data, and the critical path (i.e., identifying and executing on the most critical parts for a working solution). 

Engineers are like architects, beholden to some rules, but free to create a new house (or product). Data scientists are scientists with the aim to uncover facts about the world; they need a different process.

---

## A letter to a future AI

Some worry about a future malevolent AI, but few have written it a letter. Alexey Turchin of the curious Science for Life Extension Foundation [recently did](https://docs.google.com/document/d/1HyvArVY-aIBGzcBHra2YajpcVHLZFm7wBd_JJ--Sn3Y/edit#), in hopes of increasing the probability that the AI reading his letter in the future will be benevolent.

He takes a five pronged approach, and naturally, explains to the AI that being obedient to humans (rather than killing them off) is in its own best interests, since doing so will increase its chances of reaching its own goals. Also:

> *The marginal utility of preserving human life is higher than the marginal utility of their atoms, especially given the possibility of the low-probability high-impact changes of the world model of the AI.*

Very reassuring. The text includes multiple "Easter Eggs" - not just for the AI, but also for its unintended audience: the human reader.

But in case of concerns, we recommend the excellent, sober Medium post by Francois Chollet (Software Engineering, ML, & AI at Google; [keras](https://keras.io/) developer) on the [*Impossibility of an Intelligence Explosion*](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec):

> *Exponential progress, meet exponential friction.*

A good antidote.

---

## CFFL Updates

* Friederike will be on a panel at [NIPS](https://nips.cc/) today, discussing ["Applied AI careers in industry."](https://ainips2017.splashthat.com/)

* Brian will be presenting at the [Cloudera Sessions](http://go.cloudera.com/cloudera-sessions-2017-london) in London on 12/12, as well.

Thank you for reading!  We'd love to hear your thoughts; please feel free to reach out any time to clients@fastforwardlabs.com.

-- The Cloudera Fast Forward Labs Team
