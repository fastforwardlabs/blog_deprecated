---
slug: 2019-05-23-client
layout: newsletter
---

## Prototyping FF11
_by [Grant](https://twitter.com/grantcuster)_

We’re hard at work on our 11th report, which will be focused on transfer learning for natural language processing. With every report, we build a prototype. The goal of the prototype is to both demonstrate the technology and to spark people’s imaginations about possible product applications. Here are some snapshots from our prototype-in-progress.

### Advances in NLP

With the availability of pre-trained models like [BERT](https://github.com/google-research/bert), we can build more accurate NLP models with less data. One of the most exciting parts about BERT is the ‘B’, which stands for bidirectional. Bidirectional means the model was trained so that when it analyzes a word, it takes into account what comes before and after it. This greater sense of context makes BERT a great starting point for a wide range of NLP tasks.

### Showing the capability

One of the main tasks of our prototype is to show off the advances in NLP as a capability. That means finding a dataset and a presentation where the increased contextual understanding provided by BERT can really shine. 

An early idea for our prototype focused on document classification. We took a set of business descriptions paired with a label for business type. We imagined a simulator where the user was first tasked with classifying each business by hand. After going through several examples they would be presented with an “AI Assistant” that would predict the classification for them. This idea was based on similar problems our clients have asked for help with in the past.

![]({{ site.github.url }}/images/editor_uploads/2019-05-24-160937-Screen_Shot_2019_04_19_at_2_29_26_PM_1555698642331.png)

##### Screenshot of the early document classifier prototype

The experience of the document classifier was solid, but we had reservations about its ability to show the capability. Because of the nature of the task, models that focus on keywords (without the BERT-provided sense of context) also did well on the business description dataset. We wanted to find something where the contribution of recent advancements was clearer.

We turned to sentiment analysis: classifying whether a writer’s opinion towards a topic is positive or negative. Keyword-focused approaches can get tripped up by sentiment analysis because reviews often contain concepts like negation (“This movie was NOT good”). To classify well, you need an approach that takes context into account — exactly the capability we wanted to highlight. We ran some experiments on a movie and TV review dataset to verify that BERT-based models performed substantially better than other approaches. When we were happy with the results (which you’ll be able to read about in detail in the finished report), we set about figuring out how to present them in the prototype.

### Product possibilities

Having selected the dataset, we now needed to figure out the presentational and storytelling aspects of the prototype. We want to make something that sparks people's imaginations about the kinds of things they can build. An early exploratory step we like to do is getting the data into a preliminary web app and just see how it feels to look at. In the case of this dataset, that meant presenting a review and a sentiment classification to the user. Doing this quickly made it clear that we’d have to give the user more information about what was going on. We turned to a technique from our earlier Interpretability prototype, using the [LIME](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) technique to give each sentence a score based on its contribution to the prediction. We combined those scores with a diverging color scheme to give the user more of a view into what the model is responding too.

![]({{ site.github.url }}/images/editor_uploads/2019-05-24-161227-Screen_Shot_2019_05_20_at_5_24_42_PM_1558387555601.png)

##### We're using LIME to visualize each sentence's contribution to the review classification.

LIME helped us make the model's predictions more understandable and interesting on an individual review basis, but we still needed a narrative about what was happening to draw the user in. One thing we found that helped was grouping the reviews by movie/TV show. By showing the user several reviews with the same subject matter, we are able to demonstrate more about how the model responds to different language. It also lets us tally up the overall sentiment for the movie across all of the reviews. This is one of the central product capabilities that more accurate NLP models unlock: the ability to go from unstructured text to quantifiable measurements.

![]({{ site.github.url }}/images/editor_uploads/2019-05-24-161418-Screen_Shot_2019_05_23_at_9_39_04_AM_1558619093755.png)

##### We're presenting the reviews within an imaginary message board to explain the eclectic selection.

Another storytelling challenge was how to explain the eclectic selection of movies and TV shows we are displaying. We decided we should present the different topics as a message board, which are often somewhat random in their focus. The message board presentation also provides a story for why all this data exists as unstructured text. Usually when posting to a message board, your only input option is a text box (compared to a movie review site, where a star rating capability is built-in). Even on a topic-focused movie review site, you may end up with walls of text, where the only way to get a sense of the general sentiment is to read through all of it. This is exactly the type of problem we believe NLP can tackle, so we are building the prototype as a tool of analysis layered on top of an imaginary review message board, so you can transform a wall of text into quantifiable sentiment and the highlights related to it.

### Work in progress

That’s a quick tour of where we are so far. Issues often come up so the final prototype may end up somewhere different. (You’ll have to wait for our report and prototype release to find out.) In the meantime, please [let us know](mailto:cffl@cloudera.com) if you have any really good message board name ideas!