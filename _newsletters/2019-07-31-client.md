---
slug: 2019-07-31-client
layout: newsletter
---

## Two approaches for data validation in ML production
by *[Shioulin](https://twitter.com/shioulin_sam)*

Machine learning models start to deteriorate once we deploy them. This is partly
because real life data changes, and models need to be re-trained to maintain
their performance. Typical ML pipelines re-train periodically (daily, for
example) using newly available data. But how do we validate data fed into the
pipelines to make sure tainted data does not accidentally sneak into the
production system? Tainted data could cause system crashes or lead to slow
degradation of model performance. The impact of the former is painful and
immediate; the impact of the latter is perhaps more dreaded. It's hard to
debug and isolate.

### Unit-tests for datasets

[Amazon research (PDF)](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf)
proposed a unit-test approach. The idea is to create a system that (i) allows
users to (easily) define constraints (or checks) on the data, (ii) converts these
constraints to computable metrics, and (iii) reports which constraints succeeded
and failed, and the metric value that triggered a failure.

![Example constraints [img src](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf)]({{ site.github.url }}/images/2019/07/amzn_constraint-1562965509938.png)
##### Example constraints ([image source](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf))

How does it work? Imagine some data as a set of log files generated by an
on-demand video platform. The log contains information about platform usage,
type of device, length of session, title, and customer id. This is ingested and
used as training data for recommendation systems. To validate this data using
the proposed system, the user defines a set of checks including (i) _completeness_
and _consistency_ (for example: *customerID* and *title* columns should have no
missing values), (ii) _uniqueness_ (for example: each row of combined *customerID*
and *title* value should be unique), and (iii) _counting_ (for example: number of
distinct values in the *title* column should be less than the total number of
movies in the system). Once the constraints are specified, the system converts
them into actual computable metrics. For example, _completeness_ would convert
into a "fraction of non-missing values in a column" metric. The last step is to
generate a report to show how all the constraints fared. The report also lists
the ones that failed, along with the value that triggered a failure.

![System architecture [img src](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf)]({{ site.github.url }}/images/2019/07/amzn_system-1562965579363.png)
##### System architecture ([image source](http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf))

Because new data comes in continously, the system uses a recursive computation
approach that only looks at _new data since the last time step_ to update the
metrics incrementally. In addition, to lower the barrier of adoption, the system
automatically suggests constraints for datasets. This is accomplished through
clever use of heuristics and a machine learning model. 

The heuristics approach employs single-column profiling - all the user needs to provide is a single
table dataset with column names to start. The system then executes single column
profiling in three passes; the goal is to figure out data size, data type,
summary statistics (min, max, mean, etc.) and frequency distribution of the
data. Based on the profiling results, the system recommends constraints based on
a set of heuristics. For example, if a column is complete, the system suggests
an _isComplete_ constraint. If the number of distinct values in a column is
below a threshold, the column is assumed to be categorical and the system
suggests an _isInRange_ constraint that checks whether future values are
contained in the set of already observed values. The heuristic approach is
complemented by a machine learning model. The model is trained to predict
constraints based on table name, column name, and type. Finally, the system
performs anomaly detection on historic time series of data quality metrics (for
example: the ratio of missing values for different versions of a dataset). This
can be done using either built-in or user-provided algorithms.
