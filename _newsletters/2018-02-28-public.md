---
slug: 2018-02-28-public
layout: newsletter
---

## Hello!

In this month's edition of the Cloudera Fast Forward Labs newsletter, we discuss multi-task learning, DLib and MaxLIPO+TR, and DeepMind's release of PsychLab.

---

## Multi-task Learning

The common approach in machine learning is to train and optimize one task at a time. In contrast, multitask learning (MTL) trains related tasks in parallel, using a shared representation. One advantage of MTL is improved generalization - using information regarding related tasks prevents a model from being overly focused on a single task, while it is also learning to produce better results.
                                                                                                     
MTL is an approach, and is not restricted to any particular algorithm. A straightforward application of MTL in a feed-forward network means that the model is trained using multiple tasks. Instead of having one output node to predict a single task, there are multiple output nodes (one for each task). Once trained, the network is used to predict outcome of the task we are most interested in. The prediction for other tasks are ignored but the shared representation (weights and the network architecture) is derived from all tasks used in training. In a decision tree, an MTL approach implies that multiple tasks are taken into consideration when deciding on a split that results in a maximum information gain.                                 

![]({{ site.github.url }}/images/2018/02/MTLNet-1517931635465.png)
##### Multitask Network with four tasks. Image credit: [Algorithms and Applications for Multitask Learning](https://pdfs.semanticscholar.org/3980/c955f95092e527c580f9cfe066a17f752c08.pdf) 
                                                                                                 
Where do we get the [training signals for the multiple related tasks (extra tasks)](https://pdfs.semanticscholar.org/3980/c955f95092e527c580f9cfe066a17f752c08.pdf)? One possibility is to use future signals to predict the present. In offline systems, we can use future measurements (features that become available after the predictions must be made) as extra tasks during training. For example, results for lab tests administered after hospital admissions can be used as extra tasks when building a model to predict high risk patients _at time of admissions_. At deployment, predictions for the test results are ignored; test results are only used for training purposes. 

Another possibility is to intentionally collect additional labels for each training pattern, and use those as extra tasks. In image recognition, this can be easily  accomplished. Instead of annotating an object of interest in an image, a human would also annotate several other objects surrounding it. When we choose to annotate objects that make up a small part of the image (lane markings on roads, for example) and use those as extra tasks, we force the model to focus attention on patterns that it might otherwise ignore.

Are you (thinking about) using MTL in your organization? We would love to hear about use cases and applications!

---

## DLib and MaxLIPO+TR

Recently, David King of the infamous DLib project released a new implementation of a fast global optimization algorithm called [MaxLIPO+TR][1], based off of a paper called "[Global optimization of Lipschitz functions.][2]" The DLib project itself is the home of many machine learning and computer vision algorithms that are used throughout the field, so having this new algorithm implemented is definitely going to help a lot of people.

But you may be thinking: global optimization? Isn't that very hard? And the answer is yes, global optimization is one of those problems that plague pretty much every aspect of machine learning. In fact, many of the technical criticisms of deep learning relate to the fact that stochastic gradient descent doesn't necessarily get you a global minimum, and instead just gives you a local minimum (or, even worse, a [saddle point][3]!). The reason it's hard is that you never know if there is a better minimum than your current best guess unless you a) have evaluated every other choice (also known as a grid search) or b) have strong assumptions about the way your function behaves.

Evaluating every choice can be prohibitively slow, so making assumptions about our function is necessary. In David King's blog post on the new algorithm, he shows that in 40 evaluations, he is able to get the same error that BayesOpt (another optimization algorithm) takes 80 evaluations to achieve. If each function evaluation takes 15 minutes, that means we can find the solution to the problem overnight instead of having to wait a full day! So in the end, the name of the game is always being as smart as possible with the guesses the algorithm has for what the solution is.

This method tries to reduce the number of guesses for a minimum it makes, by using an estimation about how the function behaves. This estimation is done with an approximate Lipschitz constant, which comes with it some constraints.  Most importantly, the function you're optimizing can't be too noisy or discontinuous. This currently rules out the use of this algorithm to deep learning (although there is [work being done][4] to help with that), but keeps it applicable for most of the other analysis work that data scientists do.  Also, the noise constraint is slightly mitigated in DLib's implementation of the algorithm because they chose to add in an additional noise term.

In addition to using this approximate Lipschitz constant to help understand the function being optimized, this method uses [trust regions][5] to keep track of how confident it is that the current proposal for a global maximum is in fact the true global maximum. This helps us stop the algorithm early, so that we don't need to evaluate our potentially time consuming function more than we need to.  In practice, this means we can optimize a function to much higher accuracy in much less time!

In the end, this new algorithm's implementation in a well-used package is another powerful tool in the machine learner's belt. Often, we need to optimize functions that can take quite a long time to evaluate, so anything that can get us more accurate results in less time is helpful in making more robust models.

[1]: http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html
[2]: https://arxiv.org/abs/1703.02628
[3]: http://bair.berkeley.edu/blog/2017/08/31/saddle-efficiency/
[4]: https://arxiv.org/abs/1701.05217
[5]: http://www.applied-mathematics.net/optimization/optimizationIntro.html

---

## Comparing human and agent performance: DeepMind releases PsychLab

Google’s DeepMind released [PsychLab](https://deepmind.com/blog/open-sourcing-psychlab/) this week, which has been developed internally and released to the public as part of [DeepMind’s efforts](http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3) to apply decades of research in cognitive science/neuroscience to advance the state of the art in machine learning and artificial intelligence. Many modern machine learning models have taken inspiration from principles derived from decades of research in cognitive science/neuroscience. This announcement, along with the accompanying [paper](https://arxiv.org/abs/1801.08116), provide an open-source playground for testing how agents (built using LSTM deep learning alrogirthms) perform when compared to humans on a slew of cognitive tasks that are fairly well-understood and widely used to study human perception.   

The research findings point out some potentially non-obvious ways in which the models used to build artificial agents are missing some fundamental aspects of how primate (human and non-human) vision and cognition operate. For instance, one experiment that tested psychophysical thresholds of an agent found that visual acuity was affected by the size of the images presented.  This led researchers to build a secondary model that loosely approximated the fovea (the center of the retina at the back of the eye, where sight is most acute) to improve performance. The need for this secondary model was only made clear by comparing performance of the agent to human performance and relating the difference back to human physical anatomy.

Interestingly, the agent failed to produce many well-known effects in humans. The pattern of differences between humans and the agent isn’t complete enough yet to make any big theoretical claims, but it appears that the agent has some deficits in integrating information over time, yet is spared the deficit typically seen in humans when asked to search for an object composed of the conjunction of two features (e.g., orientation and color). As these patterns continue to emerge, they will inform how new models are developed and will more clearly delineate the fundamental differences between how agents and humans perform cognitive tasks.

![]({{ site.github.url }}/images/2018/02/PsychLabs_DeepMind_figure-1517949199677.png)
##### Figure from [the PsychLab paper](https://arxiv.org/abs/1801.08116)

Most of the machine learning models in _production_ today, as opposed those used for more pure research, are aimed at automating tasks typically performed by humans or augmenting already-existent human capabilities. Currently, many practitioners make tuning decisions to increase the efficiency of machine learning models, but may be inadvertently making trade-offs that affect how well their models actually reliably replicate or augment human abilities. This collaboration between machine learning and cognitive science/neuroscience research, as it evolves, will bring to light new potential approaches to decrease that error. 

This type of open-source release allows practitioners to test their models on a myriad of cognitive tasks. This will greatly increase the speed at which machine learning models will change based on cognitive science/neuroscience. This burgeoning era gives us a moment to pause, however, and think critically about what a particular business might want to gain by using machine learning. It’s clear that we’re not yet near a generalized Artificial Intelligence (and these experiments reinforce that idea). As machine learning algorithms borrow more and more from cognitive science/neuroscience, we can think of these models in different ways. We can think of them as evolving along the same trajectory as humans (akin to studying infant brains and how they change and evolve throughout a human’s life); we can look at these models as something related to human cognition, but fundamentally separate (akin to the research on non-human primates); or we can think of these models as performing something entirely their own - with no tie to human evolution or development.
 
As this field continues to develop, and we begin to employ machine learning in every facet of business, it’s important to establish and maintain a goal for these tools, perhaps with these distinctions in mind.

---

## Recommended Reading

Be sure to check out this post on our blog:
* [Probabilistic Cookies!](http://blog.fastforwardlabs.com/2018/02/14/probabilistic-cookies.html)  
* as well as this post by Seth Hendrickson on Cloudera's Engineering Blog, [Production Recommendation Systems with Cloudera](http://blog.cloudera.com/blog/2018/02/production-recommendation-systems-with-cloudera/)

We love to read; here are a few of our (notably eclectic) favorite finds from this month:
* [A Code of Ethics for Data Science](https://medium.com/@dpatil/a-code-of-ethics-for-data-science-cda27d1fac1)
* [Quantum algorithms: an overview](https://blog.acolyer.org/2018/02/06/quantum-algorithms-an-overview/)
* [Quantum computing in the NISQ era and beyond](https://blog.acolyer.org/2018/02/05/quantum-computing-in-the-nisq-era-and-beyond/)
* [All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness](http://proceedings.mlr.press/v81/ekstrand18b.html)
* [Datasette: instantly create and publish an API for your SQLite databases](https://simonwillison.net/2017/Nov/13/datasette/)
* and last, but not least, [Finally, Facial Recognition for Cows Is Here](https://gizmodo.com/finally-facial-recognition-for-cows-is-here-1822609005)!

Also, while there's definitely much more to it, we appreciated the humor:

![]({{ site.github.url }}/images/2018/02/machine_learning_2x-1519239482038.png)
##### source: [https://xkcd.com/1838/](https://xkcd.com/1838/)
---

## Jobs

Come work with us!  In addition to several other [open positions](https://www.cloudera.com/careers/careers-listing.html), **Cloudera** is hiring a **Director of Product Management, Data Science** ([job description](https://cloudera.wd5.myworkdayjobs.com/External_Career/job/USA--California--Palo-Alto/Director-Product-Management--Data-Science_180286)).

Here a couple of other positions we've heard about as well:
* **Silectis** - Data Engineer ([job description](https://www.silect.is/careers-data-engineer))
* **Silectis** - Platform Engineer ([job description](https://www.silect.is/careers-platform-engineer))
* **Pew Research Center** - Director, Data Labs ([job description](https://jobs-prc.icims.com/jobs/5386/director%2C-data-labs/job?mobile=false&width=641&height=500&bga=true&needsRedirect=false&jan1offset=-300&jun1offset=-240))

---

## CFFL Updates

* Hilary will be speaking on March 7th at the [Strata Data Conference](https://conferences.oreilly.com/strata/strata-ca/) in San José, and Mike will be presenting on [Interpretable Machine Learning Products](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/63572) at Strata on March 7th, as well.

* Brian will be speaking at the [Cloudera Sessions](https://www.cloudera.com/more/events/sessions/sao-paulo.html) event in São Paulo on March 13th.

* and Mike will be speaking on interpretability at [Qcon.ai](https://qcon.ai/) on April 11th in San Francisco.

* You can catch both Hilary and Mike in conversation with Hugo Bowne-Anderson on DataCamp's podcast, [DataFramed](https://soundcloud.com/dataframed/9-data-science-and-online-experiments-at-etsy#t=17:10).  Hilary joined Hugo to talk about [the past, present, and future of data science](https://www.datacamp.com/community/podcast/data-science-past-present-and-future), and Mike spoke with him about [interpretability](https://soundcloud.com/dataframed/9-data-science-and-online-experiments-at-etsy).  (Stay tuned for future podcasts featuring Mike and Friederike, as well!)

* We'd also like to recommend this webinar on March 21st: [Cloudera Data Science Series #1: sparklyr, implyr, and More - dplyr Interfaces to Large-scale Data](https://info.cloudera.com/LP=1922?src=Newsletter), featuring Ian Cook (Senior Curriculum Developer, Cloudera) and Thomas Dinsmore (Director of Product Marketing for Cloudera Data Science).

As always, thank you for reading. We welcome your thoughts, feedback, and suggestions; please drop us a note anytime at cffl@cloudera.com.

All the best,

The Cloudera Fast Forward Labs Team
