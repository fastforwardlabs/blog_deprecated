---
slug: 2019-11-13-client
layout: newsletter
---

Hello!  In keeping with our reputation as your data nerd best friends, here are our (eclectic) recommendations for some great reading this month:

---

#### [ML beyond Curve Fitting: An Intro to Causal Inference and the do-Calculus](https://www.inference.vc/untitled/) 
I worked through this article in detail, taking notes, and left convinced that causal inference is a useful and practical framework for thinking about problems. It also helped me understand where Judea Pearl’s do-calculus fits in the broader ML picture. I certainly have details to fill in with more reading, but I substantially enhanced my reasoning toolbox by reading this article alone. Very highly recommended. - *[Chris](https://twitter.com/_cjwallace)*

---

#### [Evading Real-Time Person Detectors by Adversarial T-shirt](https://arxiv.org/abs/1910.11099) 
A fun read! Instead of holding a cardboard with an adversarial patch, we can now wear a T-shirt and go undetected. It's a physical adversarial wearable! - *[Shioulin](https://twitter.com/shioulin_sam)*

---

#### [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html)
A long read with interesting insights. This paper introduces a framework for applying CD methods to machine learning applications. - *[Keita](https://twitter.com/keitabr)*

---

#### [Auptimizer - an Extensible, Open-Source Framework for Hyperparameter Tuning](https://arxiv.org/abs/1911.02522) 
Great overview highlighting the difficulties of training hyperparameters for production ML. Their solution provides data scientists more flexibility, usability, scalability and extensibility in training production ML while also addressing reproducibility - crucial to successful implementation. - *[Melanie](https://www.linkedin.com/in/melanierbeck/)*

---

#### [Museum of the Future: The building designed by an algorithm](https://www.bbc.com/future/article/20191028-museum-of-the-future-the-building-designed-by-an-algorithm)
A fascinating and insightful read, highlighting the ways technology and an eye towards sustainability are changing the future of architecture.  [The Museum of the Future](http://www.museumofthefuture.ae/), scheduled to open in Dubai next fall, aims to be a cultural center of creativity and hope. I'm definitely adding a visit to my bucket list! - *[Danielle](https://www.linkedin.com/in/daniellethorp/)*

---

#### [Twitter thread: programming book recommendations](https://twitter.com/dan_abramov/status/1190762799338790913)

A thread of responses to Dan Abramov’s (React.js developer) request for books on programming at a conceptual level. So far I’ve been enjoying [A Philosophy of Software Design](https://www.amazon.com/Philosophy-Software-Design-John-Ousterhout/dp/1732102201) and saved a bunch more of the suggestions to check out later. - *[Grant](https://twitter.com/GrantCuster)*

---

#### [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)

The [lottery ticket hypothesis](https://arxiv.org/abs/1803.03635) suggests that there exist smaller subnetworks within large models, such that if trained, can achieve the same performance with a fraction (10 - 20%) of the original model size. These subnetworks, known as _winning tickets_, are those who have _won_ the initialization lottery: their connections have weights that make training particularly effective.

In the current paper, the authors provide an extension of this idea, scaled up to very deep networks, trained on complex tasks such as imageNet. This line of research is valuable as it can lead to new energy efficient methods for compressing models, and reduce the massive compute resource requirements needed to train high performance neural networks. - *[Victor](https://twitter.com/vykthur)*

---

#### [What BERT Is Not](https://podcasts.google.com/?feed=aHR0cDovL2RhdGFza2VwdGljLmNvbS9mZWVkLnJzcw&episode=OTE0M2JkYjkyNDY1NDYyNGE0NzUxOGIwOWE5OWJkNGM&hl=en-GB&ved=2ahUKEwjXtpfFrtvlAhVUtnEKHfRcBm8QiOUEKAJ6BAgEEAc&ep=6&at=1573241854670)
Following [Alice's newsletter article](https://blog.fastforwardlabs.com/2019/08/28/is-machine-learning-research-moving-in-the-right-direction.html) from a few months back, I have found myself spending more time reading up on some of the more skeptical takes (with Gary Marcus leading the pack) on the likelihood that the approaches receiving the most attention in terms of research funding (such as deep learning) will lead us to the promised land of Artificial GeneraI Intelligence. I think that's why I was especially intrigued when I heard Allyson Ettinger talking on the [Datasceptic podcast](https://podcasts.google.com/?feed=aHR0cDovL2RhdGFza2VwdGljLmNvbS9mZWVkLnJzcw&episode=OTE0M2JkYjkyNDY1NDYyNGE0NzUxOGIwOWE5OWJkNGM&hl=en-GB&ved=2ahUKEwjXtpfFrtvlAhVUtnEKHfRcBm8QiOUEKAJ6BAgEEAc&ep=6&at=1573241854670) about the research (detailed in their paper: [What BERT Is Not](https://arxiv.org/abs/1907.13528))  that she and colleagues had done into BERT's strengths and weaknesses.  

What I found most interesting about the paper and Allyson's description of the research were:

* the value of a multidisciplinary approach to research (the impact of her training really came through as she discussed the importance of understanding the linguistic capacities with which the pre-training processes imbue a model)
* the idea that understanding how BERT's weaknesses manifest might tell us something about where we could/should be focusing research and, possibly, help to iterate our theories of machine learning

The paper is definitely worth a read, if only so you can better assess the suitability of BERT for your NLP use cases. - *[Ade](https://twitter.com/Adewunmi)*

---

#### [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)

In reading up on causation, which has been on many of our minds, I encountered Simpson's Paradox, an apparent incongruity in statistical analysis that can arise when analyzing data a different levels of granularity. Simpson's paradox appears when results at one level of granularity are in opposition to results at another level of granularity. This could be useful for deciding which data to include in or exclude from an analysis. -  *[Ryan](https://twitter.com/MicallefEsq)*

----

## Upcoming Events

* Victor Dibia is speaking on "ML in the Browser: Interactive Experiences with Tensorflow.js" at [QCon](https://qconsf.com/sf2019/track/machine-learning-without-phd) in San Francisco today.
* Shioulin Sam will be presenting at [Data Science Wrangle UK](https://events.attend.com/f/1383790543) in London on November 20th, and speaking on *Learning with Limited Labeled Data* at [ODSC London](https://odsc.com/london/europe-schedule/) on November 21st. 

If you're attending any of these events, please let us know - we'd love to say hello!   Reach out anytime to [cffl@cloudera.com](mailto:cffl@cloudera.com).
