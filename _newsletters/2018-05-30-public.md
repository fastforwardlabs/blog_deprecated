---
layout: newsletter
slug: 2018-05-30-public
---

Greetings from Brooklyn, NY!  In this month's edition of our newsletter, we discuss an interesting modification of convolutional neural networks in temporal domains, adversarial attacks to medical data, and the challenges of building models that don't learn or perpetuate bias.

---

## Convolve all the things

While deep learning can be applied generally, much of the excitement around it has stemmed from significant breakthroughs in two main areas: computer vision and natural language processing. Practitioners have typically applied convolutional neural networks (CNNs) to spatial data (e.g. images) and recurrent neural networks (RNNs) to sequence data (e.g. text). However, a [recent research paper](https://arxiv.org/pdf/1803.01271.pdf) has shown that convolutional neural networks are not only capable of performing well on sequential data tasks, but they have inherent advantages over recurrent networks and may be a better default starting point.

CNNs were designed originally to take advantage of spatial structure in the input data; for example, a pixel in an image is strongly related to nearby pixels. Sequence data also exhibits a “spatial” structure of sorts, where a particular word is strongly related to surrounding words. The observation is not new, though, and CNNs have been successfully applied to tasks involving sequences for decades. These applications have traditionally been things like sentiment or topic classification, where the output has the freedom to inspect every element in the input sequence. Until fairly recently, CNNs were not popular choices for tasks which involve mapping an input sequence to an output sequence (e.g., time series forecasting).

Vanilla CNNs applied to sequence forecasting have two pitfalls - the output incorporates input from both the past and the future, and they struggle to “see” or “remember” events in the distant past. Luckily, there are solutions for these two shortcomings: _causal convolutions_ and _dilated convolutions_, respectively. A causal convolution adjusts the convolution kernel to only look at data in the past:

![]({{ site.github.url }}/images/2018/04/CausalConv-1524689210501.jpg)

while dilated convolutions introduce gaps that allow the output to incorporate information from the distant past:

![]({{ site.github.url }}/images/2018/04/DilatedCausalConv-1524689251611.jpg)

CNNs that have been modified for use in temporal domains are called temporal convolutional networks or TCNs. One of the main benefits of using TCNs for sequence modeling tasks is that the convolutions can be computed in parallel since the output at a given timestep does not need to wait for previous timesteps. This is in contrast to an RNN, where each prediction must wait for all previous predictions. One potential downside to TCNs is that they do not encode the history of the sequence in a single hidden state like RNNs do, but instead require the entire input sequence to generate predictions. 

The authors of the paper present results that show that simple TCNs can beat popular recurrent architectures at sequence modeling tasks that have traditionally only used recurrent networks. While it would be counter-productive to declare a winner, it may be time to question our assumptions and consider TCNs as a first-class citizen for sequence modeling. If you’ve had success with using convolutional networks for time series or sequence modeling, we’d love to hear more about it!

---

## Adversarial Attacks to Medical Data in Deep Learning

In April, a group from Harvard raised worrying [concerns](https://arxiv.org/abs/1804.05296) about the risks of adversarial attacks unique to healthcare. Their argument: due to this sector’s large economic size, history of fraud, balkanized infrastructure, and other factors, deep learning for medical image classification is especially likely to be attacked.  To prove their point, the group used [projected gradient descent](https://arxiv.org/abs/1706.06083) to perturb multiple images and trick deep learning predictive models into deciding benign tumors were malignant. In the real world, this kind of attack would be worrying indeed!  

While the authors see a special vulnerability to adversarial attacks in the medical image space, current trends belie some of the paper’s sense of alarm. Work on defending against adversarial attacks is underway in the healthcare field; for example, [one research team](https://arxiv.org/abs/1802.04822) recently attacked a deep learning model to find which data fields in electronic health records (EHR) were most vulnerable, and proposed a screening procedure leveraging the attack strategy that could assist practioners in paying special attention to fields that pose risk factors. Ongoing work outside the healthcare domain can also be leveraged within healthcare, particularly around [defending](https://arxiv.org/abs/1803.10840) against adversarial attacks in image classification and using locally-interpretable models like [LIME](http://blog.fastforwardlabs.com/2017/09/28/the-product-possibilities-of-interpretability.html) to stress-test implementations. 

Many hospital infrastructures have been flexible enough to accept deep learning, and so they likely will be able to accommodate deep learning ‘defenses’ as well. Recently, the US Department of Health and Human Services (HHS) outlined incentives for [‘promoting interoperability’](https://www.cms.gov/Newsroom/MediaReleaseDatabase/Press-releases/2018-Press-releases-items/2018-04-24.html). The small concentration of EHR software vendors, the success of HHS incentive programs, and the interest to get the right diagnosis will all lead to an EHR in every hospital room, and will (eventually) lead to defenses against attack in every diagnostic, including those that are image-based - for while medical image-based diagnostics is an area where ‘ground truth’ is not always evident, deep learning has also been successful in other areas. [One team](https://ieeexplore.ieee.org/document/7801947/) found twenty-two healthcare application areas where deep learning has been posited or tried outside of the medical imaging domain. (In those spaces, it is more common to have shared diagnostic criteria that is less subjective.)

It is also  worthwhile to consider that while attacks are not impossible, they are also expensive. It would be quite costly for a fraudster to develop an adversarial attack that updates to stay ahead of defenses. Philip Tully, principal data scientist at ZeroFox, confirms in [an interview with SearchSecurity](https://searchsecurity.techtarget.com/news/252440026/Philip-Tully-AI-models-are-cost-prohibitive-for-some-enterprises) with regard to adversarial attacks: “...machine learning itself is an investment. Data science shops are not cheap. The tools they use are not cheap.” Fraudsters tend to bypass more difficult quarry when there is lower-hanging fruit. They could more easily, for example, ransom medical records rather than constructing an elaborate adversarial model.

[The Harvard team](https://arxiv.org/abs/1804.05296) mentioned both algorithmic and infrastructural defenses as promising paths for defense against adversarial attacks, so there was some hope leavened in with their alarm. Practitioners can help by including ideas for defenes with every model they design, using healthcare training data for prototyping models, and forecasting the possible risks associated with implementing those models.

---

## Rules to Learn By

Longtime readers of this newsletter know that we follow the Fairness, Accountability, and Transparency in Machine Learning conversation closely (see [here](https://fatconference.org/2018/index.html) and [here](https://www.fatml.org/)). These conversations address and attempt to mitigate the potential for technical systems to produce unfairness. Much of this unfairness arises from how algorithmic systems might perpetuate historical inequalities or otherwise produce discriminatory effects. This conversation is broader than could be encapsulated in any newsletter, but we want to point to some recommendations that have come out of this conversation to demonstrate how we think through the challenges of building models that don't learn or perpetuate bias. We embrace these challenges not just because of an overriding ethical commitment to build safely, but also because addressing these challenges helps us build things that work better than they otherwise might. 

Joanna Bryson identifies three sources of bias, and offers recommendations for how to fix the the problems they pose. With the intention of generalizing Bryson's [analysis](https://joanna-bryson.blogspot.com/2017/07/three-very-different-sources-of-bias-in.html?m=1) to all machine learning, we'd like to restate her argument slightly by identifying the sources of bias as 1) bias in the training data (with the assumption that historical biases tend to be represented in datasets), 2) bias in the assumptions and intuitions that guide our model, and 3) the bias of unintended consequences. Taking these as three sources of bias leads to three recommendations that are also best practices for machine learning. 

Bias in the real world (and therefore in the data available for training an algorithm) can be thought of as a class imbalance, regardless of where it comes from. There are acute disparities in how members of different populations are represented in datasets, due to historical social and economic inequities, but rather than perpetuating this disparate representation in machine learning, data can be balanced at the training step and thereby produce a more balanced representation in the model. This is true of all data as well; we still strive to produce balanced representations in any model we build. For an image classifier, if photographs of, say, "hamburgers with fries" are underrepresented in the training data set, the classifier might not work as well across all food items as it might if trained on balanced classes. While humans and hamburgers are very different things, ethically speaking, any machine learning model should learn robust, balanced representations.

![]({{ site.github.url }}/images/editor_uploads/2018-05-09-164638-55f9d8d8bd86ef1d008bac98_750_577.png)

Bias in the assumptions and intuitions that guide our models arises from how we ask and answer questions in machine learning. Bryson refers to this source of bias as "poorly reasoned rules," but this source of bias can also arise from a lack of robust product testing. Because our own life experience and domain expertise is necessarily partial, we cannot necessarily account for all the ways in which a system might not work as expected. Bryson gives the example of facial recognition not working well for people with dark skin: if we don't ask whether or not our system works well for all skin tones, it may not work as expected (see Joy Buolamwini's [project](http://gendershades.org/) on this). One solution to this problem is to develop robust testing pipelines prior to deployment; another solution is to bring in people with diverse sources of expertise and lived experiences to inform the assumptions that guide development and testing; a third solution is to think deeply and broadly about the world in which systems will be deployed. They may work exactly as designed when in 'the lab,' but produce unexpected results when deployed. The rules we have in mind about how the world behaves should correspond with the actual world beyond the lab.

The bias of unintended consequences is closely related to the causes of unexpected results, and arises in part from uninterpretable and opaque black box models. When models don't behave as expected it can be difficult to understand why they misbehaved. Developing (and utilizing) tools that enable interpretability (such as [LIME](http://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html)) and auditing algorithms with them can help developers and users understand why a certain result was returned, can identify sources of bias in a model that can then be fixed, and can even result in new and useful insights that have value as [products](http://blog.fastforwardlabs.com/2017/09/28/the-product-possibilities-of-interpretability.html). Bias, in the sense that Joanna Bryson discusses it, perpetuates some of the most harmful tendencies in society. Correcting for that bias is of profound ethical value to society - in machine learning, it also focuses our attention on the things we want to be able to learn (for example, how to create more robust representations of risk factors for disease without overfitting a model to signals that distract from the end goal).

---

## Recommended Reading

* [AI researchers allege that machine learning is alchemy](http://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy)
* [Metal Band or My Little Pony?](http://aiweirdness.com/post/174211306032/metal-band-or-my-little-pony)

---

## CFFL Updates

* Friederike will be speaking at the [Data Science Salon](https://www.eventbrite.com/e/data-science-salon-nyc-tickets-40072527007) on applying AI and Machine Learning To Media and Entertainment on June 13th in New York.

* Friederike will also be speaking at the [Research & Applied AI Summit](https://raais.co/) in London on June 29th, and at [Curious2018](https://curious2018.com/) in Darmstadt, Germany in mid-July.


As always, thank you for reading. We welcome your thoughts, feedback, and suggestions; please [drop us a note](mailto:cffl@fastforwardlabs.com) anytime!

All the best,

The Cloudera Fast Forward Labs Team
