---
slug: 2019-08-13-client
layout: newsletter
---

## NBSVM - a strong document classification baseline (code release!)

*by [Chris](https://twitter.com/_cjwallace)*

[Transfer Learning for Natural Language Processing](https://blog.fastforwardlabs.com/2019/07/17/new-research-transfer-learning-for-natural-language-processing.html) is on your screens, and we hope you're enjoying it!
To further your enjoyment and assist in your document classification projects, we're releasing the code for a benchmark algorithm we used in the report.

In the report itself, we detail the benefits and tradeoffs of transfer learning with large, pretrained deep neural networks.
To establish where transfer learning shines, we needed a reasonable baseline to compare against for our chosen task: sentiment classification. 
Establishing a baseline is vital when beginning experimentation for _any_ machine learning application.
For instance, in a simple binary classification problem, it is always wise to know the accuracy score if one simply predicts the majority class: 95% accuracy doesn't seem quite so impressive if 95% of the examples have the same label.

For our sentiment classification task, a great baseline model is provided in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification](https://aclweb.org/anthology/papers/P/P12/P12-2018/), which has aged well since its 2012 release.
The paper examines two primary learning algorithms as applied to sentiment and topic classification: multinomial naive Bayes (MNB) and support vector machines (SVMs).
It's a short and pragmatic paper, providing practical wisdom for applications of the algorithms: multinomial naive Bayes outperforms more complex methods on short snippets of text, whereas support vector machines win for longer documents.
The authors combine these two methods into the so-called NBSVM: a support vector machine using naive Bayes log-counts as features.
Further, a weighted interpolation between this NBSVM and plain MNB is used, and is shown to be a robust classification algorithm across a variety of text lengths.

![]({{ site.github.url }}/images/editor_uploads/2019-08-14-195003-Classifier.png)
It surprised us how effective this algorithm is, given enough data, especially since it uses only bag-of-words type features (with some preprocessing: uni- and bigrams, stopword removal, and stemming).
When there are very few training examples (a few hundred), NBSVM performed poorly, but with thousands of labeled examples, it reached a respectable 85% accuracy on our problem.
This does not compete on accuracy with deep models, especially in the region with few labeled samples, where transfer learning allows neural models to maintain good accuracy.
However, it _does_ compete on ease of use, and avoids much of the complexity of serving large neural networks.

To make using the technique easy and repeatable, we implemented it as a scikit-learn classifier.
This saved us writing a lot of custom code - we didn't even have to provide a `predict` function - and enabled us to easily plug into any tool that conforms to the scikit-learn classifier API.

Open source is awesome.

[We're happy to share our implementation](https://github.com/fastforwardlabs/nbsvm), and we hope it's useful to you.

---

## Upcoming Events

* Victor Dibia will be speaking at the [Google Developers ML Summit '19](https://events.withgoogle.com/mlsummit19/) in Cambridge MA, on the topic *"Art + AI : Generating Novel African Mask Art using Generative Adversarial Networks"* on August 28th.
* Victor is also speaking on *"Handtrack.js: Building Gesture Based Interactions in the Browser Using Tensorflow.js"* at the [GDG Rochester DevFest](https://www.meetup.com/GDG-Rochester/events/262710495/) event on September 7th; he'll be presenting on this topic at the [Strata Data Conference]((https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/77389)  in NYC on September 26th, and at the [TensorflowWorld](https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/77833) Conference on October 31st as well.
* Chris Wallace is speaking on Federated Learning at [MCubed](https://www.mcubed.london/sessions/federated-learning/) in London on October 1st.
* Shioulin Sam and Ryan Micallef will both be speaking at [PyGotham](https://2019.pygotham.org/) in New York City on October 4th/5th.
* Ryan Micallef is also speaking on natural language processing with transfer learning at [AI Dev World](https://aidevworld.com/) in San Jose on October 9th.

If you're attending any of these events, please let us know - we'd love to say hello!

All the best,

The Cloudera Fast Forward Labs Team