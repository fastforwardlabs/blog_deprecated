---
slug: 2019-03-08-client
layout: newsletter
---

## Do you see what I see? Visualizing higher order categories in neural networks

_by [Alice](https://twitter.com/AliceAlbrecht)_

This week Google and OpenAI released a jointly developed tool to visualize the activation of neurons in an artificial neural network. They have coined the output of this tool “activation atlases.” (For reference, see: [Google AI Blog: Exploring Neural Networks with Activation Atlases](https://ai.googleblog.com/2019/03/exploring-neural-networks.html), and [Activation Atlas](https://distill.pub/2019/activation-atlas/).) 

Typical convolutional neural networks used in image classification are made up of different layers of artificial neurons that build on each other. This tool furthers previous work ([Feature Visualization](https://distill.pub/2017/feature-visualization/)) that overlaid activation grids on images to illustrate which features the model was picking up on at various layers of the network. 

Instead of examining a single image at a time, these activation atlases draw information from a randomly selected locations within 1 million images and average the attributes coming from each of those locations. These averaged attributes are then projected into a 2-dimensional space where similar attributes are located near each other. This illustrates which categories (rather than individual features) are represented. These are at a higher level (conceptually) than individual features. This work adds to an increasing body of work that allows deeper insight into how neural networks work, and makes them more interpretable.

![]({{ site.github.url }}/images/2019/03/Activation_atlases-1551983981470.png)
##### Image taken from the [OpenAI Blog](https://distill.pub/2019/activation-atlas/)

While this release allows much needed transparency into how neural networks represent information, it also raises an interesting question about how we think about artificial neural networks. It’s tempting to look at these activation atlases and see hope that these models are able to represent information in a way that’s similar to the way the human brain processes images (to be clear, the authors of this work do _not_ draw that conclusion). However, the similarities between artificial and human neural networks are slim, and the evolution from visualizing features to visualizing categories will necessarily have to end there; we won’t be able to infer higher-level information from these images this way (for example, whether something in an image is funny or surprising) given the way neural networks are currently structured.

We know that the human visual cortex encodes individual features and that information flows to other cortical areas that encode information about object categories, but the ways in which that information is transmitted is very different in human cortex than it is in silico.  Human brains have many different types of connections that aren’t accounted for in today’s artificial neural networks. (For instance, human brains contain lateral connections between neurons in the same layer, both local and long-range connections.)

As neural networks gain broader use, and we start to see human-understandable outputs of different layers of these networks, it’s tempting to believe that we’re getting closer to replicating something as complicated as human perception. We caution, however, that without a big shift in the way neural networks are designed, we’re unlikely to see progress in real human-like artificial intelligence until the complexity of the human brain is fully considered in our models. That said, this work is a great addition to the current body of work on making machine learning models more interpretable.

---

## Upcoming Events

* Several of us will be speaking at the [Dataworks Summit](https://dataworkssummit.com/barcelona-2019/) next week on March 21st:
 -  Hilary Mason will be speaking on [Building an Enterprise AI Factory](https://dataworkssummit.com/barcelona-2019/keynote/8410/)
 - Chris Wallace will be speaking on [Federated Learning: Artificial Intelligence and Data Science](https://dataworkssummit.com/barcelona-2019/session/federated-learning-artificial-intelligence-and-data-science/)
 - Alice Albrecht will be speaking on [A Framework for Developing a Winning Data Project Portfolio](https://dataworkssummit.com/barcelona-2019/session/a-framework-for-developing-a-winning-data-project-portfolio/)
 - Justin Norman will be speaking on [Machine Learning Model Deployment: Strategy to Implementation](https://dataworkssummit.com/barcelona-2019/session/machine-learning-model-deployment-strategy-to-implementation-2/)
 - Tristan Zajonc will be speaking on [Cloud-Native Machine Learning: Emerging Trends and the Road Ahead](https://dataworkssummit.com/barcelona-2019/session/cloud-native-machine-learning-emerging-trends-and-road-ahead/) and [Edge to AI: Analytics from Edge to Cloud with Efficient Movement of Machine Data](https://dataworkssummit.com/barcelona-2019/session/edge-to-ai-analytics-from-edge-to-cloud-with-efficient-movement-of-machine-data/)
 - Vidya Raman will be speaking on [Starting with the End in Mind: Learnings from Data Strategies across Companies and Verticals](https://dataworkssummit.com/barcelona-2019/session/starting-with-the-end-in-mind-learnings-from-data-strategies-across-companies-and-verticals/)
* Victor Dibia will also be giving a talk called "Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Network" at the [NVIDIA GTC 2019](https://www.nvidia.com/en-us/gtc/) in San Jose, CA on March 21st. 
* Mike Lee Williams will be speaking on [Federated Learning](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/72661) at the Strata Data Conference in San Francisco on March 27th.
* Hilary Mason will be speaking at the [Marketing Analytics and Data Science Conference](https://marketing.knect365.com/marketing-analytics-data-science/) in San Francisco, CA on April 10th.
* Mike Lee Williams will be chairing the "deep learning in practice" track at [Qcon.ai](https://qcon.ai/) in San Francisco on April 16-17th.
* Chris Wallace will be speaking on [Federated learning: machine learning with privacy on the edge](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74327) on May 1st at the [Strata Data Conference](https://conferences.oreilly.com/strata/strata-eu) in London.
* Shioulin Sam will also be speaking at Strata in London about [Learning with Limited Labeled Data](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74341) on Thursday, May 2nd.

If you're attending any of these conferences, please [let us know](mailto:cffl@cloudera.com) and do stop by to say hello! 

All the best,

The Cloudera Fast Forward Labs Team