---
layout: newsletter
slug: 2018-05-31-client
---

Hello!  In this week's newsletter, we discuss a recent paper ([Multi-label Classification of Surgical Tools with
Convolutional Neural Networks](https://arxiv.org/pdf/1805.05760.pdf)) and (our favorite topic) ethics in tech.

---

## Multi-label Classification and Surgical Tool Detection

Surgical tool detection has a multitude of useful applications ranging from generating operation reports, reconstructing 
surgical workflows, coming up with intervention systems that would provide real-time recommendations or warnings to the 
surgeon, inferring surgery phases based on tool presence and others. The construction of such a detection system is guided
by a wide array of experiments that explore different design decisions.

A recent [paper](https://arxiv.org/pdf/1805.05760.pdf) discusses possible solutions and employs a 50-layer residual network (ResNet) architecture that can distinguish 21 different tools in [cataract surgery videos](https://cataracts.grand-challenge.org/data/). The authors explore various transfer learning approaches, either in the form of fine tuning (FT) or as a fixed feature extractor (FFE). When working with the FT network family, it uses all 49 convolutional layers of ResNet and the output feature maps of the 49th convolutional layer are either fed into the *avg-fc* or *conv-max* classification head as shown in the figure below. The *avg-fc* classification head is a standard global average-pooling followed by a fully connected layer and the *conv-max* uses a fully convolutional network and a global max-pooling layer. In general, all weights in this network family are trainable but for some experiments the first k layers of ResNet are frozen. Training this architecture requires a significant amount of memory due to the many trainable weights and input image resolution. On the other hand, the FFE network family uses the first k layers as a feature extractor with fixed weights. The resulting feature maps are fed to a max-pooling layer and three layers of convolutions with 384 feature maps each. Like for the first network family, the final part is either the *avg-fc* or *conv-max* classification head. Training this architecture is relatively inexpensive in terms of required memory because all ResNet weights are fixed and only the custom layers are trained. 

<p align="center">
  <img width="500" height="400" src="./images/FT vs FFE2.png">
</p>

In both network families each output node corresponds to a predicted score for one of the *c* surgical instruments. Because the instruments are not mutually exclusive, the binary relevance transformation is used so that the task is treated as *c* separate binary classification problems.

Prior to the training process various pre-processing strategies are applied to the dataset. For instance, due to the nature of video, subsequent frames tend to be extremely similar to one another. Therefore, only every sixth frame of each video is used. Further, since a predecessor or successor of a frame is part of the training set, the validation error would be a significant underestimation of the test error if one were to split all the data randomly into train-validation. Hence, instead a split at the video level is performed. In addition, because some tools only appear in a single training video the variability between the frames showing such tools is low. To tackle this problem, several image augmentation techniques are employed.

In the end, the FT model instances perform better than the FFE ones which seems plausible as the cataract dataset is different than ImageNet. The *conv-max* classification head does not seem much useful and instead using a weighted loss fuction slightly improves the performance. The resulting network works exceptionally well for some tools but performance suffers in other cases because not enough training data is available. Nonetheless, it explores interesting solution design decisions!

---

## Ethics in Tech

We love a good epistolary [debate](https://pbs.twimg.com/media/DdwYsoPVMAEyY7G.jpg), particularly when both sides are bringing good and important points to the table. This week, the Financial Times published an editorial ([Making Decisions Computers Cannot](https://www.ft.com/content/6f36a2c2-5dad-11e8-9334-2218e7146b04) - paywall) and a [response](https://www.adalovelaceinstitute.org/letter-to-the-financial-times-embed-ethical-thinking-in-tech-culture/) that demonstrate the importance of a lively debate around how to incorporate ethics in data science, machine learning, and artificial intelligence.

In the editorial, the author worries that the tight market for computer science PhDs means that few experts in artificial intelligence are likely to remain in academia, where they are most likely to perform research that addresses ethical issues free of corporate concerns. The author calls for independent funding for such research to preserve the autonomy of such research. Tim Gardam, the chief executive of the [Nuffield Foundation](http://www.nuffieldfoundation.org/) which funds the [Ada Lovelace Institute](https://www.adalovelaceinstitute.org/), points out in his letter that not only are there are many such independent organizations already pursuing this work (such as the one he funds), but that the deep involvement of tech companies in ethics work needs to be supported, not kept separate. Gardam says, "We need to build a more considered research base that explores who is most at risk of harm and the cumulative distributional social impacts. Above all, we need to embed ethical thinking in the tech industry, as an inherent part of its culture."

Indeed, there are many paths towards building this base. These paths travel within companies, from academia to industry and back again, and across a veritable menagerie of independent and semi-independent research institutes. These paths should also find their trailheads in the curricula of tomorrow's computer scientists, not as an add-on or elective that implies that 'ethics happens in this class and everything else is computer science', but deeply embedded in every aspect of training (see [here](https://twitter.com/cfiesler/status/931200575873490944) for an expansive list of current ethics training in tech curricula). At CFFL, we keep track of many such efforts to embed ethical thinking in tech, and while we know how important it is to bracket our expectations within the motives that might underlie the missions of each of these efforts, we are glad to see flowers of many kinds blossom. A cursory and partial list of these efforts includes:
 
 - [The Ada Lovelace Institute](https://www.adalovelaceinstitute.org/)
 - [Open.ai](https://openai.com/)
 - [AI Now Institute](https://ainowinstitute.org/)
 - [Partnership on AI](https://www.partnershiponai.org/)
 - [Data & Society Research Institute](https://datasociety.net/)
 - [Google DeepMind Ethics & Society](https://deepmind.com/applied/deepmind-ethics-society/)
 
These institutes and research groups are, we are glad to see, often in conversation with each other. What other institutes belong on this list? Please do [let us know](mailto:cffl@cloudera.com)!

---

## CFFL Updates

* Friederike will be speaking at the [Data Science Salon](https://www.eventbrite.com/e/data-science-salon-nyc-tickets-40072527007) on applying AI and Machine Learning To Media and Entertainment on June 13th in New York.

* Mike will be speaking on probabilistic programming at [QCon](https://qconnewyork.com/ny2018/presentation/modern-cs-presentation-1) in New York on June 28th.

* Friederike will be speaking at the [Research & Applied AI Summit](https://raais.co/) in London on June 29th, and also at [Curious2018](https://curious2018.com/) in Darmstadt, Germany in mid-July.


As always, thank you for reading. We welcome your thoughts, feedback, and suggestions; please [drop us a note](mailto:clients@fastforwardlabs.com) anytime!

All the best,

The Cloudera Fast Forward Labs Team
