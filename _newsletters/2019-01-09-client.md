---
slug: 2019-01-09-client
layout: newsletter
---

Hello, and happy new year!  Here is the first edition of our 2019 newsletter.

---


## Model Server Madness

_by [Justin](https://twitter.com/JustinJDN)_

Model serving is getting easier, but you still have to know what you’re doing.  

Model serving is that thing that everybody wants to do but nobody actually has a plan for until it's an emergency: deploying ML models into a production environment and keeping them there so that they be utilized by apps, people, and other models. AutoML products may seem like a tailor-made solution to this challenge, and often do boast incredible benchmark performance numbers and easy to use APIs, but the need to address the [data product gap](https://www.slideshare.net/cloudera/demystifying-ml-ai-96567346) prevents these tools from becoming a practical, multi-use solution for the model serving requirement. 

Some companies, armed with lots of funding and a specific mandate to integrate AI/ML capabilities into the core of their business models, have spent years (and millions of dollars) developing fully bespoke end-to-end ML platforms - and we all benefit!  Not only do these modern model deployment and serving frameworks exist as reference architectures (such as Uber's Michelangelo, Facebook's FBLearner, or AirBnB's BigHead), but increasingly robust open source tools are now available to help automate the process of scoring real-world data by deployed ML models.  These platforms also serve as powerful data ingest systems, cross-domain feature stores, model evaluation tools, collaboration enablers, and workflow orchestrator/automators.  While these capabilities are certainly important to a robust enterprise ML/AI toolkit, they are not the focus of this article;  instead, it will explore the recent viability of **ML model servers**.   

Alex Vikati wrote about [*The Rise of the Model Servers*](https://medium.com/@vikati/the-rise-of-the-model-servers-9395522b6c58) last year, and things have only accelerated since then.  Put simply, an ML model server simplifies the tasks associated with deploying models and applying them for inference at scale.  Like more traditional application servers, these machines maintain a persistent architecture and typically serve results using robust APIs.  In her post, Vikati highlights five model server projects that graduated from the prototype to the robust phase recently.  Of that group: corporate projects like [TensorFlow Serving](https://www.tensorflow.org/serving/) and [MXnet Model Server](https://aws.amazon.com/blogs/machine-learning/introducing-model-server-for-apache-mxnet/) are especially attractive options for many enterprise teams seeking to productionize ML workflows.  

Other projects such as [Clipper](http://clipper.ai/) and [PredictionIO](https://predictionio.apache.org/) should also be mentioned in this conversation, but as they are less mature offerings and may appeal to a smaller audience, this article will not explore their capabilities in depth.

### Saying Hello to the World is easy when you have community

Getting started with TensorFlow Serving or MXNet Model Server only takes a couple lines of code and a few commands, and both server architectures provide immediate reproducibility and enhanced security, while allowing for easy version control and code isolation.  What's more, both architectures are well maintained by both the corporate dev teams and a robust user community focused on extending and optimizing their use.

 **Note:** just because a data science practitioner has the power to create an ML pipeline, doesn't mean that CI/CD practices should be sidestepped.  Also, scheduling and managing a few pipelines is quite different than hundreds or thousands of parallel jobs in production.

#### TensorFlow Serving

[This example](https://medium.com/tensorflow/serving-ml-quickly-with-tensorflow-serving-and-docker-7df7094aa008) from the Google Brain team quickly demonstrates the capability of TensorFlow Serving by applying a pre-trained ResNet to the task of image classification.  In this demo, the saved ResNet model is served to an image of a cat one hundred times using the Docker-based TensorFlow serving image.  The output provides the average server latency, as well as the prediction class for the image.

The relevant portion of the demo's python script is below: 

```console 
# Start the model Server

docker pull tensorflow/serving

docker run -p 8501:8501 --name tfserving_resnet \ 
--mount type=bind,source=/tmp/resnet,target=/models/resnet \
-e MODEL_NAME=resnet -t tensorflow/serving &
```

```python 

# Prerequisites include: installing and configuring Docker, downloading the TensorFlow serving image and then telling attaching the serving image to a pretrained ResNet model
# model with the name "resnet" and using the predict interface.

                                                                                                                                                                 
total_time = 0
num_requests = 100
for _ in xrange(num_requests):
    response = requests.post(SERVER_URL, data=predict_request)
response.raise_for_status()
total_time += response.elapsed.total_seconds()
prediction = response.json()['predictions'][0]

# Send few actual requests and time average latency.  

print('Prediction class: {}, avg latency: {} ms'.format(
prediction['classes'], (total_time*1000)/num_requests))

```

Though this example is running on a simple laptop configuration, it's not hard to see how this process could (and has been) successfully adopted to many production-level tasks. As mentioned above, the serving layer of an ML application's architecture is only one piece of the deployment puzzle. Two critical capabilities which are not covered in this demo, but would be required in even the simplest of production, are workflow orchestration and scheduling.  


#### MXNet Model Server

Getting started with MXNet's Model server is even simpler, assuming all prerequisites have been met.  To set up the listening function for a simple model server all that's needed is a single CLI command. 

```console

mxnet-model-server --start --models squeezenet=https://s3.amazonaws.com/model-server/model_archive_1.0/squeezenet_v1.1.mar
```

Of note: if your team is using python for ML development, you will need to be able to define and test models using either the Module or Gloun APIs available within the MXNet library.  There are specifications for converting PyTorch models to MXNet using the ONNX format, but many other libraries and model development environments are not supported or not fully supported.  [This quickstart guide](https://github.com/awslabs/mxnet-model-server#quick-start) walks a new user through server setup in a virtual environment, and serves up classification scores from a pre-trained squeezenet model for - you guessed it - images of cats, in JSON format.

![]({{ site.github.url }}/images/2019/01/mikhail_vasilyev_253977_unsplash-1547141586982.jpg)
##### Photo by [Mikhail Vasilyev](https://unsplash.com/photos/NodtnCsLdTE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/)


```console
http://[server location]:8080/predictions/squeezenet -T [image location]/cat.jpg

{
    "class": "n02123045 tabby, tabby cat", 
    "probability": 0.5015981197357178
  }, 
  {
    "class": "n02123394 Persian cat", 
    "probability": 0.3994636833667755
  }, 
  {
    "class": "n02124075 Egyptian cat", 
    "probability": 0.09281889349222183
  }, 
  {
    "class": "n02123159 tiger cat", 
    "probability": 0.004449998494237661
  }, 
  {
    "class": "n02127052 lynx, catamount", 
    "probability": 0.0015094280242919922
  }

```


### Not so fast...

As exciting as these projects are, each of them come riddled with caveats, and actually implementing any of them in a real-world business context still requires significant investment along multiple dimensions:

1. **Integration Headaches**

	One of the biggest challenges lies in integration.  Many of these servers assume specific data ingest and output conventions are in place, which may not have been top of mind during experimentation and model development.

	Ultimately, the commoditized version of a model will not prove to be a truly novel competitive advantage widening IP.  Thus enterprises must invest in scalable yet flexible model service capabilities that can adapt to real-world models that actually impact business outcomes.  

	From a technical standpoint, this flexibility is usually represented by the amount of modeling algorithms supported and/or the performance that the architecture or platform can guarantee from the modeling capabilities that it supports.  Both TensorFlow Serving and MXNet Model server require very specific model "asset" types, which enable each tool to load a model into its particular serving architecture.  This capability comes with the requirement of ensuring that every model to be deployed meets a very specific convention (in these cases building models using either the TensforFlow API or MXNet library).  If your organization is unwilling or unable to adhere to these specifications, the ready-to-use server architectures won't work for you. 

2. **Training & Retraining Data Scientists (not to mention Data Engineers, ML Engineers, Analysts, etc..)**

	Another hurdle in the model serving process is ensuring that everyone involved in developing ML applications (from researchers through DevOps) is fluent in the same technology stack, tools, and environment necessary to utilize a model serving capability.  Though open source projects provide nearly infinite flexibility, the complexity of maintaining a common knowledge base can be daunting.  On the other hand, if an organization decides to go all-in on building out a bespoke end-to-end serving architecture - and invests in the processes, knowledge management, and technology infrastructure to maintain it - it must also commit to training new employees on how to use it.  

	Ultimately, a substantially different skillset may be required of your ML/Data Engineering teams that includes management of network ports, firewalls, server health, and more. 

	Uber shares in its engineering [blog](https://eng.uber.com/michelangelo/) that *"Uber ML education starts during an employee’s first week, during which we host special sessions for ML and Michelangelo boot camps for all technical hires"*.  Not every ML team has the bandwidth or political capital to design, deliver and maintain centralized platform education curriculum in this way.



3. **Knowing what model framework/technique and target production architecture to use**

	One of the best things about the practice of data science is also one of the worst things about it.  Rapid increases in computational power coupled with an explosion of commodity and semi-commodity tools (read: SciKit-learn, TensorFlow, PyTorch, R, etc...) now enable just about anyone to perform previously impossible machine learning tasks.  That same ease of use and flexibility often results in some pretty terrible code and impossible-to-recreate modeling environments.  As mentioned above, both server architectures explored in this post support only a limited number of ML libraries out of the box.  

	As a result, it's unreasonable to expect an IT or Engineering team to select the perfect scalable architecture for production without seriously constraining the experimentation process.

4. **Security and Access Control**

	Though both server architectures provide some inherent security through simple isolation of the ML workload, more robust security features like role-based access control, server hardening, and protection from outside attacks to the model architecture are not in scope for these projects.  To utilize these projects in a production environment out of the box, IT and Engineering teams are required to stitch together additional toolkits, or even develop net new features, in order to meet even simple security and compliance needs.


### So what is both possible *and* reasonable?

A third class of model serving projects are also receiving a lot of development attention: this time from companies seeking to simplify the entire AI/ML development to deployment experience using open source.  These products feature both the flexibility to support popular community-supported ML development libraries, while also adding in enterprise features such as fine-grained security, permissioning, and role-based access control.  

Products like Anaconda Enterprise, Domino Data Lab and IBM's Data Science Experience all provide some method of "serving" ML models - either as fully managed workflows or at least via RESTful APIs - that can be integrated into other applications. 

At Cloudera, [CDSW](https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_overview.html) takes this approach, allowing users to deploy models as RESTful APIs to serve predictions through lightweight automated jobs - and schedule them, too.  One major benefit is that CDSW enables data scientists and engineers to operate on a common software/dependency stack and deploy ML workloads without the need for re-writes or extensive bespoke pipeline development.

Even though model servers and model serving architectures have made tremendous advances in viability recently and should certainly be considered when designing a production environment, it is clear that this area is not fully mature and is certainly not commoditized.  Data scientists, engineers, and leaders must work together to determine what is right for each organization.  

The good news is that Cloudera Fast Forward Labs can help! If selecting a model serving solution is on your mind, be sure to bring it up in your next advising appointment with us.

---

## Recommended Reading

We love to read! Here are a few of our favorite recent finds:

* [Firm Led by Google Veterans Uses A.I. to ‘Nudge’ Workers Toward Happiness](https://www.nytimes.com/2018/12/31/technology/human-resources-artificial-intelligence-humu.html) - a thoughtful look at an effort to use AI for good, with some insightful commentary on ethical considerations.
* [How Much of the Internet Is Fake? Turns Out, a Lot of It, Actually.](http://nymag.com/intelligencer/2018/12/how-much-of-the-internet-is-fake.html) - a great synthesis of a bunch of ideas and criticisms that are probably familiar to anyone working in adtech or otherwise trying to measure engagement on the web (be sure to read in combination with [this twitter thread](https://twitter.com/Chronotope/status/1078003966863200256).
* [The Greatest Trade Show North of Vegas (Pressing Lessons from NeurIPS 2018)] - essential reading for anyone thinking of attending NeurIPS 2019!
* [The Yoda of Silicon Valley](https://www.nytimes.com/2018/12/17/science/donald-knuth-computers-algorithms-programming.html) - The New York Times profiles the legendary Don Knuth

---

## CFFL News

* Join us for an upcoming webinar with Tristan Zajonc on [Cloudera’s New Cloud-Native Platform for Enterprise Scale Machine Learning](https://www.cloudera.com/more/events/webinars/cloud_native_ml_and_data_engineering.html?src=ffl) at 10:00am PT/1:00pm ET on January 15th. You can register [here](https://www.cloudera.com/more/events/webinars/cloud_native_ml_and_data_engineering.html?src=ffl).
* Mike Lee Williams will be speaking on [Federated Learning](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/72661) at the Strata Data Conference in San Francisco on March 27th.

As always, thanks for reading!  We welcome your comments and feedback; reach out anytime to [cffl@cloudera.com](mailto:cffl@cloudera.com).

All the best,

The Cloudera Fast Forward Labs Team
