---
slug: 2019-12-4-public
layout: newsletter
---

Hello!  In this month's edition of the Cloudera Fast Forward Labs monthly newsletter, we share a fun experiment, some thoughts on Google AI's new multi-lingual model for BERT, the first post from an ongoing series on ML privacy considerations, and some recommended reading. This will be our last newsletter for the year, but we'll be back in 2020 with more great content.

Enjoy!

---

## Squote - semantic quote search

##### By [Chris](https://twitter.com/_cjwallace)

Here in the (virtual) Fast Forward lab, we’re currently deep in the topic selection process for FF13. While one or two of our research engineers take the lead for each report, we work through several rounds of discussion and debate about the merits and demerits of topics on our collective radar as a team. Part of that process is dreaming up potential prototype applications.

Our multi-talented designer/dev [Grant](https://twitter.com/grantcuster) proposed a prototype: given a block of text, find a relevant quote.

It tickled my brain in the right way, and I decided to try and build it. When doing speculative work, there is some wisdom in trying to fail fast, and - thanks entirely to the hard work of other people - I had something working a few hours later. So continuing a theme of “staying up late hacking with pre-trained models,” we offer you  [squote.xyz](http://squote.xyz/) , a semantic search tool for quotes.

![]({{ site.github.url }}/images/editor_uploads/2019-10-31-191907-squote_better.gif)

The premise is that given a block of text - say the body of an email, or a paragraph from an essay - Squote should return you a semantically related quote. By “semantically related” I mean “has a similar embedding under BERT, measured with cosine similarity.”  The result is that entering some text about mountains might also return you some text about hills. However, entering a short specific phrase will not necessarily return a block of text containing that phrase: we compare whole text embedding of the input to the whole text embedding of the quote (the whole text embedding being taken here as the mean of the contextual word embeddings). As such, the presence of other words in the quote may reduce its similarity to a phrase it contains.

Useful search is complex and hard, and Squote is intended only as a minimally viable demonstration. Conveniently for the sake of this story, on precisely the same day I put together Squote, Google announced the use of BERT in their search engine in the blog post [Understanding searches better than ever before](https://www.blog.google/products/search/search-language-understanding-bert). Not having had quite the same development investment as Google's search engine, Squote certainly doesn’t always produce a quote that is obviously relevant. At the least though, it’s usually entertaining to ponder why it returned what it did. We hope you have some fun with it.

For those interested, the source is available for your perusal, use, and modification at [cjwallace/squote](https://github.com/cjwallace/squote). Squote is built entirely in python (including the interface) on three open source projects. Essentially all the functionality was provided “out-the-box” by these projects.

* [BERT-as-a-Service](https://github.com/hanxiao/bert-as-service) is exactly what it sounds like! This repo allows you to host the BERT model as a micro-service, and provides a python client that can communicate with the service through a simple API. This reduces getting a state-of-the-art sentence embedding (a length 768 vector) to a simple function call in your python app.
* [FAISS](https://github.com/facebookresearch/faiss) is a library for similarity search and clustering of vector representations. While it’s easy enough to define L2 or cosine similarity in pure python, when matching against embedding vectors representing a corpus of 75,000 quotes, it’s nice to know that someone else has thought about indexing that efficiently. After two native library installs, usage was incredibly straightforward through the python library (thanks to a prebuilt version at [faiss_prebuilt](https://github.com/onfido/faiss_prebuilt)).
* [Streamlit](https://streamlit.io/) is a new library for building interactive machine learning tools with python. It’s spiritually somewhere between RShiny and ObservableHQ, staying in pure python but incorporating a kind of reactive flow - when an input is updated, the output is regenerated without any explicit code to trigger it. It only took a few minutes to feel comfortable with the basic model of Streamlit, and I’ll definitely be keeping it in my toolbox.

The pace of advancement in natural language technology has increased substantially over the past 18 months, and seems impossible to keep up. However, I believe there are hundreds of useful natural language applications that could be built with what is rapidly becoming commodity technology (and maybe some fine tuning)! Happy building!

---

## Bringing Information to Speakers of Less-common Languages

*by [Ryan](https://twitter.com/micallefjd)*

NLP has made major strides forward in the last year or two, and still has a lot of momentum. Now we're starting to see this research applied to universal translation that can reach broad audiences, including speakers of less-common languages.

![]({{ site.github.url }}/images/editor_uploads/2019-11-05-200808-WikipediaLanguages.png)

##### Wikipedia has content in many languages, but the content available in most languages is several orders of magnitude smaller than the most common languages.

New [pre-trained models](https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/) are being released frequently, as well as improved versions of older models - including [smaller versions](https://medium.com/syncedreview/googles-albert-is-a-leaner-bert-achieves-sota-on-3-nlp-benchmarks-f64466dd583) suitable for mobile devices and high-performance applications.

These models are uniformly trained to work primarily or exclusively with text in English and a few other common languages like French, Spanish, or Chinese. These models are broadly useful for tasks *within* these popular languages. But they're *only* useful for single-language tasks in those languages. To bring text information to more of the world's population, though, we need translation models, which of course cover more than one language. 

There are a number of *bilingual* models trained to translate between two (usually popular) languages. This bilingual approach to inclusivity has two big problems. 

* First, to be able to share information *from* any language *to* any language we would need to include bilingual models for every language pair. The number of bilingual models necessary to translate among each language grows quickly. For example, to translate from and to any of one hundred languages we would need 4,950 bilingual models. Managing the training and deployment of thousands of models is a substantial pain.

* Second, models addressing less-common languages are more difficult to train. Less-common languages simply have less data with which to train. Training these models typically requires pairs of sentences in each language. Finding English-Spanish pairs is relatively straightforward compared to finding English-Estonian pairs - and finding Tamil-Basque pairs is even more daunting.

To address these problems, Google AI has released a [multilingual model](https://github.com/google-research/bert/blob/master/multilingual.md) trained on over a hundred languages, with more languages being added regularly. This is itself a great feat and important to speakers (well, writers) of less-common languages.

Part of what's interesting about this multilingual model is, of course, its scope, which makes model training and management simple. But the researchers also used some [clever techniques](https://arxiv.org/abs/1907.05019) to mitigate the lack of training data for the less-common languages. In building their model, they took pages from multi-task learning and transfer learning (both of which are techniques we have [studied here at Fast Forward Labs](https://clients.fastforwardlabs.com/)), and some creative strategies for amplifying the signal. The result is a model with solid performance on less-common resources without giving up much performance on more-common languages. And researchers at Google are [applying these techniques and others](https://arxiv.org/abs/1910.10683v2) to a higher quality set of training data that should improve performance in several NLP tasks, including translation.

These are great steps on the way to bringing ML to *everyone*, and we look forward to seeing what other techniques and data sources will help bring information to people who write in less-common languages.

---

## Privacy-Preserving Machine Learning: A Primer

*by [Melanie](http://www.linkedin.com/in/melanierbeck)*

I have a bit of a cybersecurity background, so a year or two ago I started paying attention to how often data breaches happen - and I noticed something depressing:  they happen literally *3+ times each day* on average in the United States alone!  (For a particularly sobering review check out this [blog post](https://digitalguardian.com/blog/history-data-breaches).) All that information, *your information*, is out there, floating through the ether of the internet, along with much of my information: email addresses, old passwords, and phone numbers. 

Governments, countries, and individuals - with good reason - are taking notice and making changes. The GDPR was groundbreaking in its comprehension, but hardly alone in its goal to shape policy surrounding individual data rights. More policies will pass as the notions of individual privacy and data rights are explored, defined, and refined. 

![]({{ site.github.url }}/images/editor_uploads/2019-11-15-213429-statistic_id273550_cyber_crime__number_of_breaches_and_records_exposed_2005_2018_3ac14829_368a_4064_beb4_7ca3d99e3d78.png)

So what does this mean for machine learning? 

Machine learning algorithms are known to be data-hungry, with more sophisticated ML models requiring ever more data in order to learn their objectives (caveat: your ML model may be learning some unintentional things, too!). With the need for so much data, what happens to the privacy of data as it is processed through an ML workflow? Can data stay private? If so: how, and at what cost?  This article is by no means comprehensive or exhaustive. It is meant to give the reader an introduction to privacy-preserving techniques developed specifically for ML environments. In future articles, I'll dive deeper into each of these security topics.

But before we can understand security and privacy solutions, we have to understand some of the threats to data privacy imposed in machine learning workflows. 

### Threats to Data Privacy from Machine Learning

While there are certainly more threat vectors to ML workflows than we'll cover here, for now we'll focus on those that specifically adversely affect the *privacy of the underlying data* used to train an ML model. That means we won't cover other types of attacks (like adversarial attacks or data poisoning) as these pertain more to the *security of the model itself,* rather than to the underlying data on which the model was trained. 

There are three general steps when developing a machine learning application: extracting features from raw data, training a model on the extracted features, and publishing the trained model in order to gain new insights (wouldn't be much good if the trained model sat unused on a dusty server somewhere!).  Privacy concerns lurk at each step in the process. Here we'll cover three of the most common attacks on data privacy in machine learning. 

![whattttt]({{ site.github.url }}/images/editor_uploads/2019-11-15-213337-Screen_Shot_2019_11_12_at_11_5febf957_d7f6_46e8_b3a4_19ce02cc1a44_52_39_AM.png)
*Threats inherent within the machine learning development cycle.
Source: [Privacy-Preserving Machine Learning: Threats and Solutions](https://arxiv.org/pdf/1804.11238.pdf)*


#### Reconstruction Attacks

A reconstruction attack is when an adversary gains access to the feature vectors that were used to train the ML model and uses them to reconstruct the raw private data (i.e., your geographical location, age, sex, or email address). For this attack to work, the adversary must have white-box access to the ML model; that is, the internal state of the system can be easily understood or ascertained. Consider, for example, that some ML models (SVMs, KNNs) store the feature vectors as part of the model itself. If an attacker were to gain access to the model they would also have access to the feature vectors, and could very likely reconstruct the raw private data. Yikes! 

#### Model Inversion Attacks

A model inversion attack is one step removed from the above. In this attack, the adversary only has access to the results produced from the trained ML model, not the feature vectors. However, many ML models report not only an "answer" (label, prediction, etc.) but a confidence score (probability, SVM decision value, etc.). For example, an image classifier might categorize an image as containing a panda with a confidence of .98. This additional piece of information - the confidence - opens the door for a model inversion attack. The adversary can now feed many examples into your trained model, record the output label and confidence, and use these to glean what feature vectors the original model was trained on in order to produce those results.  

#### Membership Inference Attacks

This is a particularly pernicious attack. Even if you keep the original data and the feature vectors completely under lock and key (e.g., encrypted), and only report predictions without confidence scores, you're still not 100% safe from a membership inference attack. In this attack, the adversary has a dataset and seeks to learn which of the samples might have been used to train the target ML model by comparing its output with the label associated with that sample. This type of attack is illustrated in the dashed box in the figure above. Successful attempts allow the attacker to learn which individual records were used to train the model, and thus identify an individual's private data.  

Anonymization is a privacy technique that sanitizes data by removing all personally identifying information before releasing the dataset for public use. Unfortunately, this technique also suffers from membership inference attacks as was demonstrated in the now-infamous [Netflix case](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf). The attack is the same as in the ML case: in that instance, researchers used the open-source internet movie database (IMBD) to infer the identities of the anonymized Netflix users. 

### Privacy-Preserving Solutions

So what are responsible machine learning enthusiasts and practitioners to do? Give up and go home? Obviously not. It's time to present some solutions! While some of these ideas aren't particularly new (for example, differential privacy has been around for at least 13 years), developing practical ML applications with these privacy techniques is still a nascent area of research. And while there is not yet a one-stop-shop for ML privacy on the market, this is definitely a space to follow as developments come quickly. 

#### Secret Sharing  and Secure Multi-Party Computation

Secret sharing is a method of distributing a secret without exposing it to anyone else. At its most basic, this scheme works by assigning a "share" of the secret to each party. Only by pooling their shares together can they reveal the secret.  This idea has been developed into a framework of protocols that make secure multi-party computation (SMPC) possible.  In this paradigm, the "secret" is an entity's private data which is typically secured cryptographically. SMPC then allows multiple entities (banks, hospitals, individuals) to pool their encrypted data together for training an ML model without exposing their data to other members of the party. This type of protection strongly protects against reconstruction attacks. 

![]({{ site.github.url }}/images/editor_uploads/2019-11-15-213250-Screen_Shot_2019_11_12_at_1_1b3ba742_8fbd_43d4_9ff5_4ecd0227dd0c_50_42_PM.png)
##### [source](https://www.tno.nl/en/focus-areas/information-communication-technology/roadmaps/data-sharing/secure-multi-party-computation/)

#### Differential Privacy

To thwart membership inference attacks, differential privacy (DP) was born. DP provides mathematical privacy guarantees and works by adding a bit of noise to the input data in order to mask whether or not a specific individual participated in the database. Specifically, the output will be statistically identical (within a very small tolerance) whether or not an individual's data was present in the computation.  However, the system has limits: every time the database is questioned (output is computed), a bit more privacy is lost. If the adversary is allowed to ask infinitely many questions of the database they could eventually successfully execute a membership inference attack. Thus, setting the "privacy budget" - or how much total information leakage is allowed - is crucial for safeguarding the data.   

#### Homomorphic Encryption

Standard encryption techniques effectively make data completely useless. You can't glean any information from it; you can't compute on it; you can't learn anything from it. That's the point! It's private, unless you know the encryption key. But wouldn't it be great if there was a type of encryption that could keep data private but still allow basic computation without access to the secret key? Turns out, this exists and it's called homomorphic encryption (HE). This type of encryption allows very simplistic computations to be performed on the encrypted data (addition, multiplication). When combined into more complex series of computations, simple ML models can be built and trained. There are drawbacks, of course - HE is computationally expensive. 

#### Hardware-based Approaches

The methods discussed so far have all relied on algorithms and software to achieve data privacy, but that's not the only approach. Security for code, data, and access can also be achieved through hardware such as [Intel's SGX](https://software.intel.com/en-us/sgx) processor. Essentially, its a standard CPU with beefed-up security instructions that allow developers to define private regions of memory, called enclaves, in the computer. These memory regions are internally encrypted and, as such, resist modification, reading, or tampering from external users or any processes existing outside the enclave.

### Closing Thoughts

There's a big world of privacy-preserving machine learning out there and this article just scratches the surface. The important take-away is that data privacy is a big deal   - and getting bigger each year - as more and more high profile data breaches occur. It's important to make thoughtful choices about how your ML applications will put privacy into practice. Different ML workflows will require different privacy paradigms, so thinking about this before jumping in is critical. The big questions you should ask of your ML application: 

1. Who holds the data and what level of security is appropriate? 
2. Who computes on the data (trains the ML model) and should they have access to the data? 
3. Who hosts the trained ML model and does it require additional security to prevent inadvertent information leakage?   

The answers to these questions will hopefully point you in the direction of one (or more!) of the privacy-preserving solutions presented above. 

Here's to a more secure world!

---

## Recommended Reading

#### [ML beyond Curve Fitting: An Intro to Causal Inference and the do-Calculus](https://www.inference.vc/untitled/) 
I worked through this article in detail, taking notes, and left convinced that causal inference is a useful and practical framework for thinking about problems. It also helped me understand where Judea Pearl’s do-calculus fits in the broader ML picture. I certainly have details to fill in with more reading, but I substantially enhanced my reasoning toolbox by reading this article alone. Very highly recommended. - *[Chris](https://twitter.com/_cjwallace)*

#### [Evading Real-Time Person Detectors by Adversarial T-shirt](https://arxiv.org/abs/1910.11099) 
A fun read! Instead of holding a cardboard with an adversarial patch, we can now wear a T-shirt and go undetected. It's a physical adversarial wearable! - *[Shioulin](https://twitter.com/shioulin_sam)*

#### [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html)
A long read with interesting insights. This paper introduces a framework for applying CD methods to machine learning applications. - *[Keita](https://twitter.com/keitabr)*

#### [Auptimizer - an Extensible, Open-Source Framework for Hyperparameter Tuning](https://arxiv.org/abs/1911.02522) 
Great overview highlighting the difficulties of training hyperparameters for production ML. Their solution provides data scientists more flexibility, usability, scalability and extensibility in training production ML while also addressing reproducibility - crucial to successful implementation. - *[Melanie](https://www.linkedin.com/in/melanierbeck/)*

#### [Museum of the Future: The building designed by an algorithm](https://www.bbc.com/future/article/20191028-museum-of-the-future-the-building-designed-by-an-algorithm)
A fascinating and insightful read, highlighting the ways technology and an eye towards sustainability are changing the future of architecture.  [The Museum of the Future](http://www.museumofthefuture.ae/), scheduled to open in Dubai next fall, aims to be a cultural center of creativity and hope. I'm definitely adding a visit to my bucket list! - *[Danielle](https://www.linkedin.com/in/daniellethorp/)*

#### [Twitter thread: programming book recommendations](https://twitter.com/dan_abramov/status/1190762799338790913)

A thread of responses to Dan Abramov’s (React.js developer) request for books on programming at a conceptual level. So far I’ve been enjoying [A Philosophy of Software Design](https://www.amazon.com/Philosophy-Software-Design-John-Ousterhout/dp/1732102201) and saved a bunch more of the suggestions to check out later. - *[Grant](https://twitter.com/GrantCuster)*

#### [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)

The [lottery ticket hypothesis](https://arxiv.org/abs/1803.03635) suggests that there exist smaller subnetworks within large models, such that if trained, can achieve the same performance with a fraction (10 - 20%) of the original model size. These subnetworks, known as _winning tickets_, are those who have _won_ the initialization lottery: their connections have weights that make training particularly effective.

In the current paper, the authors provide an extension of this idea, scaled up to very deep networks, trained on complex tasks such as imageNet. This line of research is valuable as it can lead to new energy efficient methods for compressing models, and reduce the massive compute resource requirements needed to train high performance neural networks. - *[Victor](https://twitter.com/vykthur)*

#### [What BERT Is Not](https://podcasts.google.com/?feed=aHR0cDovL2RhdGFza2VwdGljLmNvbS9mZWVkLnJzcw&episode=OTE0M2JkYjkyNDY1NDYyNGE0NzUxOGIwOWE5OWJkNGM&hl=en-GB&ved=2ahUKEwjXtpfFrtvlAhVUtnEKHfRcBm8QiOUEKAJ6BAgEEAc&ep=6&at=1573241854670)
Following [Alice's newsletter article](https://blog.fastforwardlabs.com/2019/08/28/is-machine-learning-research-moving-in-the-right-direction.html) from a few months back, I have found myself spending more time reading up on some of the more skeptical takes (with Gary Marcus leading the pack) on the likelihood that the approaches receiving the most attention in terms of research funding (such as deep learning) will lead us to the promised land of Artificial GeneraI Intelligence. I think that's why I was especially intrigued when I heard Allyson Ettinger talking on the [Datasceptic podcast](https://podcasts.google.com/?feed=aHR0cDovL2RhdGFza2VwdGljLmNvbS9mZWVkLnJzcw&episode=OTE0M2JkYjkyNDY1NDYyNGE0NzUxOGIwOWE5OWJkNGM&hl=en-GB&ved=2ahUKEwjXtpfFrtvlAhVUtnEKHfRcBm8QiOUEKAJ6BAgEEAc&ep=6&at=1573241854670) about the research (detailed in their paper: [What BERT Is Not](https://arxiv.org/abs/1907.13528))  that she and colleagues had done into BERT's strengths and weaknesses.  

What I found most interesting about the paper and Allyson's description of the research were:

* the value of a multidisciplinary approach to research (the impact of her training really came through as she discussed the importance of understanding the linguistic capacities with which the pre-training processes imbue a model)
* the idea that understanding how BERT's weaknesses manifest might tell us something about where we could/should be focusing research and, possibly, help to iterate our theories of machine learning

The paper is definitely worth a read, if only so you can better assess the suitability of BERT for your NLP use cases. - *[Ade](https://twitter.com/Adewunmi)*

#### [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)

In reading up on causation, which has been on many of our minds, I encountered Simpson's Paradox, an apparent incongruity in statistical analysis that can arise when analyzing data a different levels of granularity. Simpson's paradox appears when results at one level of granularity are in opposition to results at another level of granularity. This could be useful for deciding which data to include in or exclude from an analysis. -  *[Ryan](https://twitter.com/MicallefEsq)*

---

And that's a wrap!  Wishing all of you a joy-filled holiday season and a happy new year.

All the best,

The Cloudera Fast Forward Labs Team