---
slug: 2019-04-24-client
layout: newsletter
---

# AutoML, open and closed
*by [Chris](https://twitter.com/_cjwallace)*

The trouble in writing about AutoML is one first has to decide what AutoML is. Is it a catchall for automating any part of the machine learning process? Is it those things discussed in the [ICML workshop](https://www.automl.org/events/) of the same name? Is it a specific product? The answer to all of those questions is [yes](https://blog.fastforwardlabs.com/2017/11/30/the-promise-of-automated-machine-learning-automl.html).

Beginning with the last definition, Google recently enhanced their AutoML offering with [Tables](https://cloud.google.com/automl-tables/), available in beta form. Whereas Google’s previous AutoML functionality was limited to very specific applications such as images or speech, AutoML Tables allows users to construct automated machine learning on generic tabular data. The promise of the tool is to enable domain experts and citizen data scientists with labelled data to create predictive pipelines without having a deep understanding of machine learning.

Here at Cloudera Fast Forward Labs, we’re duly cautious of such black box approaches. Quoting the conclusion of our Interpretability report:

> Interpretability is a powerful and increasingly essential capability. A model you can interpret and understand is one you can more easily improve. It is also one you, regulators, and society can more easily trust to be safe and nondiscriminatory. And an accurate model that is also interpretable can offer insights that can be used to change real-world outcomes for the better.

By implication, a non-interpretable model suffers from the reverse effects, being harder to improve and offering little insight other than their exact output. Most dangerously, lack of interpretability can enable dangerous biases to exist without detection. Google is clearly aware of the problem, and it’s encouraging to see the company [releasing advice](https://cloud.google.com/inclusive-ml/#predict) to help users of their AutoML product create systems that are fair and ethical.

There are model-agnostic [methods](https://blog.fastforwardlabs.com/2018/07/31/progress-in-machine-learning-interpretability.html) of gaining local, decision level explanations for black box model outputs; however, even without interpretability (and with due warnings issued), black box AutoML tools are not without their use. If nothing else, automated tools allow a practitioner to get a bearing on how amenable to machine learning a problem is. Rapid evaluation of predictability could be a powerful enabler for teams of data scientists faced with many business problems.

At the other end of the openness spectrum, AutoML - used in its broadest sense of automating parts of the machine learning workflow - is empowering “developer data scientists” to be more productive. There is a maturing ecosystem of open source tools in the space. For instance, there are several python libraries that already support highly automated end-to-end workflows:

* **[TPOT](http://epistasislab.github.io/tpot/)**: TPOT calls itself your “Data Science Assistant.”  It uses genetic programming  to automatically explore and optimize feature selection and processing, model selection (from sklearn models) and parameter tuning. As well as providing the sklearn model as a python object, it can export code for the winning combination.
* **[AutoSklearn](https://automl.github.io/auto-sklearn/master/index.html)**: Similar in nature to TPOT, but leveraging ensemble and meta-learning approaches in place of genetic programming, as outlined in the paper [Efficient and Robust Automated Machine Learning](https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning).
* **[automl-gs](https://github.com/minimaxir/automl-gs)**: Automl-gs goes a step further than the above libraries in automating the workflow, and can actually be run in a one-liner at the command line. It takes a csv file and the name of the target column and generates python code for the whole pipeline. It currently supports TensorFlow and XGBoost models.

The above libraries are great for establishing a baseline model and generating the accompanying code to build upon. However, one need not go the whole way to automation in one step. For instance, [Hyperopt](http://hyperopt.github.io/hyperopt/) implements a sophisticated hyperparameter search method, as described in [Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization).  The subsequent paper [Making a Science of Model Search](http://proceedings.mlr.press/v28/bergstra13.pdf) mentions a particularly novel (and increasingly important) application of this kind of optimisation: situations where there are constraints on the model. For example, we may need a classifier with a small enough memory footprint for a mobile device, or have a strict requirement on the time it takes to predict a single instance, and want to maximise accuracy within those constraints.

There is clearly value in automating parts of process of machine learning. While the world at large may not have settled on a single definition of the term AutoML, we expect the use of all flavours of it to grow.

---

## Upcoming Events

* Chris Wallace will be speaking on _[Federated learning: machine learning with privacy on the edge](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74327)_ on May 1st at the [Strata Data Conference](https://conferences.oreilly.com/strata/strata-eu) in London.
* Shioulin Sam will also be speaking at Strata in London on _[Learning with Limited Labeled Data](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74341)_ on May 2nd.
* Hilary Mason will be speaking at [ODSC East](https://odsc.com/boston) in Boston on May 2nd.
* Nisha Muktewar will be speaking on [Learning with Limited Labeled Data](http://vervecon.org/speakers/nisha-muktewar/) at VerveCon in Santa Clara, CA on May 7th.

We'll also be at the Dataworks conference in Washington, D.C. on May 23rd! Here are the talks we'll be giving:
* Hilary Mason: _[Building an Enterprise AI Factory](https://dataworkssummit.com/washington-dc-2019/keynote/building-an-enterprise-ai-factory/)_
* Tristan Zajonc: _[Cloud-Native Machine Learning: Emerging Trends and the Road Ahead](https://dataworkssummit.com/washington-dc-2019/session/cloud-native-machine-learning-emerging-trends-and-the-road-ahead/)_
* Nisha Muktewar: _[Learning with Limited Labeled Data](https://dataworkssummit.com/washington-dc-2019/session/learning-with-limited-labeled-data-2/)_
* Alice Albrecht: _[A Framework for Developing a Winning Data Project Portfolio](https://dataworkssummit.com/washington-dc-2019/session/a-framework-for-developing-a-winning-data-project-portfolio-2/)_
* Justin Norman and Sagar Kewalramani: _[Machine Learning Model Deployment: Strategy to Implementation](https://dataworkssummit.com/washington-dc-2019/session/machine-learning-model-deployment-strategy-to-implementation-3/)_

If you're attending any of these conferences, please [let us know](mailto:cffl@cloudera.com) and stop by to say hello! 


All the best,

The Cloudera Fast Forward Labs Team