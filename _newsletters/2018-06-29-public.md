---
layout: newsletter
slug: 2018-06-29-public
---

Greetings from Brooklyn, NY!  In this month's newsletter, we discuss ML for surgical tool detection, some new developments since our initial report on [Deep Learning](https://www.fastforwardlabs.com/research/FF03), and share some fun links to things we've been reading.  We're also thrilled to announce that our newest report on Multi-Task Learning will be finished soon; please join us on July 11th for [a webinar](https://info.cloudera.com/LP=2027?elq_id=CCLOU000007199414) on this topic - and in the meantime, you can catch a sneak peek of some of the things we'll be sharing in the webinar here.

---

## Where is my scalpel? ML for surgical tool detection

Surgical tool detection has a multitude of useful applications ranging from generating operation reports, reconstructing 
surgical workflows, coming up with intervention systems that would provide real-time recommendations or warnings to the 
surgeon, inferring surgery phases based on tool presence and others. The construction of such a detection system is guided
by a wide array of experiments that explore different design decisions.

A recent [paper](https://arxiv.org/pdf/1805.05760.pdf) discusses possible solutions and employs a 50-layer residual network (ResNet) architecture that can distinguish 21 different tools in [cataract surgery videos](https://cataracts.grand-challenge.org/data/). The authors explore various transfer learning approaches, either in the form of fine tuning (FT) or as a fixed feature extractor (FFE). When working with the FT network family, it uses all 49 convolutional layers of ResNet and the output feature maps of the 49th convolutional layer are either fed into the *avg-fc* or *conv-max* classification head as shown in the figure below. The *avg-fc* classification head is a standard global average-pooling followed by a fully connected layer and the *conv-max* uses a fully convolutional network and a global max-pooling layer. In general, all weights in this network family are trainable but for some experiments the first k layers of ResNet are frozen. Training this architecture requires a significant amount of memory due to the many trainable weights and input image resolution. On the other hand, the FFE network family uses the first k layers as a feature extractor with fixed weights. The resulting feature maps are fed to a max-pooling layer and three layers of convolutions with 384 feature maps each. Like for the first network family, the final part is either the *avg-fc* or *conv-max* classification head. Training this architecture is relatively inexpensive in terms of required memory because all ResNet weights are fixed and only the custom layers are trained. 

![]({{ site.github.url }}/images/2018/05/Retained_Medical_Devices_Surgical_Errors_Operation_Tools_Left_Inside_Body_X_Ray-1527712910412.jpg)
##### Don't lose your scalpel, or your scissors.

In both network families each output node corresponds to a predicted score for one of the *c* surgical instruments. Because the instruments are not mutually exclusive, the binary relevance transformation is used so that the task is treated as *c* separate binary classification problems.

Prior to the training process various pre-processing strategies are applied to the dataset. For instance, due to the nature of video, subsequent frames tend to be extremely similar to one another. Therefore, only every sixth frame of each video is used. Further, since a predecessor or successor of a frame is part of the training set, the validation error would be a significant underestimation of the test error if one were to split all the data randomly into train-validation. Hence, instead a split at the video level is performed. In addition, because some tools only appear in a single training video the variability between the frames showing such tools is low. To tackle this problem, several image augmentation techniques are employed.

In the end, the FT model instances perform better than the FFE ones which seems plausible as the cataract dataset is different than ImageNet. The *conv-max* classification head does not seem much useful and instead using a weighted loss fuction slightly improves the performance. The resulting network works exceptionally well for some tools but performance suffers in other cases because not enough training data is available. Nonetheless, it explores interesting solution design decisions!

---

## Deep Learning Vendor Update: Hyperparameter Tuning Systems

In our [research reports](https://www.cloudera.com/products/fast-forward-labs-research/fast-forward-labs-research-reports.html), we cover "the recently possible," and what makes "the recently possible" possible. In addition to a detailed how-to guide of new machine learning capabilities, each of our reports contains a section on open source projects, commercial offerings, and vendors that help implement this new machine learning capability to realize the opportunities opened up by technological innovation. We like to keep an eye on the the technologies we research, of course. Our report on deep learning (for image classification) was published a few years ago, and we have seen noteworthy new developments.

Training neural networks requires a set of "hyperparameters" that guide the training process, and sometimes aspects of the network itself (such as the number of layers of the network or the number of nodes per layer). Hyperparameter tuning is important because hyperparameters have substantial effects on the quality of the trained model.

![Tuning hyperparameters is important to a neural net's performance]({{ site.github.url }}/images/editor_uploads/2018-06-19-231235-Compressorhead___Fingers_on_Gibson_Flying_V_Bones_on_Fender_Precision_Bass___Musikmesse_Frankfurt_2013.jpg)

Hyperparameter tuning has generally been at least as much of an art as a science. Machine learning specialists select preliminary hyperparameters, train the model with those hyperparameters, and evaluate the performance and characteristics of the trained model. Then they adjust the hyperparameters (applying some educated guesses), retrain the model with those hyperparameters, and evaluate again. This process is repeated until the specialist is satisfied with the model.

Several new options promise to automate at least a part of the work of hyperparameter tuning, freeing up the personnel building the model, and possibly finding better hyperparameter settings than a human might. They generally work by automating the hyperparameter selection, model training and evaluation, and hyperparameter adjustment, searching for better model performance. These automated hyperparameter tuning packages include offerings from major cloud services providers (e.g., SageMaker from Amazon and Azure ML Studio from Microsoft) as well as smaller independent vendors (e.g., [Comet.ml](https://www.comet.ml/), [Weights & Biases](https://www.wandb.com/), and [SigOpt](https://sigopt.com/)). Afficionados of open source projects may enjoy [Hyperopt](https://github.com/hyperopt/hyperopt), a python library for "serial and parallel optimization over awkward search spaces." As anyone who has ever embarked on a quest for optimal hyperparameters can tell you, "awkward" is a good description of the process of searching for the optimal hyperparameters of neural networks.

For a thorough overview, Jesus Rodriguez rounds up automated hyperparameter tuners in a recent [Hackernoon article](https://hackernoon.com/hyperparameter-tuning-platforms-are-becoming-a-new-market-in-the-deep-learning-space-7106f0ac1689). Hyperparameter tuning is one of the more easily automated aspects of machine learning (in contrast to, for example, feature engineering). In most practical settings, careful feature engineering requires an understanding of the organization and business, data collection protocols, as well as data science tools and algorithms; it is hard to automate with out-of-the-box solutions. We welcome appropriate automation; automated hyperparameter tuning promises to cut down development time of neural networks and lead to better results.

---

## Sequence labeling with semi-supervised multi-task learning

Sequence labeling tasks attempt to assign a categorical label to each member in
the sequence. In natural language processing, where a sequence generally refers
to a sentence, examples of sequence labeling include named entity
recognition (NER), part-of-speech tagging (POS) and error detection. NER, as the
name implies, tries to recognize names in a sentence and classify them into
pre-defined labels such as *Person* and *Organization*. POS tagging assigns labels
such as *noun*, *verb*, and *adjective* to each word, while error detection identifies
grammatical errors in sentences. In many of these tasks, the relevant labels in
the dataset are very sparse and most of the words contribute very little to the
training process. But why let the data go to waste? 

[A recent
paper](https://arxiv.org/abs/1704.07156) proposes using multitask learning to
make more use of the available data. In addition to assigning labels to each
token (or words, loosely), the authors propose a model that also predicts the
surrounding words in the dataset. By adding the secondary unsupervised
objective, "_the model is required to learn more general patterns of semantic and
syntactic composition, which can be reused in order to predict individual labels
more accurately_".

For the sequence modeling neural network, the authors take one
sentence as input and use a bidirectional Long Short Term Memory network (LSTM) to assign a label to every token in the
sentence. Each sentence is first tokenized and the resulting tokens are mapped into a
sequence of word embeddings before being fed into the LSTM. Two LSTM components,
moving in opposite directions (forward and backward) through the sentence, are
then used for constructing context-dependent representations for every word. The
hidden representations from both LSTMs are concatenated in order to obtain a
context-specific representation for each word. This concatenated representation
is passed through a feed-forward layer, allowing the model to learn features
based on both context directions. To predict a label for each token, the authors
use either a softmax or conditional random field (CRF) output
architecture. Softmax predicts each label independently. CRF, on the other hand,
handles dependencies between subsequent labels by looking for the best label
sequence.

To predict the surrounding words, the authors cannot use the concatenated
(forward and backward) representation because it contains information on both
the previous word and next word. Instead, they use the pre-concatenated
version. The hidden representation from the forward-moving LSTM is used to
predict the next word; the hidden representation from the backward-moving LSTM
is used to predict the previous word.

![Architecture of the sequence labeling model with secondary task of predicting surrounding words. The input tokens are shown at the bottom; the expected output labels are at the top.]({{ site.github.url }}/images/2018/06/Screen_Shot_2018_06_14_at_4_27_13_PM-1529008110241.png)

The architecture was evaluated on a range of datasets, covering the tasks of
error detection, named entity recognition, chunking, and POS-tagging. Introducing
a secondary task resulted in consistent performance improvements on every
benchmark. The largest benefit was observed on the task of error detection -
perhaps due to the very sparse and unbalanced label distribution in the dataset. 

---

## Recommended Reading

We love to read (after all, we *are* your nerd best friends); here are some of our recent favorite finds:

* [FPGAs in Data Centers](https://queue.acm.org/detail.cfm?id=3231573)
* [The Visual Chatbot](http://aiweirdness.com/post/175110257767/the-visual-chatbot) - pro-tip: you'll want to sign up for Janelle Shane's updates to get the bonus material for this post - it is **well** worth it.
* [How to Become A Centaur](https://jods.mitpress.mit.edu/pub/issue3-case)
* [AI researchers allege that machine learning is alchemy](http://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy)

---

## CFFL Updates

* Mike will be speaking on probabilistic programming at [QCon](https://qconnewyork.com/ny2018/presentation/modern-cs-presentation-1) in New York on June 28th.

* Friederike will be speaking at the [Research & Applied AI Summit](https://raais.co/) in London on June 29th, and also participating in a panel at [Curious2018](https://curious2018.com/) in Darmstadt, Germany on July 17th.

And, as we mentioned earlier, we do hope you'll join us on Wednesday, July 11th for [a webinar](https://info.cloudera.com/LP=2027?elq_id=CCLOU000007199414) on **Multi-Task Learning: Stepping away from narrow expert models** (the topic of our next report)!


As always, thank you for reading. We welcome your thoughts, feedback, and suggestions; please [drop us a note](mailto:cffl@cloudera.com) anytime!

All the best,

The Cloudera Fast Forward Labs Team
