---
layout: newsletter
slug: 2018-01-31-client
---

Hello!  Welcome to another edition of the Cloudera Fast Forward Labs client newsletter.  This week, we discuss DLib and MaxLIPO+TR, Amazon's latest retail product, and yes, the Super Bowl.  (Don't worry, we nerded out.)

---

## DLib and MaxLIPO+TR

Recently, David King of the infamous DLib project released a new implementation of a fast global optimization algorithm called [MaxLIPO+TR][1], based off of a paper called [Global optimization of Lipschitz functions."][2] The DLib project itself is the home of many machine learning and computer vision algorithms that are used throughout the field, so having this new algorithm implemented is definitely going to help a lot of people.

But you may be thinking: global optimization? Isn't that very hard? And the answer is yes, global optimization is one of those problems that plague pretty much every aspect of machine learning. In fact, many of the technical criticisms of deep learning relate to the fact that stochastic gradient descent doesn't necessarily get you a global minimum, and instead just gives you a local minima (or, even worse, a [saddle point][3]!). The reason it's hard is that you never know if there is a better minima to your current best guess unless you a) have evaluated every other choice (also known as a grid search) or b) have strong assumptions about the way your function behaves.

Evaluating every choice can be prohibitively slow, so making assumptions about our function is necessary. In David King's blog post on the new algorithm, he shows that in 40 evaluations, he is able to get the same error that BayesOpt (another optimization algorithm) takes 80 evaluations to achieve. If each function evaluation takes 15 minutes, that means we can find the solution to the problem overnight instead of having to wait a full day! So in the end, the name of the game is always being as smart as possible with the guesses the algorithm has for what the solution is.

This method tries to reduce the number of guesses for a minima it makes, by using an estimation about how the function behaves. This estimation is done with an approximate Lipschitz constant, which comes with it some constraints.  Most importantly, the function you're optimizing can't be too noisy or discontinuous. This currently rules out the use of this algorithm to deep learning (although there is [work being done][4] to help with that), but keeps it applicable for most of the other analysis work that data scientists do.  Also, the noise constraint is slightly mitigated in DLib's implementation of the algorithm because they chose to add in an additional noise term.

In addition to using this approximate Lipschitz constant to help understand the function being optimized, this method uses [trust regions][5] to keep track of how confident it is that the current proposal for a global maximum is in fact the true global maximum. This helps us stop the algorithm early, so that we don't need to evaluate our potentially time consuming function more than we need to.  In practice, this means we can optimize a function to much higher accuracy in much less time!

In the end, this new algorithm's implementation in a well-used package is another powerful tool in the machine learner's belt. Often, we need to optimize functions that can take quite a long time to evaluate, so anything that can get us more accurate results in less time is helpful in making more robust models.

[1]: http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html
[2]: https://arxiv.org/abs/1703.02628
[3]: http://bair.berkeley.edu/blog/2017/08/31/saddle-efficiency/
[4]: https://arxiv.org/abs/1701.05217
[5]: http://www.applied-mathematics.net/optimization/optimizationIntro.html


---

## Shoplifting Gets a Makeover: Amazon Go

Last week, Amazon opened its latest retail product, [Amazon Go](https://www.amazon.com/b?node=16008589011). Amazon Go is a bricks-and-mortar supermarket with fully automated payment; there is no checkout. Customers scan a QR code from an Amazon app on their way in, they take whatever items they need from the shelves, and Amazon charges them when they walk out.

Amazon doesn't say explicitly how its store works. We only know that they use a combination of computer vision, deep learning, and sensor fusion. Here's how _we think_ it works. The computer vision component uses [extensive camera arrays](https://www.geekwire.com/2018/check-no-checkout-amazon-go-automated-retail-store-will-finally-open-public-monday/) to track objects (in this case customers and perhaps products) as they move throughout the store. The deep learning component would enable tracking cameras or shelf-mounted cameras to identify _which_ products have been selected. This is a good application for deep learning; accurately classifying limited sets of objects (i.e., products) plays to deep learning's strengths. (We explored this technology in an [earlier research report](https://www.fastforwardlabs.com/research/FF03).) Sensor fusion in this case likely refers to a combination of the above information with, e.g., weight sensors that detect when a product has been picked up from (or replaced on) a shelf. Thus, the weight sensor would identify the weight change, while a local camera could confirm that a product of that weight was selected, and the camera arrays collectively could confirm that the product was taken, and by which customer.

![]({{ site.github.url }}/images/2018/01/Shoplifting_sign_cuffs1-1517354725376.jpg)

So, Amazon has pulled off a neat trick. But why? And what does it mean?

Amazon's [acquisition of Whole Foods](https://www.forbes.com/sites/gregpetro/2017/08/02/amazons-acquisition-of-whole-foods-is-about-two-things-data-and-product/#fb68024a8084) is a clear signal that they are interested in the grocery market. It's a regular, cyclical business, and it offers a wealth of data about customers. Given [Amazon's track record and the economics underlying the grocery industry](https://stratechery.com/2018/amazons-go-and-the-future/), there's good cause to think Amazon will apply the lessons and data it collects from a first store to spread widely, and quickly.

Of course, as we note in each of our reports, there are ethical concerns involved in any new technology.

One often-cited concern about "artificial intelligence" is that "the machines will take our jobs." This concern is often overstated, but in this case, may be legitimate. If Amazon can bring the cost and reliability of this technology to the right levels, it could obviate or displace many of the [3.5 million Americans](https://slate.com/business/2018/01/will-amazon-go-eliminate-cashier-jobs.html) who work as cashiers. 

Obviously, the data Amazon intends to gather has implications for privacy. Given the personal nature of the groceries and products we consume daily and weekly, it's easy to see how machine learning could be [used - and abused](https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/#583b9ac86668) - to sell more product.

But on a brighter note, these technologies dovetail with others to enable or get us closer to other new capabilities.

Amazon Go's fine-grained understanding of the locations of products in its store allows it to guide customers to a given aisle, shelf, and shelf spot for a particular product. Knowing which products a customer or family usually buys could light up a given shelf space, offer a coupon, or suggest alternative products. Similarly, selecting a recipe could add the components to a shopping list to be selected by the customer, possibly with the help of such in-store guidance. (This would be a fun inversion of the classic [traveling salseman problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem)!).

Going one step further, the sensor and deep learning technologies used by Amazon Go could finally enable the [long-awaited](http://www.independent.co.uk/arts-entertainment/science-a-smart-house-that-does-everything-voice-activated-curtains-and-robots-that-know-whats-in-2316537.html) dream of a [smart refrigerator](https://newatlas.com/go/1132/) and cabinet that can [tell you what it contains](https://www.technewsworld.com/story/31239.html). Coupled with the recipe and in-store guidance functions above, this would enable a customer to pick a recipe and only buy what they didn't already have. And, of course, help you [remember the milk](https://www.rememberthemilk.com/).

We look forward to seeing how this technology evolves, and we'll be there when they open their first Brooklyn store!

---

## SPORTS!

Even those of us at Fast Forward who don’t know the difference between home-running a touchdown and slamming a dunk past a goalie know what the Super Bowl is (mostly because of, you know, the commercials). Super Bowl Sunday is just around the corner, and last year the American Gaming Association [estimated](www.espn.com/chalk/story/_/id/18590533/american-gaming-association-estimates-super-bowl-li-generate-47b-bets-most-illegally-wagered) ~$4.7 Billion was spent on predicting the outcome of this game via gambling (97% of it illegally) so we thought we'd take a look at what’s behind the predictions and the movement of all that money. 

First, we looked for the data sources usable for modeling. A script using [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) for scraping [Pro Football Reference’s](https://www.pro-football-reference.com/) pages was [posted on GitHub](https://github.com/pjoos/NFL_Project) a few years ago. While the [Football Database](www.footballdb.com) has a slightly cleaner site and details like yards after catch for Receiver comparisons, both are “goto” sites for reliable NFL (National Football League) team and individual statistic data. Several other sites offer NFL stats as well, though many are tailored to “Fantasy Football” needs — like [Next Gen Stats](nextgenstats.nfl.com) from the NFL itself — or are simply more focused on team stats for win-loss betting — like [oddsshark.com](oddsshark.com). As with any data set though, it's important understand what the data categories represent - if you don’t know the difference between yards after catch and yards from scrimmage (pro tip: it's not the [software](https://www.wescrimmage.com/)), then it’s probably worth asking the neighbor who wears jerseys on autumn weekends to help with the data exploration and scrubbing. 

It’s also probably helpful to know who the teams in the Super Bowl this year are: the [Philadelphia Eagles](www.philadelphiaeagles.com) and the [New England Patriots](www.patriots.com/).

![]({{ site.github.url }}/images/2018/01/CB-1517354481517.png)

Given that we can get good data from all aspects of the game, we wanted to see what approaches people are using to build models for predicting winners and losers of games. [Linear](pjoos.github.io/2015/11/11/nfl_ml.html), [logistic regression](support.sas.com/resources/papers/proceedings17/2023-2017.pdf), or [Gaussian process](www.cs.cornell.edu/courses/cs6780/2010fa/projects/warner_cs6780.pdf) methods (the last of which we touched on in our [previous newsletter](http://blog.fastforwardlabs.com/newsletters/2018-01-24-client.html)) are among the most frequent examples we found online. The folks at [FiveThirtyEight](http://fivethirtyeight.com/) have been making season-long predictions using an [Elo model](https://projects.fivethirtyeight.com/2017-nfl-predictions/?ex_cid=endlink) while Microsoft’s [Cortana](https://www.bing.com/search?q=who+will+win+the+super+bowl&qs=LS&pq=who+will+w&sc=8-10&cvid=EED2F57A8A094D58A356B8F88FD84146&FORM=QBRE&sp=1)-based system has been used extensively (and successfully) this year to make individual game predictions against the “point spread” based on factors modeled by the Bing search algorithm. That last part sounds like a lot of hand waving from them, but likely means they are building the decisions from a social media aggregator which should closely parallel the general public’s thoughts (and bets) on games. The last of which, in turn, influences the odds provided by “Las Vegas.” Both of these last two models choose the Patriots. 

At last check, New England was listed as a 5-point favorite over the Eagles. In gambler parlance, the “Spread” (a.k.a. the “Line”) is 5 points, with an over-under of 48<sup id="a1">[1](#f1)</sup>. Taken another way, the oddsmakers are expecting a final score near 26-21 or 27-22 in favor of the Patriots. Note though, that because points in football are generally scored in roughly even combinations of 7 and 3 the point totals of 26 and 22, while possible, have low occurrence rates.

Before staking too much money on the predictions, another thing to keep in mind is that the “House” has complete control over the line and the odds - and the House ALWAYS wins. The typical payoff for predicting the winner of a straight bet, a bet covering the spread<sup id="a2">[2](#f2)</sup>, or the over-under, is 10/11. Meaning a bet of $11 will return $10 in addition to the original money wagered. Phrased another way: the house keeps 9% of the bet from the bettors who win and 100% of those who lose. Also, while it’s generally expected that the point spread offered by sports books is tied to the value of the bets coming in for both teams, there’s no law against the casinos modeling behavior of the bettors to maximize profits by learning how much the line has to change for people to change their mind about placing a bet (or how much money they’ll spend). The casinos also offer ridiculous but often tempting proposition bets (“[Prop Bets](http://www.oddsshark.com/super-bowl/props)”) which have almost no basis in learnable behavior. These include the outcome of the pre-game coin-toss, the duration of the national anthem, will Pink be airborne at any point while singing the national anthem, the number of times the temperature in Minneapolis will be mentioned on air, and whether or not certain players will gain more yards in the football game than specific basketball players, golfers, hockey, and rugby teams will score points in their own games on the same day. 

So if you feel like betting and have done your homework, feel free to invest. But if you aren’t really that interested and would rather just frustrate the heck out of die-hard football fans or stats trackers, feel free to pick the teams based on the colors of their uniforms - the Eagles will be wearing [dark green jerseys](http://www.philadelphiaeagles.com/photos/photo-gallery/Eagles-Vs-Rams-December-10/21f639ec-81fb-4bac-a05e-565ffe19331f#425d1324-3ad5-4d99-b6fa-a5dbdd0438e9) [with silver, white and black trim](http://www.philadelphiaeagles.com/photos/photo-gallery/Eagles-Vs-Cowboys-November-19/ac6e88f7-762d-4f3e-8661-0b209cdf2e83#22822512-5ceb-4afb-9c1f-0adb19877cab); the Patriots will wear [white jerseys with silver, blue, and red trim](http://www.patriots.com/galleries/2017/12/18/silvermans-best-presented-carmax-patriots-steelers-1217#626756). Good luck; we're picking the EAGLES!

<b id="f1">1</b> The Over-Under is the expected combined point total for the two teams - frequently bets are made on whether a gambler feels this number is too high (“bet the under”) or too low (“bet the over”).  [↩](#a1)

<b id="f2">2</b> In this case, if the Eagles lose by less than 5 points they are counted as having won.  [↩](#a2)

---

## CFFL Updates

* The next Strata Data Conference is coming up in San Jose; Hilary will be speaking on March 7th, and Mike will be presenting on [Interpretable Machine Learning Products](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/63572) on March 7th as well.

* Mike will also be speaking on interpretability at [Qcon.ai](https://qcon.ai/) on April 11th in San Francisco.

As always, thank you for reading!  We welcome your thoughts, feedback, and suggestions anytime; just drop us a note at clients@fastforwardlabs.com.

Until next week,

The Cloudera Fast Forward Labs Team
