---
layout: newsletter
slug: 2018-08-29-public
---

Hello! 

---

## Hyperparameter Tuning and Meta-Interpretability: Why You Should Track All Your Experiments, Always!

From random forest to neural networks, many modern machine learning algorithms involve a number of parameters that have to be fixed before training the
algorithm. These parameters, in contrast to the ones learned by the algorithm
during training, are called hyperparameters. The performance of a model on a
task given data depends on the specific values of these hyperparameters.

Hyperparamter tuning is the process of determining the hyperparameter values
that maximize model performance on a task given data. The tuning of
hyperparameters is done by machine learning experts, or increasingly, software
packages (e.g., [HyperOpt](http://hyperopt.github.io/hyperopt/),
[auto-sklearn](https://automl.github.io/auto-sklearn/stable/),
[SMAC](https://github.com/automl/SMAC3)). The aim of these libraries is to turn
hyperparameter tuning from a "black art", requiring expert expertise and
sometimes brute-force search, into a reproducible science - reducing the need for
expert knowledge, whilst keeping computational complexity at bay (e.g.,[Snoek,
Larochelle, & Adams;
2012](http://papers.nips.cc/paper/4522-practical-bayesian-optimization)).

Traditionally, hyperparameter tuning is done using grid search. Grid search
requires that we choose a set of values for each hyperparameter and evaluate
every possible combination of hyperparameter values. Grid search suffers from
the *curse of dimensionality*; the number of joint values grows exponentially
with the number of hyperparameters.

In 2012, [Bergstra and Bengio](http://www.jmlr.org/papers/v13/bergstra12a.html)
showed that random hyperparameter search is more efficient than grid search, a
perhaps counter-intuitive result. This is because only a few hyperparameters
tend to really matter for the performance of a model on a task given data.
Grid search tends to spend more time in regions of the hyperparameter space
that are low-performing compared to random search. What's more, random search
allows one to easily add more experiments that explore even more sets of
hyperparameter values without (expensive) adjustment of the grid (most
recently, [sequential
approaches](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)
have shown great promise).

![]({{ site.github.url }}/images/2018/07/Screen_Shot_2018_07_31_at_11_01_41_AM-1533049344437.png)
##### Grid (left) and random (right) search for nine experiments. With random search, all nine trials explore distinct values of the hyperparameters. Random search is more efficient. (Picture taken from Bergstra and Bengio, 2012)

If only a few hyperparameter values really matter, for a given model on a task
given data, what are those parameters and what are their values? Current
software libraries for hyperparameter tuning do not tend to discriminate
important from unimportant hyperparameters and/or do not expose important
parameters and their values. This limits insights into the workings of a
model - which is important for a variety of reasons, as we explain in depth in
our report on [model
interpretability](http://blog.fastforwardlabs.com/2017/08/02/interpretability.html);
interpretability allows us to verify, for example, that a model gives high
quality predictions for the right, and not the wrong, reasons. 

A series of recent papers tackles this *"meta-interpretability" problem*: what
hyperparameters matter for model performance on a task given data? In
[*Hyperparameter Importance Across
Datasets*](https://arxiv.org/abs/1710.04725), Jan van Rijn and Frank Hutter
first evaluate a model on a task given data and a set of randomly chosen
hyperparameters to assess model performance. They then use these
hyperparameters as inputs to a model, a so-called surrogate model, that they
train to predict the oberved performance. Given a trained surrogate model, they
predict model performance for hyperparameters not previously included in their
experiments. Finally, they conduct an analysis of variance (ANOVA) to determine
how much of the predicted model performance by the surrogate model can be
explained by each hyperparameter or combination of hyperparameters. To draw
conclusions across data sets, a more generalizable result, the authors repeat
this procedure across several data sets. For random forests, for example, they
find that only the `minimum samples per leaf` and the `maximum numbers of
features` really matter. This finding is consistent with expert knowledge,
which is great: it validates the method; we can use it to study more
complex models for which we have no such intuition yet while it helps beginners
to get started.

Using a related approach also based on surrogate models, Philipp Probst, Bernd
Bischl, and Anne-Laure Boulesteix
[demonstrate](https://arxiv.org/abs/1802.09596) that some default values of
hyperparameters as set by software packages (e.g., scikit-learn) lie
outside the range of hyperparameter values that tend to yield optimal model
performance across tasks and data; we can use solutions to the
meta-interpretability problem to define better default values, or to define
prior distributions for even more efficient random hyperparameter search
(Bergtra and Bengio sample from a uniform distribution which we can replace by
a "more informed" distribution).

Within organizations, these results suggest that one should track and store the
results of hyperparameter tuning - not only the set of parameters that result in
the best performing model, but *all* results. These results can be used to
train surrogate models that allow us insight into the importance of
hyperparameter values and increase the efficiency of hyperparameter tuning by
defining sensible default values (or distributions) for the classes of problems
tackled by data teams at these organizations.

---

## Breakthroughs in transfer learning for natural language processing

One of the most exciting parts of our jobs at Cloudera Fast Forward Labs is [our work](https://www.fastforwardlabs.com/research) on applied machine learning research. Through this research we see and work with some of the most exciting developments in machine learning, deep learning, and AI, but - as with any field that has been overhyped - we sift through a lot of noise. By noise, we generally mean research that is too immature to be of practical use, or research that follows one or more of the [troubling trends in machine learning](https://arxiv.org/pdf/1807.03341.pdf). 

The research we get really excited about hits a sweet spot of delivering new capabilities that are of practical use to general data science teams. That’s why we’re particularly excited about developments in transfer learning, a technique that allows anyone from ML beginner to expert to deploy state-of-the-art models on challenging tasks like computer vision and NLP. Specifically, there have been several recent results which indicate that transfer learning will have huge impacts in NLP, similar to what we’ve observed in computer vision.

### Transfer learning makes deep learning accessible to everyone

Transfer learning exploits the idea that many machine learning tasks are related to each other, and so the skills required to do well on one task are often transferable to other tasks. This is similar to how humans who learn to throw a baseball do not need to completely re-learn the mechanics of throwing to also throw a football, or how skills developed in learning to speak one foreign language would also be useful in learning another foreign language. Consider some of the common tasks that we might be interested in doing in the field of computer vision:

![]({{ site.github.url }}/images/2018/07/task_dict_v-1532535013679.jpg)

##### Various computer vision tasks. Image credit: [Taskonomy](https://github.com/StanfordVL/taskonomy/tree/master/taskbank).

* Object detection - is there a tv in this image? A bed? A couch?
* Scene detection - what type of room is this?
* Semantic segmentation - locate objects in the image
* Depth estimation - estimate the depth of objects in a 2D image

Intuitively, we know that these tasks require similar capabilities. Being able to pick out which pixels belong to the same object, as in semantic segmentation, is undoubtedly useful in depth estimation, since pixels belonging to one object should have the same or similar depths. Training a single model to do well on any of these tasks from scratch is difficult and requires extensive expertise; however, once such a model exists, we can take that model and transfer the knowledge it contains to our related task, without having to go through the complex and [brittle process](http://www.fast.ai/2018/07/12/auto-ml-1/) of training a model from scratch. 

This gives way to a general strategy: for some family of related tasks, e.g. computer vision tasks, train a model from scratch on a generic task that has plentiful data. That trained model can be applied to related tasks with minimal changes, since it already has much of the general knowledge required for this family of tasks. In computer vision, this generic task has traditionally been training an object detection model on the Imagenet dataset, which is a highly curated dataset of over 14 million images. Classification models trained on Imagenet [tend to learn general features](https://arxiv.org/abs/1311.2901) like how to detect shapes, edges, and higher level objects that are almost always helpful in related computer vision tasks. Training these models from scratch is difficult, requiring both advanced hardware and expertise, but the important thing is that anyone can take the result and use it to do very cool things - for free!

Transfer learning is not a new idea, but it has grown in popularity and importance because of the deep learning boom. Deep learning and transfer learning are a good match for several reasons:
* Transfer learning needs tasks that share the same types of inputs, which is more common when working with raw/unstructured data (text, audio, images)
* Transfer learning eliminates need to train deep models from scratch, which is difficult, time consuming, and notoriously hard to reproduce
* Deep learning is generally extremely data hungry, but transfer learning enables application of deep learning models to small data

### Discovering NLP's Imagenet

Transfer learning helps solve some of the biggest and most prohibitive problems with deep learning, but it has, until recently, mostly been limited to the domain of computer vision. Fortunately, we’re starting to see some breakthroughs in transfer learning in the field of natural language processing. One of the keys to transfer learning is to identify a generic task, with plenty of quality data, that allows us to train from-scratch models that will then transfer well to related tasks. In computer vision this is the Imagenet task, but in NLP there wasn’t an obvious solution, until now. Several recent papers (including [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146) and [Improving Language Understanding
by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)) have shown that language modeling - predicting the next word in a sequence of text - is well-suited as this generic task. They show that next-word prediction forces a model to learn long-term dependencies, hierarchical relations, and sentiment, which are all useful for other NLP tasks. They go on to present results where a pre-trained model is transferred to other tasks, and can beat well established benchmarks in natural language inference, question and answering, sentiment classification, and others. This led one NLP researcher to declare that [“NLP’s Imagenet moment has arrived.”](https://thegradient.pub/nlp-imagenet/)

Transfer learning has been extremely important in lowering the barrier to entry for deep learning in computer vision and there is no reason to think it won’t do the same for NLP. This is exciting, in particular because raw text is often the largest source of available data for many businesses. Nearly every organization can make use of better algorithmic tools for understanding natural language, and we seem to be on the cusp of a revolution in NLP!

---

## Automated Machine Learning: Hype now, reality later?
_by [Shioulin](https://twitter.com/shioulin_sam)_

[Previously in our newsletter](https://blog.fastforwardlabs.com/newsletters/2017-11-08-client.html), we had framed automated machine learning around four notions:
* Citizen Data Science / ML: Automated machine learning will allow everyone to do data science and ML. It requires no special training or skills.
* Efficient Data Science / ML: Automated machine learning will supercharge your data scientists and ML engineers by making them more efficient.
* Learning to Learn: Automated machine learning will automate architecture and optimization algorithm design(architecture search).
* Transfer Learning: Automated machine learning will allow algorithms to learn new tasks faster by utilizing what they learned from mastering other tasks in the past.

Since then, the term *automated machine learning* has been strongly linked to Google's definition of **AutoML as a way for neural nets to design neural nets**, or - expressed technically - as a way to perform neural architecture search. Google's messaging asserts that AutoML will [make AI work for everyone](https://blog.google/technology/ai/making-ai-work-for-everyone/).  [Google Cloud's AutoML](https://cloud.google.com/automl/) beta products now allow one to custom vision, language and translation models with minimum machine learning skills. The product page states that under the hood, this capability is powered by Google's AutoML and transfer learning. But, [as pointed out by fast.ai](http://www.fast.ai/2018/07/23/auto-ml-3/), transfer learning and neural architecture search are two opposite approaches. Transfer learning assumes that neural net architectures generalize to similar problems (for example, features like corners and lines show up in many different images); neural architecture search assumes that each dataset needs a unique and specialized architecture. In transfer learning, you start with a trained model with an existing architecture and further tune the weights with your data; neural architecture search requires training multiple new architectures along with new weights. In practice, one does not need to use both techniques (yet?)! Transfer learning is currently the predominant approach since neural architecture search is **currently** computationally expensive. We very much agree with [fast.ai's assessment](http://www.fast.ai/2018/07/23/auto-ml-3/) that not everyone needs to perform neural architecture search, and the ability to perform such a search does not replace machine learning expertise. In fact, blindly using computation power to search for the best architecture seems to lead us further into the abyss of un-interpretable models.

On the flip side, if we go back in time to the pre-GPU era, one could argue that we are at the same place with neural architecture search as we were back then with deep learning. Sprinkle in the notion of [Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35), and suddenly the idea of everyone designing neural nets for their particular needs looks like a reasonable trajectory!

---

## Understanding a generative space
_by [Grant](https://twitter.com/GrantCuster)_

[Kate Compton](http://www.galaxykate.com/), maker of many interesting things, recently tweeted a diagram from her work-in-progress dissertation zine. The illustration breaks down how we interact with a procedural generator, which can be anything from a photo filter to a character creation tool.

![A diagram. The top half shows the expectation space, and the bottom half the algorithmic space. The process is circular. You start with a mental model, make a hypothesis about what a change will do, then encode the change. In the algorithmic space the model is modified by the change, the code reinterprets the model and displays the results of the change. The user then evaluates the artifact and updates their mental model of how the algorithmic space works. The process restarts.]({{ site.github.url }}/images/2018/08/compton_generator-1533226496607.jpg)

##### ["How we understand a generative space" by Kate Compton](https://twitter.com/GalaxyKate/status/1012788721379303424). Understanding a generative space requires moving between the digital space (where the code is and the results are displayed) and the mental space (where we evaluate the display and make guesses about how it works).

As app creators, we tend to focus on the parts we can see: the user interface and the code that determines app behavior. Those are the parts that we can directly control. Compton’s diagram is a great reminder that there’s another dimension at work in a user’s experience of an app: their mental model of how the application works. We have much less direct control over a user’s mental model. 

The mental model is the user’s "parallel universe" reconstruction of the logic behind an application. As app creators, we try to shepherd the user so their mental model usefully aligns with our own. We employ metaphors, often based on familiar patterns from the physical world or other applications - but there’s always going to be slippage. Part of the fun of designing an application is figuring out how to minimize this slippage, and providing lots of entry points, so that if a user's mental model gets really misaligned, there are spots within the application where expectations can be reset. As Compton’s diagram shows, one of the best ways to encourage alignment of a mental model and the model itself is to allow the user to make changes and quickly see the results. Enabling that sort of iterative play taps into the human impulse to poke things in order to figure out how they work.

Understanding the interplay between a user’s mental model and an application will be especially important for creating [machine learning tools that open up new creative possibilities](http://blog.fastforwardlabs.com/2018/03/28/new-creative-possibilities-with-machine-learning.html). Because of their complexity, it may not be feasible for users to develop a comprehensive mental model of how an ML model works. It will then be up to application creators to guide users towards a simplified or alternate mental model that helps them collaborate fruitfully with the application.

---

## 3D Printed Optical Neural Networks
_by [Micha](http://micha.codes/)_

Researchers at UCLA recently released a new neural network architecture, the
Diffractive Deep Neural Network, or [D2NN][1], whose mathematical underpinnings allow it to be implemented
physically using 3D printed parts and light. The model works by using the
_phase_ of light to represent the state of individual neurons. 3D printed sheets
with various elements to introduce phase shifts into incoming light can be used
to encode the relevant weights of the neural network, allowing us to shine in
our input using light, and seeing the result on the other side. The evaluation happens at the speed of light, and at great energy savings.

![](./images/d2nn_classifier.png)

Being able to 3D print a model and use it to get instantaneous evaluations is
more than just a novelty. Even using low-tolerance 3D printers, the research
group was able to get high quality results, showing the resilience of such a
method to error. This seems to show that we can replace the 3D printed sheets
with programmable [Spatial Light Modulators (SLMs)][3] to create D2NNs without having to go through the time consuming process of 3D printing.
As it stands now, SLMs that can accurately modify light's phases are incredibly
expensive because of their lack of wide-spread use outside of research; they are not
ready for any sort of commodity use. This makes the 3D printing method a great
way to play with the technology until they are ready.

We can imagine hardware modules in the future that will operate similar to FPGAs in
that it will take a bit of overhead to "load the model," but then enable incredibly quick evaluations. With a system like this, the only
speed limitation to model evaluation would be input and readout,
which, using current day commodity projectors, could happen at 60Hz (or 16.67ms
per evaluation), regardless of model size (while generally, light-detection and modulation routinely happens at GHz speeds - for example, in the context of optical telecommunications). 

This sort of model has many interesting properties. First, a neuron in one layer
is not connected to _every_ neuron in the next layer; it is only connected to
neurons where the light can reach. This ends up acting as an effective
[receptive field][2] for the model which can be tuned based on the distance
between the physical layers of the model. This ability introduces a way to tune
the receptive field in order to make it applicable for various different tasks.

Another interesting property is how the output of various neurons interacts as
they make their way to the next layer of the network. Because of their
fundamental nature, the light interacts and creates diffraction patterns, which
is starkly different to how information transmits through conventional neural
networks. While it isn't certain what this could mean for D2NN models, it is
possible that this complex behaviour could be harnessed to give the system more
computational power. In general, these sorts of behaviours are reminiscent of
[Deep Complex][5] and [Deep Quaternion][6] networks that use properties of
complex and quaternion numbers to get increased computational power.

![](./images/d2nn_math.jpeg)

However, there are some unanswered questions about this system. First, while there are a number of well-known approaches, implementing all-optical nonlinear transformations is pretty challenging in practice. In a [previous study][4] featuring a more directly recognizable optical implementation of an all-optical deep neural network, the authors, for good reason, opted to prove their point by simulating the nonlinear transformation in a computer instead. The D2NN paper only briefly
mentioned the issue of non-linearity, but not in enough detail for the reader to fully understand
what is happening.

Secondly, the 3D printed system cannot do model training. In the paper, the
model is first trained on a standard computer using a physical representation of
the system. Once the model converged, it was encoded into a 3D model that could
be printed and tested using optics. As a result, the extreme speed at which the
resulting 3D model can be evaluated is only useful for a phase of model
deployment where extreme speeds are not necessarily required. It is really
during model _training_ where speed is required, since it takes millions of model
evaluations to make the model better.

![](./images/d2nn.png)

All that being said, this is still an incredibly interesting field that seems to
be progressing quickly. There are many directions this research could take (making
the layers programmable, better definition around model non-linearities or even
having continuous systems that don't need discrete layers!) and this paper
does a good job at making tangible what these innovations could mean to the
larger machine learning community.

[1]: https://arxiv.org/abs/1804.08711
[2]: https://arxiv.org/abs/1701.04128
[3]: https://en.wikipedia.org/wiki/Spatial_light_modulator
[4]: https://www.nature.com/articles/nphoton.2017.93
[5]: https://arxiv.org/abs/1705.09792
[6]: https://arxiv.org/abs/1712.04604

---

## Recommended Reading 

Here are a few of our more recent favorite finds:
* [Modeling User Journeys via Semantic Embeddings](https://codeascraft.com/2018/07/12/modeling-user-journey-via-semantic-embeddings/)
* [Design Patterns for Production NLP Systems](http://deliprao.com/archives/294)
* [Program Synthesis in 2017-18](https://alexpolozov.com/blog/program-synthesis-2018/)
* [Design Patterns for Production NLP Systems](http://deliprao.com/archives/294)

---

## CFFL Updates

* Shioulin will be speaking on [Semantic Recommendations](https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/69260) at the Strata Data Conference in NYC on September 12th, and Friederike will be [speaking at Strata](https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/69365) on September 12th as well.

* Hilary will be keynoting at [Strata](https://conferences.oreilly.com/strata/strata-ny) on Thursday, September 13th.

* Friederike will be speaking at the [Data Science Salon](https://www.eventbrite.com/e/data-science-salon-nyc-tickets-40072527007) on September 27th (also here in NYC).

* Shioulin will be speaking at [ODSC Europe](https://odsc.com/london) in London in mid-September.

* Add October dates

As always, thank you for reading. We welcome your thoughts, feedback, and suggestions; please [drop us a note](mailto:cffl@cloudera.com) anytime!

All the best,

The Cloudera Fast Forward Labs Team
