---
slug: 2019-06-26-public
layout: newsletter
---

Hello, and welcome to the June edition of the Cloudera Fast Forward Labs monthly newsletter!

--- 
## The trouble with unexplainable algorithms
_by [Ade](https://twitter.com/Adewunmi)_

Thanks to a mix of technology-driven disruption and savvy competitors, the business environment is an increasingly challenging one. Staying competitive requires a better understanding of customers’ behaviour and preferences. It also requires the ability to optimise internal processes to more efficiently support both these things. This is a big reason that so many organisations are investing in machine learning.

And yet the use of machine learning in organisations has not been universally welcomed by customers and service users. In fact, in some areas, there’s been a growing push back -- or ‘techlash,’ as it has been dubbed. People have expressed concern about a range of things: the breadth of data being collected and the [terms](https://www.nbcnews.com/tech/security/millions-people-uploaded-photos-ever-app-then-company-used-them-n1003371) of its use, as well as the types of decisions being recommended or made by the algorithms trained on this data. The sources of concern vary, but a lack of trust is the common denominator.

### The trust deficit
One corollary of this trust deficit is an increasingly adversarial relationship between organisations and their service users and customers. Lately, I have been thinking about this a lot. It was definitely on my mind when I read about the [legal action](https://www.theguardian.com/technology/2019/may/21/office-worker-launches-uks-first-police-facial-recognition-legal-action) that Ed Bridges has brought against the South Wales police for violating his privacy through their facial recognition trial. Bridges, who is a private citizen, is doing this with the support of [Liberty HQ](https://www.libertyhumanrights.org.uk/news/press-releases-and-statements/liberty-client-takes-police-ground-breaking-facial-recognition) . This is the first case of its kind in the UK, and it comes hard on the heels of San Francisco city legislature’s [decision](https://text.npr.org/s.php?sId=723193785) to ban the use of facial recognition by police and city agencies. Oakland and Massachusetts are also considering similar legislation:

_“Big Sister is watching us," [says Senate Majority Leader, Cynthia Creem], "and yet we don't even know how those pictures are being used ... The system that they're using now raises issues of due process and significant issues with regards to civil liberties." _[(NPR.org)](https://text.npr.org/s.php?sId=723193785)

These are widely reported examples of techlash, but they’re on the same spectrum as smaller scale adversarial action against algorithmically-driven decision-making. This latter behaviour was a topic of discussion in a recent [episode](https://www.listennotes.com/podcasts/sleepwalkers/poker-face-bSmrcn6E1H4/) of the tech podcast, Sleepwalkers. [Lisa Talia Moretti](https://www.gold.ac.uk/institute-management-studies/staff/moretti-lisa/), a guest on the show, [spoke](https://lnns.co/0lIJtPjDcOf/371) about students who figured out that a certain selection algorithm prioritised applications from students from universities such as Harvard, Oxford, or Cambridge. They started writing the names of these universities in white text (so human recruiters couldn’t pick up on it) on their CVs, in order to dupe the algorithm.

![]({{ site.github.url }}/images/editor_uploads/2019-06-07-135738-helloquence_51716_unsplash.jpg)
##### Image Source:  [Helloquence](https://unsplash.com/@helloquence?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/)

Screening out applications from certain candidates may well have been the intent of the organisation(s) using this algorithm. If so, it’s surprising that the human recruiters _didn’t_ immediately pick up on the fact that applications from less desirable candidates were getting through - in which case, they could have simply screened them out at a subsequent stage or been explicit about their preference for candidates from certain universities. My suspicion is that this _wasn’t_ their intention, and that the emergent selection bias or preference was simply a quirk of the algorithm that was left unexplored and unaddressed.

However this state of affairs came about, the low-level adversarial behaviour it has triggered is corrosive - slowly transforming a potentially positive engagement into a grudge interaction.

### Building trust, making friends 
So how _do_ organisations go about (re)building trust and - by extension - resetting the dynamic between themselves and their users/customers? There are some really obvious choices namely, clarity and honesty about the way any collected data will be used, and taking steps to protect that data. In this context, being honest is often a matter of being upfront about business models (in the case of commercial entities) as well as governance and decision-making policies (especially in the case of government), and not trying to sneakily change models and policies through privacy policy updates. 

However, achieving the requirement for clarity is a tougher challenge. There are a number of reasons for this, but I will simply touch on two: 

* Explaining how complex systems interact with the various features of a large dataset - in a way that’s easy for time-poor, non-experts to grasp - is hard.
* Sometimes,(especially in the case of so-called ‘black box’ models) organisations don’t know quite how their algorithms arrive at the results that they do. 

These problems are related: explaining the workings of a system you don’t really understand is a tall order. That’s why attaining this understanding should be a critical area of focus for organisations that use machine learning. Thankfully, approaches for doing this are becoming more mature; this was the focus of our [Interpretability](https://blog.fastforwardlabs.com/2017/08/02/business-interpretability.html) research report.  Understanding the way systems work also paves the way for better, and more considered, trust-building conversations with end-users. (If you’re curious about what this looks like in practice, I recommend reading [this blog post](https://www.projectsbyif.com/blog/learning-through-making-understanding-what-young-people-think-about-ai-and-data-privacy/) about “designing to expose the seams of [automated decision making].” It’s based on a collaboration between the design agencies [Comuzi](https://comuzi.xyz/#) and [Projects by If](https://www.projectsbyif.com/).) 

With the advent of the [General Data Protection Regulation](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation) (GDPR), there has  been a lot of focus on explainability as a regulatory requirement. There is great benefit for organisations in recognising the value of interpretability and explainability as tools for building trust with customers.

---

## Seeing is not necessarily believing

_by [Nisha](https://twitter.com/NishaMuktewar)_

Advancements in machine learning have evolved to such an extent that machines can not only understand the input data but have also learned to create it. Generative models are one of the most promising approaches towards this goal. To train such a model we first collect a large amount of data (be it images, text, etc.) and then train a model to generate data like it.

Generative Adversarial Networks (GANs) are one such class of generative models, that, given a training dataset, learn to 
generate new data with the same statistics as the training set. For instance, a GAN trained on images of dogs can help 
generate new images of dogs that at times may look authentic and have many realistic characteristics. GANs have progressed 
substantially in the last couple of years and have been applauded for their ability to generate high fidelity and diverse 
images. As such, applications of adversarial training have found their way into image translation, style transfer, and more - 
particularly data augmentation. 

So far these models have had limited success in such tasks for large-scale datasets like ImageNet, and that’s mainly because 
the models don’t generate sufficiently high quality samples. A recent model - [BigGAN](https://arxiv.org/abs/1809.11096), 
however, has generated photorealistic images of ImageNet data and has achieved considerable performance improvement when evaluating using traditional metrics like [Inception Score (IS)](https://arxiv.org/abs/1606.03498) and [Fréchet Inception Distance (FID)](https://arxiv.org/abs/1706.08500) compared to the previous 
state-of-the-art. What this means is that BigGANs are capable of capturing data distributions. And if this were true, one 
could then possibly use these generated samples for many downstream tasks, especially in situations where limited labeled data 
is available.

![]({{ site.github.url }}/images/editor_uploads/2019-06-12-143410-BigGANs.png)
##### Image source: [Large scale GAN training for high fidelity natural image synthesis - Brock et al., 2018](https://arxiv.org/pdf/1809.11096.pdf)

A recent [work](https://openreview.net/forum?id=rJMw747l_4) tested whether BigGANs can be really useful for data augmentation, or - more drastically - for data replacement of the original data distribution. The hypothesis the authors wanted to test was that if BigGANs were indeed capturing the data distribution, one could use the generated samples instead of (or in addition to) the original training set, to improve performance on classification. The authors conducted two simple experiments. First, they trained ImageNet classifiers, replacing the original training set with one produced by BigGAN. Second, they augmented the original ImageNet training set with samples from BigGAN. 

Replacing the original training data with BigGAN samples saw a substantial increase (120% and 384%) in the Top-1 and Top-5 classification errors when compared to the model performance on the original training set. Further, augmenting the training set improved the model performance only marginally, while at the expense of more training time. This suggests that **naively augmenting the dataset with BigGAN samples is of limited utility** and more work is required for BigGANs to be actually used in downstream tasks. It also further highlights the need to reflect on **better metrics that could be used to evaluate image synthesis models like GANs**. The current gold standard metrics - Inception Score (IS) and Fréchet Inception Distance (FID) for GAN model comparison could be misleading and are not predictive of data augmentation classification performance.

---

## Recommended Reading


#### [Gary Marcus - Deep Learning: A Critical Appraisal (pdf)](https://arxiv.org/pdf/1801.00631.pdf)

*I've been re-reading this; I'm interested in digging in more around the boundaries of deep learning and thinking about what the next few years will look like in machine learning research. - [Alice](https://twitter.com/AliceAlbrecht)*

#### [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
*This paper introduces the idea of finding subnetworks of a larger neural network that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. - [Victor](https://twitter.com/vykthur)*

##### [Stabilizing the Lottery Ticket Hypothesis (pdf)](https://arxiv.org/pdf/1903.01611.pdf)
*After I read the paper Victor mentioned, I read this, which expands the ideas explored in the other paper beyond network initialization by introducing the concepts of rewinding and stability to enable pruning of large-scale networks--not just small benchmarks. - [Justin](https://twitter.com/JustinJDN)*

#### [How to Do Nothing - by Jenny Odell](https://www.penguinrandomhouse.com/books/600671/how-to-do-nothing-by-jenny-odell/9781612197494/)
*A thoughtful look at how to deal with a productivity-obsessed world. This book went in a lot of directions I wasn’t expecting and gave me a lot to think about (also lots of references to great art to look into). If you want a preview, you can read the [transcript of the talk the book is based on](https://medium.com/@the_jennitaur/how-to-do-nothing-57e100f59bbb). - [Grant](https://twitter.com/GrantCuster)*

#### [Neural Approaches to Conversational AI](https://arxiv.org/pdf/1809.08267.pdf)
*A comprehensive survey paper that discusses various neural approaches to conversational AI  (social chatbots, question answering and task completion) that have been developed in the last few years, the progress made and challenges
that still exist. In addition, gives a brief review of several conversational systems in industry and a discussion of research trends. - [Nisha](https://twitter.com/NishaMuktewar)*

#### [Radford Neal’s PhD thesis: Bayesian Learning for Neural Networks (1995) (pdf)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf)

*Ok, ok, I admit I’m not even nearly through the whole thing yet, but the introduction alone is a study in lucid technical writing. Exciting things happen when we combine principled Bayesian reasoning with the flexible function approximation capabilities of neural networks, and this is a foundational text for that intersection. - [Chris](https://twitter.com/_cjwallace)*

#### [Language, trees, and geometry in neural networks](https://pair-code.github.io/interpretability/bert-tree/)
*This post helps visualize what complex NLP models like BERT have learned. It’s interesting for the visualizations alone, but they go on to show that BERT effectively learns to capture the syntactical structure of sentences. In essence, BERT can do automatically what would normally require a trained human linguist. We are now discovering that much of what makes BERT and similar models perform so well is their ability to extract and represent syntax and structure in language. As humans we know that this structure is important to understanding language, but that these algorithms discover this with no human guidance is compelling. This exploration gives us more concrete insight to “what” these models learn. - [Seth](https://twitter.com/shendrickson16)*

---- 

## Upcoming Events

* Please join us on Thursday, April 27th for a virtual event [Introducing Cloudera Data Science Workbench 1.6 and **new ML services**](https://www.cloudera.com/about/events/webinars/virtual-event-ml-services-cdsw.html?utm_medium=email&utm_source=email&utm_campaign=ml&src=email&cid=7012H000001l1jG&utm_content=CDSW_Showcase_1.6_organic_AMER_Webinar_2019-06-27)!  (Just click through to register.)
* Hilary Mason is speaking at the [VentureBeat Transform 2019](https://www.vbtransform.com/) conference on July 11th in San Francisco, CA.



And stay tuned for a special announcement in early July! 

All the best,

The Cloudera Fast Forward Labs Team