---
layout: newsletter
slug: 2018-12-28-public
---

Hello!  In this month's newsletter, we discuss our newest research report on Federated Learning, deep learning for media content, and fine-tuning for NLP - and be sure to check out the link in our updates section for our annual round-up of interesting papers, talks, art, and more!

---

## The business case for federated learning
_by [Mike](https://twitter.com/mikepqr)_

Last month, we released Federated Learning, the latest report and prototype from
Cloudera Fast Forward Labs.

Federated learning makes it possible to build machine learning systems without
direct access to training data. The data remains in its original location,
which helps to ensure privacy and reduces communication costs.

![The report and prototype](http://fastforwardlabs.github.io/visuals/shared/ff09/ff09-combo.png)

### Federated learning in a nutshell

To train a machine learning model you usually need to move all the data to a
single machine or, failing that, to a cluster of machines in a data center.

This can be difficult for two reasons.

First, the creator of the data may simply not want to share it with you. Maybe
the data is baby photos, or competitively sensitive manufacturing data, or
legally protected medical data. We’ll give more examples below.

Second, there are often practical communication challenges. A huge amount of
valuable training data is created on hardware at the edges of slow and
unreliable networks, such as smartphones, IoT devices, or equipment in
far-flung industrial facilities, such as mines and oil rigs. Communication with
such devices can be slow and expensive.

The solution is federated learning. This is when a server coordinates a network
of nodes, each of which has training data that it cannot or will not share
directly. The nodes each train a local model, and it is that model which they
share with the server. The server merges the models into a single federated
model, sends the merged model back out to the nodes, and another round of local
training takes place. (For much more technical detail, see our article on the
Fast Forward Labs blog.)

Crucially: The server never has direct access to the training data. By moving
models rather than training data, federated learning helps to ensure privacy
and minimizes communication costs.

Let’s now look at some examples of what you can do with federated learning.

### Smartphones

Machine learning has huge potential to improve the user experience on
smartphones. Apps could learn to spot good baby photos and proactively offer to
share them with friends and family. They could make it easier to write emails
that are more likely to receive quick replies. And they could make composing
text messages even quicker and easier by accurately suggesting the next phrase,
whatever the language.

But aside from the practical challenge of getting this data off a device with a
slow connection, the personal aspect of some of this data (what people type,
where they travel, what websites they visit) makes it problematic. Users are
reluctant to share this sensitive data, and possessing it exposes technology
companies to security risks and regulatory burdens. These characteristics make
it a great fit for federated learning. The use case is so compelling that it
comes as no surprise that Google researchers are usually credited with its
invention, and Samsung engineers have also contributed significant ideas.

### Healthcare

The healthcare industry offers huge financial incentives to develop effective
treatments and predict outcomes. But the training data required to apply
machine learning to these problems is of course extremely sensitive. The
consequences of actual and potential privacy violations can be serious.

![]({{ site.github.url }}/images/editor_uploads/2018-11-22-193133-ff09_05.png)

By keeping the training data in the hands of patients or providers, federated
learning has the potential to make it possible to collaboratively build models
that save lives and generate huge value. Paris-based Owkin is among the most
ambitious users of federated learning that we spoke to during our research.
They provide a platform that allows healthcare providers to collaborate on a
wide range of healthcare problems.

### Predictive maintenance

Suppose a manufacturer wants to develop a predictive maintenance model for a
piece of equipment they sell. This model needs training data—but testing lots
of turbines until they fail in order to acquire that data would be expensive
for the manufacturer. It would be less costly for the manufacturer if its
customers were to send it such data. More importantly, the failures actual
customers experience will be more representative of real-world use than those
the manufacturer would see in factory experiments. In short, training data
acquired from customers would be cheaper and better.

But there are several problems. Some of their customers are reluctant to share
details about equipment failures with vendors or competitors. Some operate in
countries such as China, where industrial facilities can be legally prevented
from exporting data. And, as a practical matter, the volume of data can be
enormous, which makes streaming it back to the manufacturer an engineering
challenge.

This too is a great fit for federated learning! If the manufacturer takes this
approach, they can train a better model with less expense. And customers get
access to a model that is better than one they could train on their own,
without compromising the security of their data.

![Turbofan Tycoon](http://fastforwardlabs.github.io/visuals/shared/ff09/ff09-turbofan-tycoon.jpg)

This situation is the focus of our interactive prototype, [Turbofan Tycoon](https://turbofan.fastforwardlabs.com/). In
that, you play a user of industrial equipment who can adopt various maintenance
strategies. Spoiler alert: the optimal strategy is federated learning, and the
ROI relative to the alternatives huge!

### Conclusion

In moving the majority of the work to the edge, federated learning is part of
the trend to move machine learning out of the data center, for reasons that
include speed and cost. But in federated learning, the edge nodes create and
improve the model (rather than merely applying it). In this sense, federated
learning goes far beyond what people usually mean when they talk about edge AI.

Federated learning makes it easier, safer and cheaper to apply machine learning
in the world’s most regulated, competitive, and profitable industries.

This newsletter article only scratches the surface. Our report goes into much more
detail, and covers use cases not mentioned here (including video analytics and
corporate IT). And of course we get into the technical details, including
systems and networking issues, libraries and frameworks, and practical
recommendations based on our experience building [Turbofan Tycoon](https://turbofan.fastforwardlabs.com/).

---

## Deep Learning for Media Content

_by [Ryan](https://twitter.com/jqpubliq)_

Machine learning continues to [make its way](STYLE_TRANSFER_LINK) [into the arts](GENERATIVE_NETWORK_LINK), most recently in film and TV.

In a [recent blog post](https://cloud.google.com/blog/products/ai-machine-learning/how-20th-century-fox-uses-ml-to-predict-a-movie-audience), data scientists at 20th Century Fox and technical staff at Google Cloud described the approach they are using to predict audiences for their movies. (The tone of the post is fairly self-promoting, befitting the subject matter and industries involved.)

Their product, [Merlin Video](https://datastudio.google.com/u/0/reporting/1Ss64x1ocKeeDdTcMVX3l4iQcr8gp9w-W/page/Hg2V), is a deep learning tool that analyzes movie trailer videos. It was based on a pre-trained model - Google's YouTube 8M video data set, which identifes objects in video, and tuned with movie trailers and marketing data from past movies. Fox released [a paper describing the technology in detail](https://arxiv.org/abs/1810.08189). They have been using Merlin - with success - for a year and a half.

![]({{ site.github.url }}/images/2018/11/Architecture_flow_diagram_for_Merlin_max_1000x1000-1541453447025.png)
##### Merlin's architecture [(image source)](https://cloud.google.com/blog/products/ai-machine-learning/how-20th-century-fox-uses-ml-to-predict-a-movie-audience)

Fox's software works to predict an *audience*, with clear implications for "testing" likely outcomes for certain types of films, which studios can use to market existing movies more effectively, and guide writers and directors toward making more commercially appealing stories. But can machines generate the stories themselves? Not yet - but this is not far off.

For a challenge, filmmaker [Oscar Sharp](http://www.thereforefilms.com/oscar-sharp.html) and creative technologist [Ross Goodwin](https://rossgoodwin.com/) used a neural network to create a screenplay. They created an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) recurrent neural network called Benjamin. They trained Benjamin with science fiction screenplays and prompted it with data from a science fiction filmmaking contest. Benjamin produced a screenplay, and Sharp and Goodwin, with a cast and crew, made [a film, Sunspring](https://www.youtube.com/watch?v=LY7x2Ihqjmc) from the screenplay with impressive and interesting results.

Clearly, Benjamin is not ready for commercial film, but the resulting film is surprisingly coherent (acknowledging that human minds will go a long way to find order in chaos).

Still, it's clear there are useful applications of machine learning in entertainment, and we expect these products to improve, with interesting results.

## Fine-tuning for Natural Language Processing

_by [Shioulin](https://twitter.com/shioulin_sam)_

2018 was a fun and exciting year for natural language processing. A series of papers put forth powerful new ideas that improve the way machines understand and work with language. They challenge the [standard way of using pretrained word embeddings like word2vec](http://ruder.io/nlp-imagenet/) to initialize the first layer of a neural net, while the rest is trained on data of a particular task. Instead, these papers propose better embeddings (feature-based approach) and pre-trained models that can be fine-tuned for a supervised downstream task (fine-tuning approach).

Under feature-based approaches, where fixed features, in the form of vectors, are extracted from the pre-trained model, [ELMo](https://arxiv.org/abs/1802.05365) provides contextualized embeddings for a word. For example, the word _bank_ in "I want to deposit money into a _bank_" and "I want to run by the river _bank_" means different things. ELMo allows the word "bank" to have multiple embeddings depending on the context in which it is used. Under fine-tuning approaches, [BERT](https://arxiv.org/abs/1810.04805), [ULM-FiT](https://arxiv.org/abs/1801.06146) and [OpenAI GPT (pdf)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) propose various model architectures that are pre-trained on a language model objective (i.e., predict the next word). Among these models, BERT stands out because it provides representations that are jointly conditioned on both left and right context in all layers. In other words, it is deeply bidirectional, as opposed to ELMo (shallow bidirectional) and OpenAI GPT (one direction, left to right).

![BERT is bidirectional in all layers and uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo concatenates two independently trained left-to-right and right-to-left LSTMs. [Image credit](https://arxiv.org/abs/1810.04805)]({{ site.github.url }}/images/2018/12/Screen_Shot_2018_12_07_at_12_03_44_PM-1544202300577.png)

BERT's architecture is based on a bidirectional [Transformer encoder](https://arxiv.org/abs/1706.03762). (We will not go into details of the Tranformer, but the paper is worth a read!) BERT's input representation is able to represent a single text sentence or a pair of text sentences (the reason will become apparent later on). Each token, or loosely, each word is represented by the summation of its word embedding, a learned positional embedding, and a learned segment embedding. The word embedding used in the paper is WordPiece embeddings. The positional embedding captures the order of the word in the sequence (or sentence). The learned segment embedding associates certain tokens with a particular sentence since the input can be a pair of text sentences. 

BERT is not trained using a traditional left-to-right or right-to-left language model. In these approaches, the model is asked to predict the next word, given what it has seen so far either from the left or right. ELMo, for example, trains two models, one left-to-right, the other right-to-left, and concatenates them together. This results in a shallow bidirectional model. It is impossible to train a deep bidirectional model like a normal language model because that would create cycles where words can indirectly "see themselves." The prediction then becomes trivial. To overcome this, BERT trains using two clever unsupervised prediction tasks. First, it masks a percentage of words from the input and asks the model to predict these masked words from the context. An example would be to ask the model to predict _[MASK1] = hairy_ from the input _"my dog is [MASK1]_". The second task teaches BERT to understand the relationship between two text sentences by pre-training a binarized next sentence prediction task (see image). This ability is not captured by language modeling, but is important for many important downstream tasks (e.g. Question Answering).

![Next sentence prediction task [Image credit](https://arxiv.org/abs/1810.04805)]({{ site.github.url }}/images/2018/12/Screen_Shot_2018_12_07_at_12_03_58_PM-1544202515836.png)

Once BERT is pretrained, task-specific models are formed by adding one additional output layer, so a minimal number of parameters need to be learned from scratch. As an example, a classifier composed of a simple feed forward neural network and a softmax layer can be added to BERT for spam detectionn. This is akin to transfer learning for image recognition. Many of us at CFFL are excited about this capability, and some of our thoughts are captured in this [previous newsletter article](https://blog.fastforwardlabs.com/2018/08/29/breakthroughs-in-transfer-learning-for-nlp.html). BERT can also be used as a feature extractor. By concatenating various hidden layers of the pretrained Transformer, the authors show that the best performing combination is only 0.3 F1 behind fine-tuning the entire model for named entity recognition task. BERT is open sourced, and we cannot wait to see it being used to solve problems outside of the research space!

## CFFL Updates

* Be sure to check out our annual round-up of our favorite research papers, talks, art, sci-fi, and more in [our latest blogpost](https://blog.fastforwardlabs.com/2018/12/18/highlights-2018.html)!

* Save the date!  Join us for an upcoming webinar with Tristan Zajonc on [Cloudera’s New Cloud-Native Platform for Enterprise Scale Machine Learning](https://www.cloudera.com/more/events/webinars/cloud_native_ml_and_data_engineering.html?src=ffl) at 10:00am PT/1:00pm ET on January 15th. You can register [here](https://www.cloudera.com/more/events/webinars/cloud_native_ml_and_data_engineering.html?src=ffl).

As always, thank you for reading.  Happy holidays!

All the best,

The Cloudera Fast Forward Labs Team
