---
slug: 2019-10-07-client
layout: newsletter
---

Hi, team - here's the format for this week's newsletter; please add your links below the dashed line.  Thanks!  :)


[Title](link)

*Two sentences explaining what the article is about and why you recommend it. . -  [yourname](linktoTwitterorLinkedInprofile)*


[Extreme Language Model Compression with Optimal Subwords and Shared Projections](https://arxiv.org/abs/1909.11687)

Pretrained language models such as BERT, ELMo, and GPT have achieved state of the art performance for a variety of language understanding tasks. However, the size of these models make them impractical for a number of scenarios, especially on mobile and edge devices. This paper introduces a new model distillation technique (more on model distillation here https://arxiv.org/abs/1503.02531 ) that enables a compressed version of BERT (from 420MB to 6.8MB, over 60x compression!!) with minimal loss of accuracy. These results will directly enable the use of such high accuracy models on end user and edge devices.

[vykthur](http://twitter.com/vykthur)

----
