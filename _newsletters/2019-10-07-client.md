---
slug: 2019-10-07-client
layout: newsletter
---

Hi, team - here's the format for this week's newsletter; please add your links below the dashed line.  Thanks!  :)


[Title](link)

*Two sentences explaining what the article is about and why you recommend it. . -  [yourname](linktoTwitterorLinkedInprofile)*


[Extreme Language Model Compression with Optimal Subwords and Shared Projections](https://arxiv.org/abs/1909.11687)

Pretrained language models such as BERT, ELMo, and GPT have achieved state of the art performance for a variety of language understanding tasks. However, the size of these models make them impractical for a number of scenarios, especially on mobile and edge devices. This paper introduces a new model distillation technique (more on model distillation here https://arxiv.org/abs/1503.02531 ) that enables a compressed version of BERT (from 420MB to 6.8MB, over 60x compression!!) with minimal loss of accuracy. These results will directly enable the use of such high accuracy models on end user and edge devices.

[vykthur](http://twitter.com/vykthur)

----

[Data Shapley: Equitable Valuation of Data for Machine Learning, pdf](http://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf)

As we think about shifting data ownership back to the generators of data and thus opening up the possiblity of compensating owners for their data, the ability to value a datapoint becomes necessary and crucial. This paper discusses one way to do so. 

[shioulin_sam](http://twitter.com/shioulin_sam)

