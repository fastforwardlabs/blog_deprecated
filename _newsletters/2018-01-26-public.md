---
layout: newsletter
slug: 2018-01-26-public
---

Hello, and welcome to the first 2018 Cloudera Fast Forward Labs public newsletter!

We are excited to announce the release of our newest research report - Semantic Recommendations - available this month via subscription to [Cloudera Fast Forward Labs Research](https://www.cloudera.com/campaign/building-a-better-recommendation-system-research-report.html).

Most recommendation systems do not understand what they are recommending. They work by i) finding users similar to you, ii) assuming you share the same item preferences, and iii) recommending to you the items that these users have liked.    

In our [latest blog post](http://blog.fastforwardlabs.com/2018/01/22/exploring-recommendation-systems.html) we share some challenges we encountered in building basic recommendation systems using two datasets (one from Flickr and one from Amazon). Both datasets contain historical rating information that users assigned to items (photos for Flicker and books for Amazon). Because each user only rates a small percentage of items available, historical interaction data is generally sparse. Our experiments show that basic recommendation systems which rely on this sparse data are unable to extract enough information to give good results. In addition, these systems are unable to recommend new items (this is known as the cold start problem) because they do not know who has interacted with them or the outcome of those interactions (as that information is not in the historical interaction data).           

![]({{ site.github.url }}/images/2018/01/recommendations-1516742153349.png)
                                                                                                     
Therefore, to make a better recommender, we built a system that understands what it is recommending by injecting content. This change not only resulted in a huge jump in performance, it also alleviated the cold start problem.

For more on building recommendation systems that can better understand content, be sure check out our new Semantic Recommendations report!

----

## Serverless data science

Cloud computing has transformed enterprise IT. It makes it possible to rapidly
scale up and down a complex and global infrastructure, without the overhead of
a datacenter. But living in the cloud can be expensive, and you still need to
maintain computers and their operating systems, even if they are virtual.
That's why we've been watching with interest the rise of "serverless"
computing.

Serverless has the potential to open up "big data" work to mere mortal data
scientists who don't have the budget or engineering support for a
long-lived analytics cluster, and to make life simpler and reduce costs for
those that do.

Traditional cloud "elastic compute" systems (like Amazon's EC2, Google's Computer
Engine, or Azure's Virtual Machines) allow you to run applications without
maintaining hardware. The goal of "serverless" is to go even further, and make
it possible to run applications without worrying about hardware _or_ the
operating system.

Serverless environments (like Amazon's Lambda, Google's Cloud Functions, or Azure's
Functions) can be thought of as computing environments that pop into existence
for the duration of a single function call, and are then destroyed. Configuring
and maintaining the underlying operating system is somebody else's problem.

Because the serverless instance exists only for the duration of the function,
there's no idle time and your bill scales almost perfectly with the amount of
compute you use. Combined with the fact that you no longer need to configure and
maintain the operating system, this can result in big savings. For example,
our friends at Postlight converted their Readability application to run on AWS
Lambda and [reduced the monthly cost from over $10,000 to
$370](https://trackchanges.postlight.com/serving-39-million-requests-for-370-month-or-how-we-reduced-our-hosting-costs-by-two-orders-of-edc30a9a88cd).

But it's not all good news. Because the environment ends after the function
finishes, input and output must occur via a web API or a database connection.
There is no internal state or disk. And the various cloud providers place CPU,
RAM, time, and programming language constraints on what you can do. (For example,
AWS Lambda functions must run Python, C#, node.js or Java; R is not an option.
And the function must return in less than 300 seconds and use no more than
1.5Gb of RAM.)

These limitations might seem to make serverless less appealing to power-hungry
data scientists and data engineers - and indeed, so far most of the prominent
uses of serverless have been in web applications, where the computational
requirements are less intense. But in many ways the constraints of serverless
are similar to those faced in distributed analytics clusters running Hadoop or
Spark. To do data analytics in these environments, we have the map-reduce
computing paradigm, which parallelizes work by splitting it into small parcels.

![]({{ site.github.url }}/images/2018/01/serverless_pywren-1515449109658.png)
##### PyWren's computational power can grow to many TFLOPS as the number of workers (inexpensive AWS Lambda instances in this case) increases. Image credit: [Occupy the Cloud: Distributed Computing for the 99%](https://arxiv.org/abs/1702.04024) 

[PyWren](http://pywren.io/) is a distributed analytics tool that connects the
dots. It splits up large analytics jobs into smaller parcels of work, and ships
them off to hundreds or even thousands of serverless AWS Lambda instances. This
makes PyWren (with AWS Lambda as a computational "backend") a light-weight
alternative to complex, expensive (and admittedly more robust) map-reduce
frameworks such as Hadoop and Spark. 

And - perhaps most intriguing for us at Cloudera Fast Forward Labs, given our
interest in machine learning - it's exciting to see serverless used to [speed up
hyperparamter
optimization](http://tothestars.io/blog/2016/10/19/serverless-ai), a relatively
simple (but computationally intensive) part of model building.

For more on PyWren, see [Occupy the Cloud: Distributed Computing for
the 99%](https://arxiv.org/abs/1702.04024). For more on the implications of
serverless more generally, see [Serverless Computing: Economic and
Architectural
Impact](http://www.doc.ic.ac.uk/~rbc/papers/fse-serverless-17.pdf).

---

## Google Maps's Competitive Moat - Why Better Data Matters

Cartographer Justin O'Beirne recently wrote a [blog post](https://www.justinobeirne.com/google-maps-moat/) surveying the landscape of mapping apps and sites (with particular focus on Google Maps and Apple Maps). This is one of a [series of pieces](https://www.justinobeirne.com/cartography-comparison) O'Beirne has written [comparing Google and Apple Maps](https://www.justinobeirne.com/a-year-of-google-maps-and-apple-maps). In short, O'Beirne concludes that Google has a substantial competitive advantage in mapping that boils down to having better data.

The article is centered around the example of Google's _Areas of Interest_ feature. Simply put, Google adds subtle shading around areas that are commercial districts, i.e., clusters of restaurants and shops, while Apple does not. This feature is quite useful as it reflects [the way humans think of cities](http://groups.ischool.berkeley.edu/mentalmaps/) in terms of their commercial districts.

![]({{ site.github.url }}/images/2018/01/Smith_Court_Area_of_Interest-1516748269178.JPG)
##### Court and Smith Streets are commercial districts in the Cobble Hill neighborhood of Brooklyn

Google built this feature from two distinct datasets: satellite imagery and street view imagery. Apple has - or had - access to similar datasets, but Google outpaces Apple in both categories to great effect.

Google uses machine learning and computer vision to digest huge volumes of satellite imagery into 3D outlines of buildings with great detail, and they do it automatically, resulting in tremendous coverage within the United States with very high accuracy. Meanwhile, Apple appears to buy its building data from outside vendors.

Google also uses machine learning to process its street view imagery to understand which businesses are where. This provides very precise positioning of businesses that allow Google to define the boundaries of Areas of Interest with great accuracy. 

Applying machine learning to these two datasets gives Google a superior product to begin with: more coverage of (and more accurate) building models, and more accurate building locations. But combining these datasets yields a further advantage. Google knows exactly where buildings are and what they contain, so they can automatically define Areas of Interest. Apple simply lacks the data to do so.

In sum, Google's applications of machine learning and access to superior data make possible a useful feature that Apple just cannot compete with in a meaningful way.

---

## Job Postings

Here are a few job postings we've heard about recently: 

* **The Rockefeller Foundation** - Managing Director, Data Technology ([job description](https://www.rockefellerfoundation.org/about-us/careers/managing-director-data-technology/))
* **TD Ameritrade** - Senior Specialist, UI/UX Lead ([job description](https://jobs.tdameritrade.com/job/st-louis/senior-specialist-ui-ux-lead/1121/6333455))
* **Center for Court Innovation** - Project Manager, Technology ([job description](https://www.courtinnovation.org/careers/project-manager-technology))

We're always happy to share opportunities; please [let us know](mailto:cffl@cloudera.com) if you have any job descriptions you'd like for us to pass along in next month's newsletter! (And please do include a link with your request.)

---

## Recommended Reading

Here are a few articles we've found interesting:

* [30 Amazing Machine Learning Projects for the Past Year (v.2018)](https://medium.mybridge.co/30-amazing-machine-learning-projects-for-the-past-year-v-2018-b853b8621ac7)
* [10 Things Everyone Should Know About Machine Learning](https://hackernoon.com/10-things-everyone-should-know-about-machine-learning-d2c79ec43201)
* [A brief tour of Python 3.7 data classes](https://hackernoon.com/a-brief-tour-of-python-3-7-data-classes-22ee5e046517)
* [Should Deep Learning use Complex Numbers?](https://medium.com/intuitionmachine/should-deep-learning-use-complex-numbers-edbd3aac3fb8)

---

## CFFL Updates

* The next Strata Data Conference is coming up in just a few short weeks in San Jose; Mike will be presenting on [Interpretable Machine Learning Products](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/63572) on March 7th.  Hilary will also be speaking at Strata that day.
* Mike will be speaking on interpretability at [Qcon.ai](https://qcon.ai/) on April 11th in San Francisco.
* Friederike was recently featured on [DataKind's blog](http://www.datakind.org/featured-volunteers/friederike-schuur) for her data science volunteer work.


As always, thank you for reading!  We welcome your thoughts, reactions, feedback, and suggestions anytime; just drop us a note at cffl@cloudera.com.

Until next week,

The Cloudera Fast Forward Labs Team
