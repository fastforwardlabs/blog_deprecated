---
slug: 2019-03-21-client
layout: newsletter
---

## Three exciting developments at TensorFlow Dev Summit 2019
*by [Chris](https://twitter.com/_cjwallace)*

Right around the time Seth was composing his [assessment](https://blog.fastforwardlabs.com/newsletters/2019-03-13-client.html) of the state of deep learning tools available today, the third annual TensorFlow Dev Summit was held. The entire first day of talks was live-streamed, and videos are available on the [TensorFlow YouTube channel](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ). I stayed up late (from the UK) watching a few of the talks, and here’s an entirely idiosyncratic description of three things I find exciting. A broader [recap](https://medium.com/tensorflow/recap-of-the-2019-tensorflow-dev-summit-1b5ede42da8d) is provided by the TensorFlow team themselves.

### TensorFlow Federated

[TensorFlow Federated](https://www.tensorflow.org/federated/) is a library for federated learning. Given the title of our [last report](http://vision.cloudera.com/an-introduction-to-federated-learning/), you already know we think this is an exciting area. It was a group of Google researchers who coined the term “federated learning” with the release of the [Federated Averaging](https://arxiv.org/abs/1602.05629) algorithm for learning from decentralized data, so it makes sense that Federated Learning is getting the TensorFlow treatment.

The library includes both the high-level “Federated Learning” API designed for wrapping existing TensorFlow models, and a low-level “Federated Core” API, which is intended to the development of new federated learning algorithms.  The project is nascent, but particular attention has been paid to separating the various concerns of building a federated learning system. For instance, without some thought, it would easy to start entangling model operations and network communication. By providing high level abstractions, TensorFlow Federated allows experts in each area to contribute their respective knowledge, which could substantially lower the bar to entry for participating in federated learning research. The ability to wrap an existing Keras model is particularly appealing.

TensorFlow Federated currently includes only a simulated runtime (where all federated nodes actually live on the same, local, device), so it does not out-of-the-box “solve” federated learning. Eventually, other runtimes will be supported with the same API, meaning that the gap between researching novel federated approaches and deploying them to real federated networks is minimised.

### TensorFlow Probability

Likewise to federated learning, [probabilistic programming](https://blog.fastforwardlabs.com/2017/01/18/new-research-on-probabilistic-programming.html) is also something we’ve written about at Fast Forward Labs. Since our probabilistic programming report was released, the ecosystem has evolved. PyMC and Stan both continue to develop, but perhaps the most notable change has been the emergence of so-called “deep probabilistic programming” libraries. These libraries combine the ability of deep neural networks to learn complex functions with the probabilistic paradigm of computing with distributions. [Pyro.ai](http://pyro.ai/) runs atop PyTorch, and until recently, [Edward](http://edwardlib.org/) was the go-to library for TensorFlow.

[TF Probability](https://www.tensorflow.org/probability) takes a broader approach to probabilistic programming than Edward, introducing layers of abstraction into the “probprog” stack. At its base, it has complete interoperability with TensorFlow, which means all of TF’s linear algebra operations are available. On top of this is a set of “statistical building blocks” (their language): probability distributions and bijectors (composable transformations of random variables). There are also model building tools, including probabilistic programming language Edward2 - unsurprisingly, the successor to Edward - and a “probabilistic layers” component that can be used with high level APIs like Keras. Finally, there are a suite of probabilistic inference methods including Monte Carlo and Variational Inference.

The library has been evolving rapidly over the last year, but seems to be converging to a useful suite of tools. The TF Dev Summit [demo](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb) of TensorFlow Probability included a simple and compelling walk through of building a regression model then including aleatoric (observation noise) and epistemic (model) uncertainty. It is encouraging to see probabilistic programming continue to mature and gain traction with existing software ecosystems.

### Swift for TensorFlow

Anyone at CFFL will tell you this particular newsletter author has a bit of a soft spot for probabilistic programming. Despite developments in TF Probability, of all the developments announced at the TF Dev Summit, I find the progress in [Swift for TensorFlow](https://www.tensorflow.org/swift) (which has moved into beta) the most promising.

Swift is an open source language from Apple, first designed to provide a better development experience for iOS and OSX (but now also supporting Linux, less a few niceties like automatic test discovery). Chris Lattner, who implemented much of the Swift language while at Apple, is now working on bringing TensorFlow to Swift. However, this is not a simple wrapper around the existing TensorFlow codebase, but rather a re-imagining of what tools for machine learning can be. Swift for TensorFlow integrates automatic differentiation into the Swift language itself, allowing easy definition of performant custom operations. Swift is fast, expressive and has an advanced type system. At CFFL, we love Python, and the Swift for TensorFlow team have also provided an almost native experience for interoperability with Python.

The watch words are “differentiable computing,” and we eagerly await more demo applications in this ecosystem. One advantage to Swift, being designed as a replacement for C family languages is that it is not necessary to wrap C libraries for performant numerical code. This means one can move up and down the numerical computing stack in the same language, which is also great for pedagogy. Speaking of pedagogy, Jeremy Howard of Fast.ai has [noticed](https://www.fast.ai/2019/03/06/fastai-swift/) Swift too, and at the TF Dev Summit, it was announced that the next iteration of the Fast.ai course will include a Swift component from Chris Lattner himself.

---

## Upcoming Events

* Several of us are speaking at the [Dataworks Summit](https://dataworkssummit.com/barcelona-2019/) tomorrow:
 -  Hilary Mason will be speaking on [Building an Enterprise AI Factory](https://dataworkssummit.com/barcelona-2019/keynote/8410/)
 - Chris Wallace will be speaking on [Federated Learning: Artificial Intelligence and Data Science](https://dataworkssummit.com/barcelona-2019/session/federated-learning-artificial-intelligence-and-data-science/)
 - Alice Albrecht will be speaking on [A Framework for Developing a Winning Data Project Portfolio](https://dataworkssummit.com/barcelona-2019/session/a-framework-for-developing-a-winning-data-project-portfolio/)
 - Justin Norman will be speaking on [Machine Learning Model Deployment: Strategy to Implementation](https://dataworkssummit.com/barcelona-2019/session/machine-learning-model-deployment-strategy-to-implementation-2/)
 - Tristan Zajonc will be speaking on [Cloud-Native Machine Learning: Emerging Trends and the Road Ahead](https://dataworkssummit.com/barcelona-2019/session/cloud-native-machine-learning-emerging-trends-and-road-ahead/) and [Edge to AI: Analytics from Edge to Cloud with Efficient Movement of Machine Data](https://dataworkssummit.com/barcelona-2019/session/edge-to-ai-analytics-from-edge-to-cloud-with-efficient-movement-of-machine-data/)
 - Vidya Raman will be speaking on [Starting with the End in Mind: Learnings from Data Strategies across Companies and Verticals](https://dataworkssummit.com/barcelona-2019/session/starting-with-the-end-in-mind-learnings-from-data-strategies-across-companies-and-verticals/)
* Victor Dibia will be giving a talk called "Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Network" at the [NVIDIA GTC 2019](https://www.nvidia.com/en-us/gtc/) in San Jose, CA tomorrow, as well. 
* Mike Lee Williams will be speaking on [Federated Learning](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/72661) at the Strata Data Conference in San Francisco on March 27th.
* Hilary Mason will be speaking at the [Marketing Analytics and Data Science Conference](https://marketing.knect365.com/marketing-analytics-data-science/) in San Francisco, CA on April 10th.
* Mike Lee Williams will be chairing the "deep learning in practice" track at [Qcon.ai](https://qcon.ai/) in San Francisco on April 16-17th.
* Chris Wallace will be speaking on [Federated learning: machine learning with privacy on the edge](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74327) on May 1st at the [Strata Data Conference](https://conferences.oreilly.com/strata/strata-eu) in London.
* Shioulin Sam will also be speaking at Strata in London about [Learning with Limited Labeled Data](https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/74341) on Thursday, May 2nd.

If you're attending any of these conferences, please [let us know](mailto:cffl@cloudera.com) and do stop by to say hello! 

All the best,

The Cloudera Fast Forward Labs Team