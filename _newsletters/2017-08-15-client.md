---
layout: newsletter
slug: 2017-08-15-client
---

## Introduction
As we announced last week, we're very excited to release the latest Fast Forward Labs research: [_Interpretability_](http://blog.fastforwardlabs.com/2017/08/02/interpretability.html). As a client, you have access to the report and prototype through the [client portal](http://clients.fastforwardlabs.com/).

We will reach out to you to schedule a presentation on our research, technical or conceptual, depending on your audience, to discuss use cases and answer any questions you may have. Can't wait? On September 6, we will host a [public webinar on interpretability](https://mlinterpretability.splashthat.com/) with special guests Patrick Hall (Senior Data Scientist and Product Engineer at H2o.ai, author of O'Reilly's [Ideas on Interpreting Machine Learning](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)) and Sameer Singh (Assistant Professor of Computer Science at the University of California, co-creator of [LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/), a model-agnostic tool for extracting explanations from black box machine learning models).

![](/images/2017/08/4b_ps_1-1502744235666.jpg)

##### Caption for the image.

Last week at Fast Forward Labs, we had fun reading about AI's foray into [Starcraft](https://www.theverge.com/2017/8/9/16117850/deepmind-blizzard-starcraft-ai-toolset-api), played Monday-morning quarterback to Disney's reported decision to pull their catalog from [Netflix](http://money.cnn.com/2017/08/08/media/disney-netflix/), and got deep into the history of the humble [cookie](https://labs.rs/en/invisible-infrastructures-online-trackers/).

In our newsletter this week, we discuss a few of our perennial favorite topics: algorithms that push machine learning into new realms, how to make sense of some of the challenges that AI poses out in the real-world, and research that challenges how we think about intelligence in general. Enjoy!

---

## Neural Nets for low power, low memory devices

Better hardware, from more powerful CPUs to GPUs, TPUs, and VPUs comes with better capabilities: it is inconcievable that even the simplest modern neural network could be trained on the G3 SDRAM 512MB processor of a 1999 iMac. But, even the most powerful hardware limits the engineer's dream, algorithmic innovation can help us make *optimal* use of the hardware we have today.

### The necessary trade-offs for running on low-memory, low-power devices

Cellphones running the latest apps and IoT sensors need to be able to perform complex calculations localy as intermittent [wifi](https://thenextweb.com/insider/2016/11/30/wifi-coming-iot-invasion/#.tnw_NuiMni6y) and power drain, the result of frequent data transmission, are real concerns. One straightforward approach, taken by, e.g., Apple, is to train algorithms on powerful computers, since training is computationally expensive, and then [deploy pre-trained models on low-power, low-memory devices](https://www.tractica.com/automation-robotics/neural-networks-are-coming-soon-to-your-mobile-phone-camera/) only for inference (prediction). These pre-trained networks, however, may still be memory and power hungry.

For [consumer applications](https://research.googleblog.com/2015/07/how-google-translate-squeezes-deep.html), the process of curating (or crafting) training data, to reflect the most important "dimensions of variation" the model is likely to encounter once deployed, limits the dimensions the neural network has to learn. This approach reduces model complexity and, in combination with the fine-tuning of "expensive" operations, such as matrix multiplication, has proven effective. Other solutions involve using fixed-point instead of floating-point calculations, or implementing different, lightweight algorithms, such as [Bonsai](http://proceedings.mlr.press/v70/kumar17a/kumar17a.pdf) (Random Jungle) and [ProtoNN](http://proceedings.mlr.press/v70/gupta17a/gupta17a.pdf) (kNN clustering). 

### Training on low-power, low-memory devices
Small feed-forward networks can be used to [train an NLP model on an Android device](https://arxiv.org/pdf/1708.00214.pdf). The networks are designed to limit the runtime of the model, whose memory needs are dominated by the size of the vocabulary used and vector dimensions. Their approach relies on (1) taking advantage of efficient n-gram representations instead of word embeddings, that allows much smaller vocabulary sizes in the 100 - 5000 range, (2), apply [quantization](https://www.tensorflow.org/performance/quantization) and (3) use a mini-batch size that is fixed to 32. They evaluate their algorithm on various common NLP tasks against other light-weight algorithms and they report significant improvements on accuracy, memory usage and speed. We kept the best for last: their algorithm is processing up to 46 tokens/second compared with the reported LSTM speed of 8.8 words/second!

Google and MIT [joined forces](http://news.mit.edu/2017/automatic-image-retouching-phone-0802) to create a "smart" photo retouching app that runs locally on your smartphone. The way they make their neural networks more efficient is by having them calculating the **instructions** for creating the retouched image, rather than the image itself. Yes, we think too this is very clever.

### Fast, efficient neural networks through changes in network architecture
A process called [knowledge distillation](https://arxiv.org/abs/1503.02531) can help transform trained, high-accuracy, deep and wide neural networks (or ensembles) with many parameters into into small, thin, fast to execute models (e.g., [FitNets](https://arxiv.org/pdf/1412.6550.pdf)), a two-step process to get to neural networks suitable for mobile devices.

The laboratory of Vivienne Sze at MIT took a heads-on [approach](http://news.mit.edu/2017/bringing-neural-networks-cellphones-0718) to energy efficient neural nets for cell phones using a principled energy-aware pruning strategy for convolunional neural networks that eliminates costly nodes that do not significantly improve algorithm performance. She was able to reduce the energy consumption of running the model up to 70%; impressive efficiency gains through changes in neural network architecture. 

### To the future
It is easy to get carried away by the black-box beauty of deep, convoluted, twisted and interconnected networks that like magic produce the right results, given enough computational power, of course.  But there is another paradigm of beauty, that of elegance and efficiency. Algorithmic innovation to push the limits of the engineer's dream given existing hardware follows that paradigm.

## Biz piece [placeholder] 

## Jobs, Jobs, Jobs

Here's a job opportunity for a data scientist:

**Enigma** - Data Scientist ([job description](https://www.enigma.com/careers/data-scientist))

We're always happy to share opportunities we hear about; please let us know if you have any you'd like for us to pass along.

# FFL Reads

- [Better Business Through Sci-Fi](http://www.newyorker.com/tech/elements/better-business-through-sci-fi)
- [The Evolution of Trust](http://ncase.me/trust/)
- [An Overview of Multi-Task Learning in Deep Neural Networks](http://ruder.io/multi-task/)
- [Utopian Hacks](https://limn.it/utopian-hacks/)
- [The Business of Artificial Intelligence](https://hbr.org/2017/07/the-business-of-artificial-intelligence)

For more on what we're reading, be sure to check out [our blog](http://blog.fastforwardlabs.com/links.html)!)

# FFL Updates

We're giving talks at some upcoming conferences:
- [FATML](http://www.fatml.org/): August 14 **(Halifax, Micha)**
- [Synaptech](http://synaptech.ai/): September 21-22 **(Berlin, Friederike)** (Use the code 'FriederikeVIP' for a 30% discount!)
- [Strata Data Conference](https://conferences.oreilly.com/strata/strata-ny): September 28 **(New York, Hilary)**

Thank you for reading.  We'd love to hear your thoughts! You can reach us anytime at xxxxxx@fastforwardlabs.com. 

- The Fast Forward Labs Team
