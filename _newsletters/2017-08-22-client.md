---
layout: newsletter
slug: 2017-08-15-client
---

Hi,

TODO: add introduction

---

## On finding the best, first use case: the tale of Google Glass

Google Glass was first launched on April 2013 and for a moment it looked like William Gibson's [visions of occular implants](http://www.williamgibsonbooks.com/blog/2006_08_01_archive.asp#115688201954919109) had once again proven [prophetic](http://www.newyorker.com/books/page-turner/william-gibsons-man-made-future). Presenting the world with a product that combined wearable technology with augmented reality before neither of these applications had a strong foot on the ground, Google Glass seemed poised to change how we experience and interact with the world &mdash; and make Google X (now simply [X](http://money.cnn.com/2016/01/18/technology/google-x-new-logo/index.html)) money along the way. 

Production of Google Glass however, stopped on January 2015. Since then, lots of reasons have been given as to why the product did not live up to the hype: from [privacy and regulatory considerations](https://www.fastcompany.com/3009432/tracking-the-ban-on-google-glass), to its [prohibitive price-tag](http://wearablecameras.com/reviews/google-glass-steep-price-worth-ultra-fancy-gadget/), and [restrictive societal norms](http://nypost.com/2014/07/14/is-google-glass-cool-or-just-plain-creepy/). The latest chapter on the saga however, points to another direction: strategy, or, perhaps, its absence (which [Forbes](https://www.forbes.com/sites/ianaltman/2015/04/28/why-google-glass-failed-and-why-apple-watch-could-too/#3e6483544c4b) stated early on).

For many, Apple taught the bussiness world the importance of [early adopters](https://en.wikipedia.org/wiki/Technology_adoption_life_cycle) during product development and product launch. The omnipresent diagram of consumers divided into early-, mid-, and late-adopters of new technology has become something of an obsession in the start-up world. But, just like the success of Apple should be attributed to focusing on, and providing for, [consumer's needs](http://appleinsider.com/articles/16/10/05/five-years-after-steve-jobs-an-apple-with-the-courage-to-say-no), Google Glass' (initial) demise seems to have been tied to the confusion as to what purpose the product serves. Google X relied on the ingeniousness of its early adopters, the ['Explorers'](https://www.forbes.com/sites/tarunwadhwa/2014/04/24/the-google-glass-explorer-program-was-a-social-experiment-that-backfired/#527797685d95), to figure it out. But they didn't. Maybe.

While Google Glass flopped as a lifestyle gadget, it was slowly and silently finding its home as a productivity tool within the enterprise: 'Google Glass Enterprise edition' was launched just last month (July 18th) now marketed as a productivity enabling technology. According to their new [website](http://www.x.company/glass/), this space is (suddenly) rife with applications, from hands-free instructions for workers on the assembly line, to remote consultation on technical problems and smart record keeping, to automatic scanning of barcodes and workflow improvement. Imagine a company's personal assistants scanning, faxing and e-mailing documents on the blink of an eye and as fast as they can say "sent" or firefighters, being shown a secure path to safety while carrying an unconcious citizen through a building full of smoke. In combination with video and image analysis, wearable AR could help increase success rates in operation theaters and improve medical diagnoses. Or, simply, it could identify and superimpose the items of my flatpacked IKEA desk onto a 3D image of the final, assembled product.

As more companies try to 'disrupt' industries with an innovative product or algorithm, the simple rule of business "solve a problem and do it well"  rings truer today than ever. Technology by itself it not a solution to a problem, it's mere potential. Identifying the best, first use case for novel technology can be a struggle especially for technology companies with little expertise in, say, manufacturing or fire fighting. (Amazon's strategy is based on providing the first use case for its products (e.g., AWS) in house, some say a strategy that motivated the purchase of Whole Foods.) In fact, the first incarnation of Google Glass may not have failed. Instead, it may have reflected the recognition by Google that it is hard to identify the best first use case, or the best partner for the first use case, for novel technology without extensive and expensive market research and user testing. Instead, Google outsourced market research with the Explorers' program; and it seems to have worked. 

As the range of applications for AR is being defined, we should expect these products to become increasingly present in our everyday lives, first at work and eventually at home. Apple is following suit with its [just published patent](http://appleinsider.com/articles/17/07/27/apple-invention-details-ar-mapping-system-for-iphone-head-mounted-displays) for its own brand of AR products.

---

## Interpretability via Influence Functions

Interpretability is important in data science and machine learning, as we recently [argued](http://blog.fastforwardlabs.com/2017/08/02/interpretability.html).  Model-agnostic interpretability solutions allow us to understand and explain any fixed, trained model. [LIME](https://arxiv.org/pdf/1602.04938.pdf), for example, which we cover in our most [recent research report](http://blog.fastforwardlabs.com/2017/08/02/interpretability.html), uses perturbation and local, sparse linear models to explain individual predictions of complex, high-accuracy models in terms of *variables* that matter most for a given, to-be-explained prediction. The winner of this year's ICML best paper award went to ["Understanding black-box predictions via influence functions"](http://proceedings.mlr.press/v70/koh17a/koh17a.pdf). The authors trace model predictions through the learning algorithm back to its training data to identify the *training data points* (vs. variables) most responsible for a given prediction, a complementary approach to LIME.

To learn the effect of individual data points on model behavior, we could omit data points from the training data and note differences in the resulting, trained model. Alas, model training is expensive; this approach is practically infeasible. 

Influence functions, at the heart of the paper, have been around [since the 1980](https://experts.umn.edu/en/publications/characterizations-of-an-empirical-influence-function-for-detectin). 

By slightly perturbing (or zeroing out entirely) the weight around a training point, the influence of that training point on the final model is measured. It allows you to ask of a model "what would happen if we did not have this training point, or if the values of this training point were changed slightly?" as the article's authors do.

Asking this of the training data allows you to do three distinct things that build interpretability around a model: generate adversarial training examples, identify domain mismatch, and fix mislabeled examples. Being able to generate adversarial training examples that can "flip neural network test predictions" means that you can better understand the feature space, and its vulnerabilities to attack. They also reveal regions of the model with lowest confidence, and that are most susceptible to attack - they exploit (and expose) regions of high loss. Domain mismatch between training and testing data is a significant problem in many domains. The authors point to biomedical data as particularly problematic because models trained on data from one hospital may perform poorly at another hospital because of the different populations they serve. Influence functions can point to a small subset from the training examples that have the largest influence over the model's errors on the test examples. The authors also point out that training data labels tend to be noisy, and finding errors is a needle-in-a-haystack proposition. Influence functions pick out the training examples that exert the most influence over the model, flagging those for examination, because those are the cases were faulty labels would cause the most trouble for the model overall.

![Dog/Fish]({{ site.github.url }}/images/2017/08/dog_fish-1503352020658.jpeg)
##### Adversarial examples exploit areas of low certainty. Here, a training image that contains both a dog and a fish leads to less confidence in the model around these labels, and opens up an opportunity for adversarial attack.

Taking the capability for influence functions to create adversarial training examples, we wonder what would happen if you used them as part of a Generative Adversarial Network (GAN), like the ones that have been used to generate [artwork](https://medium.com/towards-data-science/gangogh-creating-art-with-gans-8d087d8f74a1). Part of the problem with GANs have been that they work well at a superficial level, but breakdown when the details are important. They're great at generating [abstract art](https://www.newscientist.com/article/2139184-artificially-intelligent-painters-invent-new-styles-of-art/), or [landscapes](http://www.businessinsider.com/google-street-view-into-pro-level-landscape-with-ai-2017-7), where its not as important for trees to have the 'right' number of branches, or for a painting to have 'correct' colored blobs, but not so great at producing faces (see the portraits made with GANGough). 

One reason for this is that they aren't trained on 'not faces' or 'not cows'. The high-dimensional representations of faces or cows capture features of faces or cows that stand up to the discriminator function of the GAN, but don't necessarily capture the salient features of faces or cows that humans could identify (like the right number of eyes or legs). What would happen if we could supply the generative function of the GAN with adversarial training examples, produced by an influence function? We're guessing it would be like adding active learning to a GAN, reducing the least salient features of a representation by providing adversarial training examples that have a strong influence over the features that have proven the trickiest for GANs.

![Fallout-Cow]({{ site.github.url }}/images/2017/08/fallout_cow-1503351768377.jpeg)
##### GANs can sometimes produce nightmarish versions of common objects that humans have no trouble perceiving as incorrect. The aptly-named ['fallout cow'](https://arxiv.org/pdf/1701.00160.pdf) in the lower left is one example.

Influence functions add an important tool to the interpretability toolkit, as well as open up a very promising avenue for future research.

---

## Bend It Like - Who Now?
When it comes to games, we often think of machine learning and AI in relation to video - but [this real-life robotic soccer tournament](https://www.robocup2017.org/eng/leagues_football.html) has a lofty goal (pun intended): according to [a recent article in the Smithsonian](http://www.smithsonianmag.com/innovation/why-funny-falling-soccer-playing-robots-matter-180964260/), the aim is to develop a team of robots by 2050 that can take down the reigning World Cup champions.  

[As you can see from this (adorable) video](https://youtu.be/OYm_9ifChFc), they've still got a ways to go, but the technology required is far more complex than one might think.  Each robot must not only "think for itself" (being "aware" of its own surroundings and keeping its balance while playing the game), but also learn to work in conjunction with others - a feat which even we humans can sometimes find difficult.  These small playing fields are the testing ground for important work that could have long-term applications in multiple fields.

As technology continues to develop, more and more use cases for ML and AI application will continue to emerge, sometimes in industries where we may have least expected it.  Algorithms are already hard at work in [wildlife conservation](https://www.oreilly.com/ideas/from-binoculars-to-big-data-citizen-scientists-use-emerging-technology-in-the-wild) and [sleep studies](http://news.mit.edu/2017/new-ai-algorithm-monitors-sleep-radio-waves-0807).  What's next?  Our reaches of our imaginations and time are the only limiting factors.

---

##TODO: add updates
