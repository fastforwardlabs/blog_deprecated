---
slug: 2020-04-23-client
layout: newsletter
---

## Interpretability: LIME and SHAP in prose and code

At Cloudera Fast Forward, we see model interpretability as an important step in the data science workflow. Being able to explain how a model works serves many purposes, including building trust in the model's output, satisfying regulatory requirements, model debugging, and verifying model safety, amongst other things. 

We have written a research report (which you can access [here](https://ff06-2020.fastforwardlabs.com/)) that discusses this topic in detail.

In this newsletter article (and its accompanying [notebook on Colab](https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N)), I revisit two industry standard algorithms for interpretability - LIME and SHAP. I discuss how these two algorithms work, and show some code examples of how to implement them in python. 

At the end of this notebook, you should be familiar with: 
* An overview of model interpretability
* Interpreting white box models, such as Linear/Logistic Regression (using model coefficients) and Tree models (using feature importance scores) and how to implement this in python
* Interpreting black box models with LIME and SHAP (KernelExplainer, TreeExplainer) and how to implement this in python
* Good practices for “debugging” LIME and SHAP explanations
* Limitations of LIME/SHAP (a.k.a., when to choose LIME over SHAP)
 
Some figures from the notebook are included below.

![]({{ site.github.url }}/images/editor_uploads/2020-04-24-180128-logreg.png)
![]({{ site.github.url }}/images/editor_uploads/2020-04-24-180103-feat.png)
 Figure shows a) Using model coefficients to as global explanations for a logistic regression model. b) Using feature importance measures as global explanations for Tree based models (Decision Tree, Random Forests, Gradient Boosted Trees).

![]({{ site.github.url }}/images/editor_uploads/2020-04-24-165354-LIME.png)
![]({{ site.github.url }}/images/editor_uploads/2020-04-24-165419-SHAP.png)
Figure shows the local explanations created with LIME and SHAP for a given test data instance across 5 models. We do agreement in magnitude and direction across all models for both explanation methods (except for the Decision Tree).


The rest of this post as well as code models and visualizations shown can be found in the notebook.  Try it out here:8
[https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N](https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N)