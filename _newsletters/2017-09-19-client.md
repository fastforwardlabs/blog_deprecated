---
layout: newsletter
slug: 2017-09-19-client
---

Hello,

*"If you want to have good ideas, you must have lots of ideas and learn to throw away the bad ones,"* according to two-time Nobel prize winner Linus Pauling. At Cloudera Fast Forward Labs, we believe in bad ideas. To have good ideas, you have to have lots of ideas, some will be bad. *"Have I had enough bad ideas,"* [asked](https://www.nature.com/naturejobs/2017/170824/pdf/nj7668-491a.pdf) emeritus professor of rheumatic diseases John Kirwan. He set out to develop an admittedly subjective but very useful scoring system for evaluation.

When businesses struggle to innovate, it may not be the lack of *good* ideas that slows progress. It may be the lack of *bad* ideas. Combined with diligent notekeeping — see Dustin Tran's recommendations for a [research engineering workflow](http://dustintran.com/blog/a-research-to-engineering-workflow) — you can now check your ratio of bad to good ideas. If you find you do not have enough bad ideas, come talk to us!

Take a look, for example, at this catalogue of inspiring ("procrastination-inspiring") [bots](https://botwiki.org/) or [work on modeling other minds](https://blog.openai.com/learning-to-model-other-minds/) to allow agents to discover strategies motivated by self-interest, and yet collaborative, for games like the prisoner's dilemma. 

Most certainly in the category of good ideas, smartwatches can now detect [Afib](http://www.mobihealthnews.com/content/study-apple-watch-paired-deep-neural-network-detects-atrial-fibrillation-97-percent-accuracy), the development of specialized [hardware](https://www.anandtech.com/show/11804/huawei-shows-unannounced-kirin-970-at-ifa-2017-dedicated-neural-processing-unit) for AI continues apace (most notably inside the newest [iPhone](https://www.cnbc.com/2017/09/12/apple-unveils-a11-bionic-neural-engine-ai-chip-in-iphone-x.html)).

Onto the newsletter. This weeek, we discuss the promise of multi-task learning for natural language processing, honor the end of [New York Fashion Week](http://nyfw.com/) with our take on Fashion-MNIST, and — continuing with our theme of good "bad ideas" — discover the opposite of "staying on top things".

Enjoy!

---

## Neural Networks can multi task

When an algorithm extracts sentiment, classifies emails as spam, or organizes text, it is performing functions that fall under the umbrella known as “text classification” (part of natural language processing, NLP). Mainstream approaches for text classification are bag-of-words and vectorization (using either Word2vec or GloVe to turn words into numbers (i.e., vectors) that computers can understand). Rather than framing text classification as a regression problem, [recent research](https://arxiv.org/abs/1704.05742) has combined text classification with multi-task learning LSTMs and adversarial learning.

Multi-task learning is motivated by the (surprising) finding that *one* neural network (a multi tasker) trained an a set of tasks *at the same time* achieves better performance on every single task compared to (specialist) networks (single taskers) trained on one task in isolation. Unlike humans, neural networks can multi-task.

During multi-task training, the multi-task network learns to separate the common, task-invariant features from the task-specific features; task-invariant and task-specific features are "stored" in separate layers of the network. To get a clean representation of task-invariant features, the authors use adversarial training to *“train a model to produce shared features such that a classifier cannot reliably predict the task based on these features”*. 

To avoid leakage of task-invariant features into the task-specific layer, the authors introduce orthogonality constraints to enforce separation between the shared and task-specific features. The final loss function of the model, used for evaluation of the model during (and after) training, is the summation of the loss from (i) multi-task learning, (ii) adversarial training and (iii) orthogonality. It's conceptually elegant.

To evaluate their solution, the authors collected 16 datasets (14 of which are product reviews and the rest are movie reviews) for evaluation purposes. Given the objective of classifying a review as either positive or negative, all multi-task models outperform the single taskers. Further, models *with* outperform models *without* adversarial training. Finally, the clean, representation of tasks-invariant features can be used to jump start other text classification models (the same way we use pre-trained word embeddings in NLP tasks) as the authors demonstrate in their [paper](https://arxiv.org/abs/1704.05742).

We think the combination of multi-task learning and adversarial networks is smart and promising. Being able to reuse and transfer knowledge from the (clean) shared layer will enable new systems to be brought up quicker - all very exciting! Curious? Check out [this article](https://arxiv.org/abs/1706.05098) for an overview of multi-task learning (and its promise).

---

## Fashion-forward algorithms

Unless you have the fashion sense and ingenuity of Alicia Silverstone in the 1995 film "Clueless", finding what to wear in the morning can be a hassle. Regardless of whether you get a sense of achievement from managing to have matching socks on in the morning, or you are a more fashion-aware individual, what to wear is a decision we all make every day. There has therefore always been interest in products that help people - busy professionals or the simply fashion-challenged - dress well without effort. Beyond Clueless-style '[closet inventory apps](https://verilymag.com/2017/03/closet-organization-apps-for-your-wardrobe)' that help you mix-and-match clothes that are currently in your wardrobe, providing fashion and styling advice is challenging. Trends change, personal styles and body types vary and different outfits are suited for different occasions. [Social shopping websites](https://www.lifewire.com/top-social-shopping-websites-3486565) are one solution, but still require considerable amount of time to navigate through all the options and styling suggestions. This is where algorithms that make your wardrobe decisions for you come in ... and new business models open up.

![](/images/2017/09/Clueless-1504732987912.gif)

##### The idea to have a computer make your fashion decisions for you is not new. In the film "Clueless", Alicia Silverstone had images of her entire wardrobe imported in her personal computer to help her pick the perfect outfit for the day.

Among the [many alternatives](https://www.whatsupfagans.com/stitch-fix-alternative-competitor/) that are currently populating the industry of personalized styling apps, Stitch Fix has recently gained the attention of both consumers and the business world. With a strong emphasis on machine learning and intelligent algorithms, its entire business structure is based on [predictive analytics and recommendation engines](http://algorithms-tour.stitchfix.com/#recommendation-systems). For the consumer, Stitch Fix uses a combination of Netflix-style collaborative filtering and more classic mixed-effects models to decide on recommendations. We especially like that they take the extra steps to cater to niche consumer categories through their 'human stylist' service, also algorithmically decided. Algorithms are also used to organize and restock inventory, taking the example of Amazon to the personalized styling services. Moreover, in an industry-first Stitch Fix is testing [Hybrid Design](http://multithreaded.stitchfix.com/blog/2017/03/13/hybrid/), a garment-designing algorithm that, if successful, would definitely make an impact on the online fashion industry.

Interest in fashion forecasting and design personalization through data science is not restricted to a handful of companies. Fashion-designing algorithms have attracted attention in the research community, both because of the rich bussiness opportunities for applications and the unique nature of the problem of building predictiction algorithms for fashion. This August, the [second international workshop on fashion and KDD](https://kddfashion2017.mybluemix.net/) took place in Nova Scotia, Canada. The title of the invited opening talk by Kavita Bala 'Fashion and Style Discovery: object and material recognition from online photo collections' is evidence that data science is currently very intrested in fashion. For those who want to start playing around with their own ideas for fashion-forward algorithms, [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist), a labelled dataset of 28x28 grayscale images of 70,000 fashion products from 10 categories, is a good place to start.

Amazon anounced this June that it is entering the wardrobe subscription market and creating [Prime Wardrobe](https://techcrunch.com/2017/06/20/amazon-prime-wardrobe/), a service that allows subscribers to order and try garments at home with the option to purchase. Right now, it looks like the company is taking advantage primarily of its inventory and distribution capabilities for its value proposition for the product. But judging from Amazon's past track record of successful innovations, we aren't surprised to see their data team announcing [aditional predictive analytics features](http://www.businessinsider.com/amazon-researchers-testing-ai-machine-learning-for-fashion-2017-8) being developed for the product. In the meantime we can imagine what the industry might look like in the near future: perhaps algorithms that decide the optimum outfit for each day depending on the weather, the user's schedule and personal sense of style, or apps that combine automated fashion design with at-home [3D printing](https://www.wired.com/2017/05/the-shattering-truth-of-3d-printed-clothing/) for truly personalized fashion options. 

One thing is for sure, the fashion industry is not 'clueless' when it comes to algorithms.

---

## In Praise of Slowness

It turns out Edward Tufte, pioneer of data visualization, is active on the [twitters](https://twitter.com/EdwardTufte). He curates great content, but our favorite of his recent posts is a [signal-boost](https://twitter.com/edwardtufte/status/737357960041598977) for Don Knuth's letter announcing that he has quit email. From 1990. 

![](/images/2017/09/knuth_letter.jpg)

We, too, see the value in being on the bottom of things, and we take great pleasure in digesting knowledge and presenting it clearly for you. But as far as emails go - well - we still try to stay on top of things, even if we sometimes let [shell scripts](https://youtu.be/IoQ4tka1zNk) do the dirty work. 

---

## Cloudera FFL Updates

We're giving talks at some upcoming conferences:
- [Bloomberg Data For Good Exchange](https://www.bloomberg.com/company/d4gx/): September 24 **(New York, Friederike)**
- [Strata Data Conference](https://conferences.oreilly.com/strata/strata-ny): September 28 **(New York, Hilary)**
- [Grace Hopper Celebration](https://ghc.anitaborg.org/): October 4-6 **(Orlando, Hilary)**
- [AEC Technology Symposium & Hackathon](http://core.thorntontomasetti.com/aec-tech-symposium/): October 19-21 **(New York, Friederike)**

And we also look forward to Strata NY talks by our new Cloudera colleagues. Check out [Mike Olson's talks and briefings on September 27/28](https://conferences.oreilly.com/strata/strata-ny/public/schedule/speaker/5259), for example.

Thank you for reading. We'd love to hear your thoughts! You can reach us anytime at [clients@fastforwardlabs.com](mailto:clients@fastforwardlabs.com). 

— The Cloudera Fast Forward Labs Team

