---
layout: post
title: "Why we talk about AI the way we do, and why we have to change it."
date: 2017-07-25 12:00
preview_image: TBA
author: Friederike
author_link: "https://twitter.com/FSchueuer"
feature: true
published: false
---

The public conversation about machine learning and AI happens at the extremes. Some say *"AI is awesome. It writes [movie scripts](https://arstechnica.com/the-multiverse/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/), wins at [poker](http://www.sciencemag.org/news/2017/03/artificial-intelligence-goes-deep-beat-humans-poker) and [Go](https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke-jie/), and helps [Google cool its data centers](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)."* Others worry *"AI is scary. It will take our [jobs](http://www.eng.ox.ac.uk/about/news/new-study-shows-nearly-half-of-us-jobs-at-risk-of-computerisation), robot investors will incite [wars for profits](http://fortune.com/2016/08/17/elon-musk-ai-fear-werner-herzog/), and machine-learning overlords will come to dominate human kind."*

Conversations at the extremes are rarely productive. They prevent a more constructive debate about the very real capabilities of AI today, its likely capabilities within upcoming years, associated problems and exciting opportunities. Granted, Elon Musk's [worries](http://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x) about machine-learning overlords motivated the creation of his companies: [SpaceX](http://www.spacex.com/) (to escape to Mars), [Neuralink](https://www.technologyreview.com/s/604254/with-neuralink-elon-musk-promises-human-to-human-telepathy-dont-believe-it/) (to become [one with the machines](https://www.theverge.com/2017/3/27/15077864/elon-musk-neuralink-brain-computer-interface-ai-cyborgs)), and [OpenAI](https://openai.com/) (to create benevolent machines). Excess worry can lead to results, but to address the very real problems associated with machine learning and AI today, and to identify and build towards the exciting new futures that could be enabled by them, we have to change how we talk about AI. Specifically, we need to avoid the distraction of dystopian doomsday scenarios.

## Productive worries, dystopian doomsdays, and cognitive biases
While machine learning and AI (like any (new) technology) can be used for good and bad, dystopian doomsdays originate less in reality and more in human cognitive biases. There is a rich tradition of research on [human biases in thought and decision making](https://en.wikipedia.org/wiki/List_of_cognitive_biases). We overestimate the probability of rare events, which is why people buy lottery tickets, and underestimate the probability of frequent events, for example, a phenomenon known as "probability distortion". We are sentitive to [anchoring](https://en.wikipedia.org/wiki/Anchoring), [survivership](https://en.wikipedia.org/wiki/Survivorship_bias), and [status-quo](https://en.wikipedia.org/wiki/Status_quo_bias) bias. The doom-and-gloom picture of the future of AI stems from the last.

Status-quo bias, an emotional bias, is a preference for the current state of affairs. The current state of affairs is taken as the benchmark, and any change is perceived as a loss. Status-quo bias is related to the endowment effect, the tendency to weigh potential losses of switching from the status quo more heavily than potential gains. The endowment effect is part of nobel laureate [Daniel Kahneman](http://www.nytimes.com/2011/11/27/books/review/thinking-fast-and-slow-by-daniel-kahneman-book-review.html)'s [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory), which states that utility functions are steeper in the loss domain than in the gains domain. In other words, change in a negative direction is perceived to be more harmful than the same amount of change in a positive direction could be beneficial. 

In general, such biases are taken to be instances of behavior that depart from a "rational observer" model (i.e., a reward maximizing observer), but not always. Status-quo bias can be rational, [some argue](http://www.journals.uchicago.edu/doi/pdfplus/10.1086/678482), especially when we have [limited information about future events](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.545.5116&rep=rep1&type=pdf), as we often do -- especially when it comes to the future capabilities of a technology that [we may not understand](https://techcrunch.com/2017/07/19/this-famous-roboticist-doesnt-think-elon-musk-understands-ai/).

We have an "imaginative" bias, too. It is simply easier for us to imagine the disappearance of things we know and with which we are familiar. We fear their loss. It is much harder for us to imagine the existence of things in the future that we do not know and which do not exist today. Losses loom larger than gains. We are biased against excitement. 

As machine learning researchers and practitioners, we have the privilege of being better positioned to imagine what may not exist today but will exist in the future; exciting new capabilities and jobs for people, and hopefully a fairer, more equal society than the one in which we live today. We have a more intimate knowledge of machine learning tools and the AI capabilities these tools enable, which allows us to be better at extrapolation. We may feel uncomfortable taking the stage and "predicting the future," but as Albert Einstein once said: *"Those who have the privilege to know have the duty to act."* If we don't fill that vacuum, others will, and we'll drift towards a conversation at the extremes, with a slant towards dystopianism. It is important for us to take part in the conversation to correct the cognitive biases at the heart of doomsday distopias.

## Change is ever present, everywhere
Today's society is already different from the one we lived in five, ten, or twenty years ago. With personal computers the size of pocket calculators, machine capabilities have long surpassed our own in some areas, and plenty of that change was driven by data and algorithms. Machine learning and AI can help diagnose disease, achieving [higher accuracy of tumor classification on slides of lung cancer tissue](https://med.stanford.edu/news/all-news/2016/08/computers-trounce-pathologists-in-predicting-lung-cancer-severity.html) compared to human medical doctors, and self-driving cars promise to reduce the number of fatalities in traffic accidents. Innovation does not happen in isolation, however; it is embedded in current reality.

Earlier in July, the News Media Alliance, a trade association representing approximately 2,000 newspapers in the United States and Canada, [asked the US Congress for an exemption from antitrust law](https://www.newsmediaalliance.org/release-digital-duopoly/) to gain the rights to collectively bargain with Google and Facebook, who have taken control over news media distribution, eating into publishers' revenue. Access to good reporting, publishers say, is essential to a well functioning, democratic, human society, so they have asked Congress to protect it.

Google's and Facebook's control over the distribution of content has changed news consumption, and not always for the better. [Fewer stories drive more traffic](https://www.theatlantic.com/technology/archive/2017/07/facebook-and-the-media/533079/); everything is "spikier" now. Consequently, as publishers chase after "spikey pieces," articles tend to cluster around a few themes and some news events receive outsized attention at the expense of others (e.g., [covfefe](https://www.wired.com/2017/05/internet-defines-covfefe/)). Messages with [moral-emotional words are more likely to get retweeted](https://phys.org/news/2017-06-messages-moral-emotional-words-viral-social.html), and writers change their language to deliver what is attractive at the time.

But the inconvenient truth is that few newspapers ever derived economic value from good reporting. For advertisers to place their ads in the papers, content had to be just good enough to attract attention. The past is not always better than the present; sometimes it is just the same. 

Data, machine learning, and AI is at the heart of Google's and Facebook's successes. Data, machine learning, and AI allow for better ad targeting, a strong value proposition for advertisers. Google and Facebook started gobbling up publishers' *advertisement* dollars and thereby started controlling the means of distribution for content. Consequently, ad-supported business models may not be viable anymore in the publishing industry. This sounds dystopian, but consider a future in which quality publications support themselves through good reporting. Publications that spend the time and effort to understand their readers, based on quantitative (data) and [qualitative insights](https://www.fastcodesign.com/90134155/the-most-crucial-design-job-of-the-future), research and write content to keep their readers informed. (Some of these publications exist today).

## Embedded innovation, work place hazards of the future
We chose the example of changing business models in news media to highlight that machine learning and AI innovation, often underpinned by the availability of data, happenes within legal and economic realities. If we don't acknowledge the embeddedness of innovation and the incentives of actors (i.e., stakeholders, decision makers), we will not be able to recognize impacts and opportunities, nor potential risks and downsides.

There are challenges ahead, regarding identifying and mitigating workplace hazards of new technology-enabled jobs. During the industrial revolution, with the introduction of machinery, workers put their bodies at risk, and those risks needed to be addressed. There will be different risks to consider in regards to machine learning and AI. For example, [content moderators](https://www.theguardian.com/news/2017/may/21/facebook-moderators-quick-guide-job-challenges), the "human eyes" that check machine-flagged content for objectionable text or images, scan through a lot of material every single day, putting their minds at risk as they view images and words that could cause harm. We have to ask ourselves, can a human being see the footage of a man killing his own child, as described in The Guardian's report on [the life of a Facebook content moderator](https://www.theguardian.com/news/2017/may/25/facebook-moderator-underpaid-overburdened-extreme-content), and walk away unharmed? These are the sorts of challenges we should consider and solve for today, rather than being distracted by future machine-learning overlords and our escape to Mars. The industrial revolution led to machines that could go down into coal mines, so that fewer miners lost life and limb as part of their jobs. The AI revolution may proceed similarly - and as it does, we should remember that the AI revolution does not "just happen." Instead, we make it happen. It is we who build the future. 

So we should focus on opportunity, identify current and near-future challenges, and provide constructive solutions. One of those solutions will be to drive the public coverage of machine learning and AI *away* from the extremes, so the general public can gain a better sense of the *real* capabilities of machine learning and AI today, and for years to come. We can empower the general public to share their perspectives as we involve them in a more balanced, nuanced, more productive conversation, which has become only ever more important.

## More than cats, truth is harder to define
In machine learning, we used to [classify cats](https://www.wired.com/2012/06/google-x-neural-network/). One's background, ethnicity, socioeconomic class, upbringing, and life experiences are unlikely to influence one's judgement of whether or not a given image is an image of a cat. We tend to agree on the concept of "catness." But what about "fairness" or "unbiasedness" - concepts at the heart of [current machine learning research](https://arxiv.org/abs/1607.06520)? Who will be the arbiter? 

As we have started to move away from classifying cats, the questions have become more complex, and truth has becomes harder to establish (if we can even do that all) - which is why we need to start taking into account diverse perspectives. We need to counter status-quo bias, the endowment effect, and imaginative bias and empower the public to understand, dream, and share their perspectives in more helpful ways than uninformed expressions of dystopian doomsday paranoia. We need to move away from the extremes in the conversation about machine learning and AI, and instead tackle some of its hardest problems - together.
