---
title: "Highlights of 2018"
date: "2018-12-18 11:12 -0800"
preview_image: "/images/editor_uploads/2018-12-19-194412-alchemist.jpg"
feature: true
published: true
---

We end 2018 with a round-up of some of the research, talks, sci-fi, visualizations/art, and a grab bag of other stuff we found particularly interesting, enjoyable, or influential this year (and we’re going to be a bit fuzzy about the definition of “this year”)!

## Research

In addition to our own research, on recommendation engines, [multi-task learning](https://blog.fastforwardlabs.com/2018/07/24/ff08-launch.html), and [federated learning](https://blog.fastforwardlabs.com/2018/11/14/federated-learning.html), we found three other themes particularly interesting.

![](/images/editor_uploads/2018-12-19-194412-alchemist.jpg)

##### "The Alchymist, In Search of the Philosopher’s Stone, Discovers Phosphorus, and prays for the successful Conclusion of his operation, as was the custom of the Ancient Chymical Astrologers" (great title!) by Joseph Wright of Derby, now in Derby Museum and Art Gallery, Derby, UK ([Wikipedia](https://en.wikipedia.org/wiki/The_Alchemist_Discovering_Phosphorus#/media/File:Joseph_Wright_of_Derby_The_Alchemist.jpg))

At NIPS in December 2017, Ali Rahimi (and Ben Recht) delivered an address that asserted that modern deep learning is more like alchemy than science. We won’t attempt to paraphrase their short talk, but many of us found it compelling, and it’s certainly worth [watching](https://www.youtube.com/watch?v=Qi1Yry33TQE) or [reading](http://www.argmin.net/2017/12/05/kitchen-sinks/). This lead to much discussion in the deep learning community, and the appearance of a subdiscipline that treats deep learning as an observational science (see e.g. [How AI Training Scales](https://blog.openai.com/science-of-ai/) and [How does batch normalization help optimization?](https://arxiv.org/abs/1805.11604)).

We published our own research on interpretability in 2017. The area we focused on has developed rapidly, with tools like Anchors and Shap (see [this back issue of our newsletter](https://blog.fastforwardlabs.com/2018/07/31/progress-in-machine-learning-interpretability.html) for more) now the state-of-the-art in black box interpretability. And to the extent interpretability will help deep learning become more scientific and less alchemical, we loved [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/). But our favorite interpretability work of 2018 questions the entire premise of this family of methods. Cynthia Rudin’s [Please Stop Explaining Black Box Models for High Stakes Decisions](https://arxiv.org/abs/1811.10154) is a bracing and highly recommended read.

The final theme we found exciting in 2018, and will be keeping an eye on in 2019, is transfer learning applied to NLP. Sebastian Ruder’s [NLP's ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/) sets the scene really well for 2019, and highlights some of the projects we’re most interested in, which are well covered in Thomas Wolf’s [The Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a). We also wrote a [blog post about transfer learning](https://blog.fastforwardlabs.com/2018/09/17/deep-learning-is-easy-an-introduction-to-transfer-learning.html), and [a newsletter about its application to NLP in particular](https://blog.fastforwardlabs.com/2018/08/29/breakthroughs-in-transfer-learning-for-nlp.html). 

## Talks

Ex-Clouderan Josh Wills’s ten minute talk on [Visibility and Monitoring for Machine Learning Models](https://blog.launchdarkly.com/visibility-and-monitoring-for-machine-learning-models/) was our favorite talk of 2018. The highlight of the talk was the koan-like “You should deploy [a model] never or prepare to deploy it over and over and over and over and over again, repeatedly forever, ad infinitum”.

![](/images/editor_uploads/2018-12-19-195505-josh.jpg)

##### Josh Wills (Image credit: [Launch Darkly](https://blog.launchdarkly.com/visibility-and-monitoring-for-machine-learning-models/) and the [Test In Production Meetup](https://www.meetup.com/Test-in-Production/)

Hillel Wayne’s [Beyond Unit Tests: Taking Your Testing to the Next Level](https://www.youtube.com/watch?v=MYucYon2-lk) was an engaging, opinionated and slightly mind-bending view of the relationship between traditional unit/integration testing and formal methods.

Highlighting a talk from 2015 feels a little like cheating, but in a year that saw the implementation of the GDPR, and our own research into federated learning, we rewatched Maciej Ceglowski’s 2015 Strata keynote [Haunted by Data](https://www.youtube.com/watch?v=GAXLHM-1Psk). His “don’t collect it, don’t store it, don’t keep it” takeaways feel like better advice than ever. Suresh Venkatasubramanian’s 2018 [blog post on regulation of the tech industry vs ethical education](http://blog.geomblog.org/2018/10/on-teaching-ethics-to-tech-companies.html) is an interesting addendum to Ceglowski’s talk.

## Art and sci-fi

In 2018 the machine learning community [rediscovered the well-trodden issue of authorship in modern art](https://hyperallergic.com/468060/christies-sells-ai-generated-art-for-432500-as-controversy-swirls-over-creators-use-of-copied-code/) thanks to Christie’s auction house and the Obvious Collective.

![](/images/editor_uploads/2018-12-19-195836-DsS8L_gXcAEiaPA.jpg)

##### Marco Klingeman's BigGAN explorations (Image credit: [twitter.com/quasimondo](https://twitter.com/quasimondo/status/1064182680265392128))

But Marco Klingeman’s explorations of the [landscapes](https://twitter.com/quasimondo/status/1064230996793614338) and [fauna](https://twitter.com/quasimondo/status/1064171634871992321) of BigGAN were the most successful AI-insipred art (and scifi!) we saw in 2018.

## Everything else

Published in 1986, [The Making of the Atomic Bomb](https://en.wikipedia.org/wiki/The_Making_of_the_Atomic_Bomb) by Richard Rhodes is perhaps not as cutting edge as some of the other things on this list. But we found it interesting for two reasons: first, it’s an interesting story about the management of research in a non-academic context, which is a topic we can’t get enough of at Cloudera Fast Forward Labs. And second, it’s a sobering look at the way researchers attempt (and in many cases fail) to grasp and control the impact of their inventions. The relevance to machine learning research is obvious.

Our favorite periodical was (and is!) Logic. If you’re a follower of Cloudera Fast Forward Labs, you’ll certainly enjoy [this interview with an anonymous data scientist](https://logicmag.io/01-interview-with-an-anonymous-data-scientist/) from their 2017 debut issue, but everything they’ve published since has been equally worthwhile, and relevant to anyone working in tech.

Finally, [this was the best Halloween costume](https://twitter.com/aengelbro/status/1057659128707829760).

Onwards to 2019!
