## The Power of Interpretability
When we evaluate a model, or the algorithm it's built upon, we generally want to evaluate its accuracy, because if it doesn't make correct predictions, then what use is it? But that shouldn't be the final word on a model. Increasingly, we want to know _how_ and _why_ a model makes its predictions. We want the model to be **interpretable**, because we want to understand it well enough to trust it (or convince someone else to trust it). We also want to understand or be able to explain the justifications for particular decisions the model makes. Furthermore, we want to understand the relationship between how the model works and the real phenomena being modeled. These motivations help us build trust in our models, adapt them to uses beyond mere prediction, and guide our analyses.

If interpretable models are so great, why not always use them? Because there is a fundamental tension between accuracy and interpretability. Interpretable models tend to be rather simple, while more accurate models rely on finely tuning a large number of complex features. In recent years, however, researchers have developed "white-box" models that are interpretable _and_ accurate.

We discuss these developments in our most recent report on Interpretability, and have also developed our own prototype to demonstrate how interpretability can be used in the wild. Our prototype can not only predict the likelihood of a customer to churn out of a subscription service, but also explains what factors contribute to that likelihood so that steps can be taken towards retaining them as a customer. Overall, the report demonstrates why interpretability is a crucial component guiding the development and deployment of machine learning models and how to design and code for interpretability. Interpretability is not a single capability, but rather achieves several ends simultaneously. [OR: A model you can interpret and understand is one you can more easily improve. It is also one you, regulators, and society can more easily trust to be safe and nondiscriminatory. And an accurate model that is also interpretable can offer insights that can be used to change real-world outcomes for the better.]

## Enhancing trust
Data scientists have established ways of measuring the performance of a model by validating it against their training data before releasing it into the 'wild'. But even if the model is not _overfitted_ to, or memorizing, the training data, it still can experience _leakage_ if it relies on features that have a problematic relationship with the actual things the model is interested in. A risk model that suggests asthma sufferers are at low risk for death when hospitalized for pneumonia is an artifact of asthma patients being highly treatable for pneumonia when hospitalized, and _not_ an signal that they don't need hospitalization. Interpretability can help understand and communicate how a model relies on its inputs to produce a result.

## Satisfying Regulations
Interpretability can also help satisfy legal frameworks that regulate machine learning in particular applications. [or: For applications in which regulations on machine learning exist, interpretability is almost always required]. Because such regulations are often intended to prevent discriminatory or dangerous models, it is often necessary to prove that the model is not overfitted to attributes like gender, race, or other protected categories. In a regulated environment, it isn't enough to show that you trained your model on race-blind or gender-agnostic data. You have to be able to explain the model as it operates on 'live' data, and this is only possible if the model is interpretable.

Furthermore, even if an algorithm is used in an unregulated industry, other ethical or legal constraints might apply. Interpretability allows you to make a purposeful judgement about the way a model embeds biases from its training data before it is applied at scale, thus anticipating and avoiding legal or ethical jeopardy.

## Explaining Decisions and Improving the Model
Interpretability is important at the local level, too. Having an explanation for how which attributes contribute to the individual decisions a model makes is good not just for insight, but can also lead to new products and better analyses. A model that makes a prediction is useful, but a model that tells you way that prediction was made lets you take actions that can change the outcome of the prediction. Knowing not just if, but also why a customer might leave can help keep that customer. Knowing not just if, but also why an engine might fail can help direct a mechanic to make the proper repairs and keep that engine running. Knowing why a model makes the predictions it does helps users understand and use the model in ways that go beyond prediction.

Interpretability makes for models that work more reliably, too. Debugging an uninterpretable, black-box model is time consuming, and relies at least in part on trial-and-error. Debugging an interpretable model, however, is easier because glaring problems stand out. Having insight into the attributes that are likely causing trouble can motivate a theory about how the model works and how problems can be fixed.
